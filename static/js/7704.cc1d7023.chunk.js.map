{"version":3,"file":"static/js/7704.cc1d7023.chunk.js","mappings":"sRAGM,SAAUA,EAAaC,GAC3B,GACEA,EAAKC,YAAYC,OAAOC,mBACxBH,EAAKI,SAASF,OAAOG,kBAErB,MAAM,IAAIC,MAAM,oBAElB,OAAON,EAAKO,UACd,CAAC,IAEKC,EAAW,0H,SAAA,GAAQF,QAcnB,SAAUG,EAAiBC,GAC/B,GAAKA,GAIDA,EAAOC,QAAS,CAElB,GAA4B,qBAAjBC,aAET,MAAM,IAAIA,aAAa,UAAW,cAElC,IAAMC,EAAI,IAAIL,EAAW,WAEzB,MADAK,EAAEC,KAAO,cACHD,C,CAGZ,CAoBM,SAAUE,EAAeC,EAAiBC,GAC9C,IAAMC,EAAwB,GAC1BC,EAA0B,KAE9B,OAAsB,IAAlBH,EAAOI,OACFJ,GAGTA,EAAOK,MAAK,SAAUC,EAAIC,GACxB,IAAMC,EAAMF,EAAGG,KAAKC,cAAgBH,EAAGE,KAAKC,cAC5C,OAAY,IAARF,EACKA,EAEAF,EAAGG,KAAKE,aAAeJ,EAAGE,KAAKE,YAE1C,IAEAX,EAAOY,SAAQ,SAAAC,GAxBX,IAAyBC,EAAeC,IAyBrCd,GAAUY,EAAMG,KAAKC,UAAUhB,GAAU,KAC1B,OAAdE,GACFD,EAAagB,KAAKL,GAClBV,EAAYU,IA5BWC,EA8BJX,GA9BmBY,EA8BRF,GA5B3BJ,KAAKC,cAAgBI,EAAOE,KAAKN,cAAgB,MACxDK,EAAOC,KAAKN,cAAgBI,EAAOL,KAAKC,cAAgB,IA4B9CG,EAAMG,KAAKC,UAAUd,EAAUa,MAAQ,IACzCb,EAAUa,KAAOH,EAAMG,OAGzBd,EAAagB,KAAKL,GAClBV,EAAYU,IAOpB,IAEOX,EACT,C,8CCtGqBiB,EAAa,WAGhC,WAAYT,EAAuBC,IAAoB,eACrDS,KAAKV,cAAgBA,EACrBU,KAAKT,aAAeA,CACtB,CAwBC,OAxBA,gCAED,WACE,MAAO,GAAP,OAAUS,KAAKV,cAAa,YAAIU,KAAKT,aACvC,GAAC,uBAED,SAAUU,GACR,OACED,KAAKV,cAAgBW,EAAEX,eAAiBU,KAAKT,aAAeU,EAAEV,YAElE,IAAC,kBAED,WAEW,IADT,IAAIW,EACAC,EAAI,EAAC,mBAFGC,EAAqB,yBAArBA,EAAqB,gBAGjC,MAAQF,EAAKC,GAAK,EAChBD,EAAME,EAAKD,GAEb,KAAOA,EAAIC,EAAKpB,OAAQmB,GAAK,EACvBD,EAAIL,UAAUO,EAAKD,IAAM,IAC3BD,EAAME,EAAKD,IAGf,OAAOD,CACT,KAAC,EA9B+B,GAgC5B,SAAUG,EAAUC,GAA4C,IAA7BC,EAAS,UAAH,6CAAG,EAAGC,EAAY,UAAH,8CAC5D,GAAIA,EACF,MAAM,IAAItC,MAAM,mDAGlB,OAAO,IAAI6B,EACW,cAApBO,EAAMC,EAAS,GACO,WAApBD,EAAMC,EAAS,GACK,SAApBD,EAAMC,EAAS,GACK,MAApBD,EAAMC,EAAS,GACK,IAApBD,EAAMC,EAAS,GACfD,EAAMC,EAAS,GAChBD,EAAMC,EAAS,IAAM,EAAKD,EAAMC,GAErC,CC5CA,IACqBE,EAAK,WAYxB,WACEpB,EACAO,EACAc,GACuB,IAAvBC,EAAc,UAAH,kDAAGC,GAAS,eAEvBZ,KAAKX,KAAOA,EACZW,KAAKJ,KAAOA,EACZI,KAAKU,IAAMA,EACXV,KAAKa,aAAeF,CACtB,CAyBC,OAzBA,sCAED,WACE,MAAO,GAAP,OAAUX,KAAKX,KAAI,aAAKW,KAAKJ,KAAI,iBAC/BI,KAAKU,IACP,yBAAiBV,KAAKW,cAAa,IACrC,GAAC,sBAED,WACE,OAAOX,KAAKc,gBACd,GAAC,uBAED,SAAUb,GACR,OACED,KAAKX,KAAKQ,UAAUI,EAAEZ,OACtBW,KAAKJ,KAAKC,UAAUI,EAAEL,OACtBI,KAAKU,IAAMT,EAAES,GAEjB,GAAC,yBAED,WACE,YAA0BE,IAAtBZ,KAAKa,aACAb,KAAKa,aAEPb,KAAKJ,KAAKN,cAAgB,MAAYU,KAAKX,KAAKC,aACzD,KAAC,EA/CuB,G,yBCSIyB,EAAS,WASrC,cAMC,IALCC,EAAU,EAAVA,WAAU,IACVC,cAAAA,OAAa,IAAG,WAACC,GAAS,OAAKA,CAAC,oBAKhClB,KAAKgB,WAAaA,EAClBhB,KAAKmB,aAAeF,CACtB,CA+CC,OA/CA,4EASM,yGAAoC,OAAlBG,EAAAA,EAAAA,OAAAA,QAAAA,IAAAA,EAAAA,GAAAA,EAAAA,GAAgB,CAAC,EAAC,SAENpB,KAAKqB,MAAMD,GAAK,OAA3B,OAA2B,SAApC,EAAPE,QAAYC,GAAI,8BACjBA,GAAI,gDACZ,kDAbA,IAaA,4BASD,SACEC,EACAC,GAEA,OAAID,EACKA,EAAW3B,UAAU4B,GAAiB,EACzCA,EACAD,EAEGC,CAEX,GAAC,4DAED,4GAMG,OANSL,EAAAA,EAAAA,OAAAA,QAAAA,IAAAA,EAAAA,GAAAA,EAAAA,GAAgB,CAAC,EACtBpB,KAAK0B,cACR1B,KAAK0B,YAAc,IAAIC,IAAJ,CAA0B,CAC3CC,MAAO,IAAIC,IAAJ,CAAa,CAAEC,QAAS,IAC/BC,KAAM,kBAAM,EAAKC,OAAOZ,EAAK,KAEhC,kBACMpB,KAAK0B,YAAYO,IAAI,QAAS,UAAMrB,IAAU,gDACtD,kDAVA,IAUA,gEAED,WAAgBsB,GAAa,wFAAoB,OAAlBd,EAAAA,EAAAA,OAAAA,QAAAA,IAAAA,EAAAA,GAAAA,EAAAA,GAAgB,CAAC,EAAC,SAC9BpB,KAAKqB,MAAMD,GAAK,OAAS,GAAT,KAAUc,EAAK,YAAbZ,QAAQ,EAAD,8BAAW,CAAC,EAAC,uCAAEa,UAAQ,gDAClE,mDAJA,MAIA,EAjEoC,GCLjCC,EAAY,SAMlB,SAASC,EAASC,EAAaC,GAG7B,MAAO,CACL,CAAC,EAAG,GACJ,CAAC,IAJHD,GAAO,IAIQ,IAAK,IAHpBC,GAAO,IAGyB,KAC9B,CAAC,GAAKD,GAAO,IAAK,GAAKC,GAAO,KAC9B,CAAC,IAAMD,GAAO,IAAK,IAAMC,GAAO,KAChC,CAAC,KAAOD,GAAO,IAAK,KAAOC,GAAO,KAClC,CAAC,MAAQD,GAAO,IAAK,MAAQC,GAAO,KAExC,CAAC,IAEoBC,EAAW,qGA8N7B,OA9N6B,0EAC9B,WAAgBC,GAAe,8FAAoB,OAAlBrB,EAAAA,EAAAA,OAAAA,QAAAA,IAAAA,EAAAA,GAAAA,EAAAA,GAAgB,CAAC,EAAC,SACzBpB,KAAKqB,MAAMD,GAAK,OAAzB,GAATsB,EAAY,EAAH,KACC,CAAF,yCACJ,GAAC,OAGyB,GAD9BC,EAAQD,EAAUE,YAAYH,GACxBC,EAAUpB,QAAQqB,GACpB,CAAF,0CACE,GAAC,QAEE,KAALE,EAAUH,EAAUpB,QAAQqB,GAA5BE,OACG,CAAF,yCACAA,EAAMC,WAAS,kCAEhB,GAAC,iDACV,mDAhB6B,IAmB9B,6DACA,kJAC2B,OADd1B,EAAAA,EAAAA,OAAAA,QAAAA,IAAAA,EAAAA,GAAAA,EAAAA,GAAgB,CAAC,EAAC,KACT2B,EAAAA,MAAK,SAAQ/C,KAAKgB,WAAWgC,SAAS5B,GAAK,yDAG/D,GAHMd,EAAQ,EAAH,KACXjC,EAAiB+C,EAAK9C,QAGlBgC,EAAM2C,aAAa,KAAOb,EAAU,uBAChC,IAAIlE,MAAM,kBAAiB,QAcS,GATtCgF,EAAW5C,EAAM6C,YAAY,GAC7BC,EAAc9C,EAAM6C,YAAY,GAChCE,EACU,MAAdD,EAAwB,uBAAyB,iBAM7CE,EALwC,CAC5C,EAAG,UACH,EAAG,MACH,EAAG,OAEmC,GAAdF,GACb,CAAF,sBACH,IAAIlF,MAAM,qCAAD,OAAsCkF,IAAc,QAoEnE,OAlEIG,EAAgB,CACpBC,IAAKlD,EAAM6C,YAAY,IACvBM,MAAOnD,EAAM6C,YAAY,IACzBZ,IAAKjC,EAAM6C,YAAY,KAEnBO,EAAYpD,EAAM6C,YAAY,IAE9BQ,IAAiB,GAAoB,IADrCC,EAAQ,GACwB,IAAW,GAAK,EAChDC,EAAe,KAAH,IAAG,EAAM,GAAa,EAARD,GAC1BE,EAAWJ,EAAYK,OAAOC,aAAaN,GAAa,KACxDO,EAAY3D,EAAM6C,YAAY,IAG9Be,EAAoB5D,EAAM6C,YAAY,IAAG,EACVnD,KAAKmE,gBACxC7D,EAAM8D,MAAM,GAAI,GAAKF,IADftB,EAAW,EAAXA,YAAayB,EAAW,EAAXA,YAKjBC,EAAa,GAAKJ,EAEhB5C,EAAU,IAAIiD,MAAMrB,GAAUnB,KAAK,GAAGyC,KAAI,WAE9C,IAAMC,EAAWnE,EAAM6C,YAAYmB,GACnCA,GAAc,EAGd,IAFA,IACIzB,EADEV,EAAuC,CAAC,EAErCuC,EAAI,EAAGA,EAAID,EAAUC,GAAK,EAAG,CACpC,IAAMhE,EAAMJ,EAAM2C,aAAaqB,GAE/B,GADAA,GAAc,EACV5D,EAAMiD,EAAe,EACvB,MAAM,IAAIzF,MACR,8DAEG,GAAIwC,IAAQiD,EAAe,EAAG,CACnC,IAAMgB,EAAarE,EAAM6C,YAAYmB,GACrCA,GAAc,EACK,IAAfK,IACF9B,EAAQ,EAAK+B,eAAetE,EAAOgE,IAErCA,GAAc,GAAKK,C,KACd,CACL,IAAMA,EAAarE,EAAM6C,YAAYmB,GACrCA,GAAc,EAEd,IADA,IAAM1F,EAAS,IAAI2F,MAAMI,GAChBE,EAAI,EAAGA,EAAIF,EAAYE,GAAK,EAAG,CACtC,IAAMC,EAAIzE,EAAUC,EAAOgE,GACrBS,EAAI1E,EAAUC,EAAOgE,EAAa,GACxCA,GAAc,GACdU,EAAgB,EAAKC,eAAeD,EAAeF,GACnDlG,EAAOiG,GAAK,IAAIpE,EAAMqE,EAAGC,EAAGrE,E,CAE9ByB,EAASzB,GAAO9B,C,EAKpB,IAAMsG,EAAc5E,EAAM6C,YAAYmB,GACtCA,GAAc,EAEd,IADA,IAAMa,EAAc,IAAIZ,MAAMW,GACrBL,EAAI,EAAGA,EAAIK,EAAaL,GAAK,EACpCM,EAAYN,GAAKxE,EAAUC,EAAOgE,GAClCA,GAAc,EACdU,EAAgB,EAAKC,eAAeD,EAAeG,EAAYN,IAEjE,MAAO,CAAE1C,SAAAA,EAAUgD,YAAAA,EAAatC,MAAAA,EAClC,IAAE,kBAEK,CACLvB,QAAAA,EACAwC,SAAAA,EACAH,aAAAA,EACAE,aAAAA,EACAI,UAAAA,EACAe,cAAAA,EACAzB,cAAAA,EACAF,eAAAA,EACAC,OAAAA,EACAe,YAAAA,EACAzB,YAAAA,EACAwC,aAAc,QACf,iDACF,kDA3GD,IA2GC,4BAED,SAAe9E,EAAeC,GAO5B,MAAO,CAAEuC,UANSnF,EAChB0H,IAAAA,YACE/E,EAAM8D,MAAM7D,EAAS,GAAIA,EAAS,KAClC,IAIN,GAAC,6BAED,SAAgB+E,GAKd,IAJA,IAAIC,EAAY,EACZC,EAAgB,EACdnB,EAAwB,GACxBzB,EAAyC,CAAC,EACvCzC,EAAI,EAAGA,EAAImF,EAAWtG,OAAQmB,GAAK,EAC1C,IAAKmF,EAAWnF,GAAI,CAClB,GAAIqF,EAAgBrF,EAAG,CACrB,IAAIsC,EAAU6C,EAAWG,SAAS,OAAQD,EAAerF,GACzDsC,EAAUzC,KAAKmB,aAAasB,GAC5B4B,EAAYkB,GAAa9C,EACzBG,EAAYH,GAAW8C,C,CAEzBC,EAAgBrF,EAAI,EACpBoF,GAAa,C,CAGjB,MAAO,CAAE3C,YAAAA,EAAayB,YAAAA,EACxB,GAAC,qEAED,WACE5B,EACAvC,EACAwF,GAAW,8HAKV,OAJDtE,EAAAA,EAAAA,OAAAA,QAAAA,IAAAA,EAAAA,GAAAA,EAAAA,GAAgB,CAAC,EAEblB,EAAM,IACRA,EAAM,GACP,SAEuBF,KAAKqB,MAAMD,GAAK,OAAzB,GAATsB,EAAY,EAAH,KACC,CAAF,wCACL,IAAE,OAGwB,GAD7BC,EAAQD,EAAUE,YAAYH,GAC9BkD,EAAKjD,EAAUpB,QAAQqB,GACpB,CAAF,yCACE,IAAE,SAGOgD,EAAGR,YAAYnG,OAC7B2G,EAAGR,YACDjF,GAtMa,IAsMYyF,EAAGR,YAAYnG,OACpC2G,EAAGR,YAAYnG,OAAS,EACxBkB,GAxMS,IA0Mf,IAAIH,EAAc,EAAG,KAEvB6F,QAAQC,KAAK,4CAKTC,EAAkBzD,EAASnC,EAAKwF,GAChC9G,EAAkB,GAExB,UAC2BkH,GAAe,IAA1C,IAAK,EAAL,qBACE,IAD0C,qBAAhCrC,EAAK,KAAElB,EAAG,KACX7B,EAAM+C,EAAO/C,GAAO6B,EAAK7B,IAChC,GAAIiF,EAAGxD,SAASzB,GAEd,IADMqF,EAAYJ,EAAGxD,SAASzB,GACrBsF,EAAI,EAAGA,EAAID,EAAU/G,SAAUgH,EACtCpH,EAAOkB,KAAK,IAAIW,EAAMsF,EAAUC,GAAG3G,KAAM0G,EAAUC,GAAGpG,KAAMc,GAOpE,+BAKA,IAJMuF,EAAQN,EAAGR,YAAYnG,OACzBH,EAAS,KACPqH,EAASC,KAAKjG,IAAIA,GAAO,GAAI+F,EAAQ,GACrCG,EAASD,KAAKjG,IAAIwF,GAAO,GAAIO,EAAQ,GAClC9F,EAAI+F,EAAQ/F,GAAKiG,IAAUjG,GAC5BkG,EAAKV,EAAGR,YAAYhF,OAEnBtB,GAAUwH,EAAGxG,UAAUhB,GAAU,KACpCA,EAASwH,GAGd,yBAEM1H,EAAeC,EAAQC,IAAO,iDACtC,uDAlEA,MAkEA,EA9N6B,CAAQkC,G,WCjBlCuF,EAAa,SACbC,EAAa,SAKnB,SAASC,EAAOC,EAAaC,GAC3B,OAAOP,KAAKQ,MAAMF,EAAM,KAAH,IAAG,EAAKC,GAC/B,CAAC,IAEoBE,EAAI,0CAIvB,WAAYxG,GAAS,MAIF,OAJE,gBACnB,cAAMA,IACDuD,aAAe,EACpB,EAAKC,MAAQ,EACb,EAAKiD,SAAW,EAAC,CACnB,CAuPC,OAvPA,0EACD,WAAgBpE,GAAe,8FAAoB,OAAlBrB,EAAAA,EAAAA,OAAAA,QAAAA,IAAAA,EAAAA,GAAAA,EAAAA,GAAgB,CAAC,EAAC,SACzBpB,KAAKqB,MAAMD,GAAK,OAAzB,GAATsB,EAAY,EAAH,KACC,CAAF,yCACJ,GAAC,OAGyB,GAD9BC,EAAQD,EAAUE,YAAYH,GACxBC,EAAUpB,QAAQqB,GACpB,CAAF,0CACE,GAAC,QAEE,KAALE,EAAUH,EAAUpB,QAAQqB,GAA5BE,OACG,CAAF,yCACAA,EAAMC,WAAS,kCAEhB,GAAC,iDACV,mDAhBA,IAgBA,+DACD,yFACQ,IAAI5E,MAAM,uCAAsC,2CAEvD,kDAJA,IAIA,0BAED,SAAaoC,EAAeC,EAAgBuG,GAC1C,GAAIA,EAAY,GACd,MAAO,CACLzC,YAAa,GACbzB,YAAa,CAAC,GAIlB,IAAMQ,EAAc9C,EAAM6C,YAAY5C,GAChC8C,EACU,MAAdD,EAAwB,uBAAyB,iBAC7CE,EACJ,CAAE,EAAG,UAAW,EAAG,MAAO,EAAG,OAGf,GAAdF,GACF,IAAKE,EACH,MAAM,IAAIpF,MAAM,qCAAD,OAAsCkF,IAEvD,IAAMG,EAAgB,CACpBC,IAAKlD,EAAM6C,YAAY5C,EAAS,GAChCkD,MAAOnD,EAAM6C,YAAY5C,EAAS,GAClCgC,IAAKjC,EAAM6C,YAAY5C,EAAS,KAE5BmD,EAAYpD,EAAM6C,YAAY5C,EAAS,IACvCuD,EAAWJ,EAAYK,OAAOC,aAAaN,GAAa,GACxDO,EAAY3D,EAAM6C,YAAY5C,EAAS,IACvC2D,EAAoB5D,EAAM6C,YAAY5C,EAAS,IAErD,EAAqCP,KAAKmE,gBACxC7D,EAAM8D,MAAM7D,EAAS,GAAIA,EAAS,GAAK2D,IAGzC,MAAO,CACLG,YALiB,EAAXA,YAMNzB,YAN8B,EAAXA,YAOnBqB,UAAAA,EACAH,SAAAA,EACAP,cAAAA,EACAD,OAAAA,EACAD,eAAAA,EAEJ,GAAC,6BAED,SAAgBiC,GAKd,IAJA,IAAIC,EAAY,EACZC,EAAgB,EACdnB,EAAc,GACdzB,EAAyC,CAAC,EACvCzC,EAAI,EAAGA,EAAImF,EAAWtG,OAAQmB,GAAK,EAC1C,IAAKmF,EAAWnF,GAAI,CAClB,GAAIqF,EAAgBrF,EAAG,CACrB,IAAIsC,EAAU6C,EAAWG,SAAS,OAAQD,EAAerF,GACzDsC,EAAUzC,KAAKmB,aAAasB,GAC5B4B,EAAYkB,GAAa9C,EACzBG,EAAYH,GAAW8C,C,CAEzBC,EAAgBrF,EAAI,EACpBoF,GAAa,C,CAGjB,MAAO,CAAE3C,YAAAA,EAAayB,YAAAA,EACxB,GAEA,6DAEA,8HAC2B,OADdjD,EAAAA,EAAAA,OAAAA,QAAAA,IAAAA,EAAAA,GAAAA,EAAAA,GAAgB,CAAC,EAAC,KACT2B,EAAAA,MAAK,SAAQ/C,KAAKgB,WAAWgC,SAAS5B,GAAK,yDAApD,IAALd,EAAQ,EAAH,MAID2C,aAAa,KAAOqD,EAAU,iBACtCS,EAAa,EAAC,2BACLzG,EAAM2C,aAAa,KAAOsD,EAAU,iBAC7CQ,EAAa,EAAC,8BAER,IAAI7I,MAAM,kBAAiB,QAwDjC,OApDF8B,KAAK6G,SAAWvG,EAAM6C,YAAY,GAClCnD,KAAK4D,MAAQtD,EAAM6C,YAAY,GAC/BnD,KAAK2D,eAAiB,GAAyB,GAAlB3D,KAAK4D,MAAQ,IAAW,GAAK,EACpDC,EAAe,KAAH,IAAG,EAAM7D,KAAK6G,SAAwB,EAAb7G,KAAK4D,OAE1CkD,EAAYxG,EAAM6C,YAAY,IAChC6D,EAGA,CACF3C,YAAa,GACbzB,YAAa,CAAC,GAEZkE,IACFE,EAAMhH,KAAKiH,aAAa3G,EAAO,GAAIwG,IAE/B5D,EAAW5C,EAAM6C,YAAY,GAAK2D,GAIpCxC,EAAa,GAAKwC,EAAY,EAC5BxF,EAAU,IAAIiD,MAAMrB,GAAUnB,KAAK,GAAGyC,KAAI,WAE9C,IAAMC,EAAWnE,EAAM6C,YAAYmB,GACnCA,GAAc,EAGd,IAFA,IACIzB,EADEV,EAAuC,CAAC,EAErCuC,EAAI,EAAGA,EAAID,EAAUC,GAAK,EAAG,CACpC,IAAMhE,EAAMJ,EAAM2C,aAAaqB,GAC/B,GAAI5D,EAAM,EAAKiD,aAGbd,EAAQ,EAAK+B,eAAetE,EAAOgE,EAAa,GAChDA,GAAc,OACT,CACL,IAAM4C,EAAU7G,EAAUC,EAAOgE,EAAa,GAC9CU,EAAgB,EAAKC,eAAeD,EAAekC,GACnD,IAAMvC,EAAarE,EAAM6C,YAAYmB,EAAa,IAClDA,GAAc,GAEd,IADA,IAAM1F,EAAS,IAAI2F,MAAMI,GAChBE,EAAI,EAAGA,EAAIF,EAAYE,GAAK,EAAG,CACtC,IAAMC,EAAIzE,EAAUC,EAAOgE,GACrBS,EAAI1E,EAAUC,EAAOgE,EAAa,GACxCA,GAAc,GAEd1F,EAAOiG,GAAK,IAAIpE,EAAMqE,EAAGC,EAAGrE,E,CAE9ByB,EAASzB,GAAO9B,C,EAIpB,MAAO,CAAEuD,SAAAA,EAAUU,MAAAA,EACrB,IAAE,qCAGGmE,GAAG,IACNG,KAAK,EACLjE,SAAAA,EACAkC,aAAc,MACdJ,cAAAA,EACA+B,WAAAA,EACAzF,QAAAA,EACAsC,MAAO5D,KAAK4D,MACZD,aAAc3D,KAAK2D,aACnBE,aAAAA,KAAY,iDAEf,kDAlFD,IAkFC,4BAED,SAAevD,EAAeC,GAO5B,MAAO,CAAEuC,UANSnF,EAChB0H,IAAAA,YACEd,MAAM6C,UAAUhD,MAAMiD,KAAK/G,EAAOC,EAAS,GAAIA,EAAS,KACxD,IAIN,GAAC,qEAED,WACEkC,EACAvC,EACAwF,GAAW,kHAKV,OAJDtE,EAAAA,EAAAA,OAAAA,QAAAA,IAAAA,EAAAA,GAAAA,EAAAA,GAAgB,CAAC,EAEblB,EAAM,IACRA,EAAM,GACP,SAEuBF,KAAKqB,MAAMD,GAAK,OAAzB,GAATsB,EAAY,EAAH,KACC,CAAF,wCACL,IAAE,OAGwB,GAD7BC,EAAQD,EAAUE,YAAYH,GAC9BkD,EAAKjD,EAAUpB,QAAQqB,GACpB,CAAF,yCACE,IAAE,QAKLmD,EAAkB9F,KAAKqC,SAASnC,EAAKwF,GACrC9G,EAAkB,GAExB,UAC2BkH,GAAe,IAA1C,IAAK,EAAL,qBACE,IAD0C,qBAAhCrC,EAAK,KAAElB,EAAG,KACX7B,EAAM+C,EAAO/C,GAAO6B,EAAK7B,IAChC,GAAIiF,EAAGxD,SAASzB,GAEd,IADMqF,EAAYJ,EAAGxD,SAASzB,GACrBsF,EAAI,EAAGA,EAAID,EAAU/G,SAAUgH,EACtCpH,EAAOkB,KAAK,IAAIW,EAAMsF,EAAUC,GAAG3G,KAAM0G,EAAUC,GAAGpG,KAAMc,GAInE,wDAEM/B,EAAeC,EAAQ,IAAImB,EAAc,EAAG,KAAG,iDACvD,uDAxCA,IA0CD,sBAGA,SAASuC,EAAaC,IACpBD,GAAO,GACG,IACRA,EAAM,GAEJC,EAAM,KAAH,IAAG,EAAK,MACbA,EAAM,KAAH,IAAG,EAAK,KAEbA,GAAO,EAKP,IAJA,IAxPyBmE,EAwPrBY,EAAI,EACJC,EAAI,EACJC,EAAIxH,KAAK6G,SAAwB,EAAb7G,KAAK4D,MACvB6D,EAAO,GACNH,GAAKtH,KAAK4D,MAAO4D,GAAK,EAAGD,IA5PPb,EA4P0B,EAAJY,EAAH,EA3PjC,KAAH,IAAG,EAAKZ,IA2PuCY,GAAK,EAAG,CAC7D,IAAMrH,EAAIsH,EAAIf,EAAOlE,EAAKkF,GACpB/I,EAAI8I,EAAIf,EAAOjE,EAAKiF,GAC1B,GAAI/I,EAAIwB,EAAIwH,EAAKzI,OAASgB,KAAK2D,aAC7B,MAAM,IAAIzF,MAAM,SAAD,OACJoE,EAAG,YAAIC,EAAG,2DAAmDvC,KAAK6G,SAAQ,mBAAW7G,KAAK4D,MAAK,6DAG5G6D,EAAK3H,KAAK,CAACG,EAAGxB,G,CAEhB,OAAOgJ,CACT,KAAC,EAhQsB,CAAQ1G,G,cCAjC,SAAS2G,EAAQC,GACf,OAAO,IAAIC,SAAQ,SAAAC,GACjBC,WAAWD,EAASF,EACtB,GACF,CAAC,IACoBI,EAAgB,WAqBnC,cAoBC,IAnBCC,EAAI,EAAJA,KACAhH,EAAU,EAAVA,WACAiH,EAAO,EAAPA,QACAC,EAAa,EAAbA,cACAC,EAAO,EAAPA,QACAC,EAAa,EAAbA,cAAa,IACbC,eAAAA,OAAc,IAAG,MAAQ,MACzBpH,cAAAA,OAAa,IAAG,WAAAC,GAAC,OAAIA,CAAC,QACtBoH,eAAAA,OAAc,IAAG,IAAI,KAAH,IAAG,EAAK,IAAE,EAY5B,IAZ4B,eAYxBtH,EACFhB,KAAKgB,WAAaA,MACb,KAAIgH,EAGT,MAAM,IAAIO,UAAU,0CAFpBvI,KAAKgB,WAAa,IAAIwH,EAAAA,GAAUR,E,CAKlC,GAAIE,EACFlI,KAAKyI,MAAQ,IAAIC,EAAI,CACnB1H,WAAYkH,EACZjH,cAAAA,SAEG,GAAImH,EACTpI,KAAKyI,MAAQ,IAAI7B,EAAI,CACnB5F,WAAYoH,EACZnH,cAAAA,SAEG,GAAIgH,EACTjI,KAAKyI,MAAQ,IAAIC,EAAI,CACnB1H,WAAY,IAAIwH,EAAAA,GAAUP,GAC1BhH,cAAAA,SAEG,GAAIkH,EACTnI,KAAKyI,MAAQ,IAAI7B,EAAI,CACnB5F,WAAY,IAAIwH,EAAAA,GAAUL,GAC1BlH,cAAAA,QAEG,KAAI+G,EAMT,MAAM,IAAIO,UACR,yEANFvI,KAAKyI,MAAQ,IAAIC,EAAI,CACnB1H,WAAY,IAAIwH,EAAAA,GAAU,GAAD,OAAIR,EAAI,SACjC/G,cAAAA,G,CAQJjB,KAAKqI,eAAiBA,EACtBrI,KAAKmB,aAAeF,EACpBjB,KAAK2I,WAAa,IAAIhH,IAAJ,CAA0B,CAC1CC,MAAO,IAAIgH,IAAJ,CAAQ,CACb9G,QAASqE,KAAKQ,MAAM2B,EAAiB,SAGvCvG,KAAM/B,KAAK6I,UAAUC,KAAK9I,OAE9B,CAmZC,OAjZD,yEAOA,WACEyC,EACAgB,EACAlB,EACAnB,GAAqC,wHAGZ,GAArB2H,EAAmB,CAAC,EAEJ,qBAAT3H,EAAoB,sBACvB,IAAImH,UAAU,kCAAiC,OAOtD,GALmB,oBAATnH,EACT4H,EAAW5H,GAEX2H,EAAU3H,EACV4H,EAAW5H,EAAK6H,mBAEFrI,IAAZ6B,EAAqB,sBACjB,IAAI8F,UAAU,0CAAyC,UAE1DS,EAAU,CAAF,qBACL,IAAIT,UAAU,kCAAiC,wBAGhCvI,KAAKyI,MAAMS,YAAYH,GAAQ,QAOrD,GAPKI,EAAW,EAAH,KACd9K,EAAiBC,GACZmF,IACHA,EAAQ,GAELlB,IACHA,EAAM4G,EAAStF,cAEXJ,GAASlB,EAAG,uBACV,IAAIgG,UACR,8EACD,WAEC9E,IAAUlB,EAAG,oEAIIvC,KAAKyI,MAAMW,eAAe3G,EAASgB,EAAOlB,EAAKwG,GAAQ,QAAtEnK,EAAS,EAAH,KACZP,EAAiBC,GAIR6B,EAAI,EAAC,aAAEA,EAAIvB,EAAOI,QAAM,iBACK,MAA9BqK,EAAOzK,EAAOuB,GAAGQ,eACZX,KAAKqI,gBAAc,uBACtB,IAAInK,MAAM,6BAAD,OACgBmL,EAAKC,iBAAgB,4CAAoCtJ,KAAKqI,eAAeiB,iBAAgB,MAC3H,QAL8BnJ,GAAK,EAAC,wBAUrCoJ,EAAOC,KAAKC,MACPC,EAAW,EAAC,aAAEA,EAAW9K,EAAOI,QAAM,iBAEnB,OADtB2K,OAA2C,EACzC3D,EAAIpH,EAAO8K,GAAS,UACuB1J,KAAK2I,WAAW1G,IAC/D+D,EAAEP,WACFO,EACA1H,GACD,iBAJOsL,EAAM,EAANA,OAAQC,EAAU,EAAVA,WAAYC,EAAU,EAAVA,YAMtBC,GACmB,qBAAhBC,YACH,IAAIA,YAAY,SAASC,OAAOL,GAChCA,EAAOnE,YACXyE,MAAM,OACFC,MAEN9L,EAAiBC,GACb8L,EAAapE,EAAE3G,KAAKE,aACpB8K,OAAG,EAEElK,EAAI,EAAC,aAAEA,EAAI4J,EAAM/K,QAAM,iBAG9B,IAFMsL,EAAOP,EAAM5J,GAEdkK,EAAM,EAAGD,GAAcN,EAAWO,GAAMA,GAAO,GAWpD,GATA,EACsCrK,KAAKuK,UACzCpB,EACA1G,EACAgB,EACAlB,EACA+H,GALME,EAAe,EAAfA,gBAAiBC,EAAQ,EAARA,gBAUK7J,IAA5B+I,QACoB/I,IAApB4J,GACAb,EAA0Ba,GAAe,uBAEnC,IAAItM,MAAM,yCAAD,OAC4ByL,EAAuB,cAAMa,EAAe,2CACtF,QAEsC,GAAzCb,EAA0Ba,GAEtBC,EAAU,CAAF,gBACVzB,EACEsB,EAAKI,OASa,IAAlBb,EAAWQ,IAAmBD,EAAaN,EAAWO,KACvD,kCAC4BzJ,IAApB4J,GAAiCA,GAAmBjI,GAAG,mDAQlE,GAFA6H,GAAcE,EAAKtL,OAAS,IAGxBuK,EAAOC,KAAKC,MAAQ,KAAG,iBAED,OADxBF,EAAOC,KAAKC,MACZpL,EAAiBC,GAAO,UAClBoJ,EAAQ,GAAE,QAnDcvH,GAAK,EAAC,wBApBOuJ,GAAY,EAAC,iEA2E/D,yDA5ID,IA4IC,kEAED,qGAAoC,OAAlBtI,EAAAA,EAAAA,OAAAA,QAAAA,IAAAA,EAAAA,GAAAA,EAAAA,GAAgB,CAAC,EAAC,kBAC3BpB,KAAKyI,MAAMS,YAAY9H,IAAK,gDACpC,kDAJA,IAMD,sEAOA,yHAAwC,OAAlBA,EAAAA,EAAAA,OAAAA,QAAAA,IAAAA,EAAAA,GAAAA,EAAAA,GAAgB,CAAC,EAAC,SACkBpB,KAAKkJ,YAC3D9H,GACD,OAOD,OAPC,SAFO4D,EAAa,EAAbA,cAAelB,EAAQ,EAARA,SAAUsB,EAAY,EAAZA,aAGjC/G,EAAiB+C,EAAK9C,QAChBqM,EACJ3F,GAAiBA,EAAc1F,cAC3B0F,EAAc1F,cAAgB8F,EAC9BA,EAEN,UAEkBpF,KAAK4K,YAAY,EAAGD,EAAUvJ,GAAK,QACxB,OADzBd,EAAQ,EAAH,KACTjC,EAAiB+C,EAAK9C,QAAO,qBAEbyE,EAAAA,EAAAA,OAAMzC,GAAM,QAA1BA,EAAQ,EAAH,6BAEW,MAFX,2BAELsF,QAAQiF,MAAM,EAAD,IACP,IAAI3M,MACR,oCAC6B,KAAEQ,KAAI,yBAAiBiM,EAAQ,oBAC7D,YAIC7G,EAAU,CAAF,gBAENgH,GAAe,EACbC,EAAc,KAAKC,WAAW,GAC9BC,EAAWnH,EAASkH,WAAW,GAC5B7K,EAAI,EAAC,aAAEA,EAAIG,EAAMtB,QAAM,oBAC1BmB,IAAM2K,EAAc,GAAKxK,EAAMH,KAAO8K,EAAQ,qDAG9C3K,EAAMH,KAAO4K,IACfD,EAAc3K,GACf,QAN+BA,GAAK,EAAC,wBAQxCG,EAAQA,EAAM8D,MAAM,EAAG0G,EAAc,GAAE,iCAElCxK,GAAK,2DACb,kDAhDD,IAkDA,gEAMA,uGAAkC,OAAlBc,EAAAA,EAAAA,OAAAA,QAAAA,IAAAA,EAAAA,GAAAA,EAAAA,GAAgB,CAAC,EAAC,SACZpB,KAAKkL,gBAAgB9J,GAAK,OACjB,OADvBd,EAAQ,EAAH,KACXjC,EAAiB+C,EAAK9C,QAAO,kBACtBgC,EAAMmF,SAAS,SAAO,gDAC9B,kDAVD,IAYA,gFAQA,uGAAkD,OAAlBrE,EAAAA,EAAAA,OAAAA,QAAAA,IAAAA,EAAAA,GAAAA,EAAAA,GAAgB,CAAC,EAAC,SACzBpB,KAAKkJ,YAAY9H,GAAK,OAA/B,OAAR+H,EAAW,EAAH,uBACPA,EAAS9E,aAAW,gDAC5B,kDAXD,IAaA,uBAUA,WAYE8G,EACAC,EACAC,EACAf,GAAY,IAbV/G,EAAa,EAAbA,cACAO,EAAQ,EAARA,SACAT,EAAc,EAAdA,eACAC,EAAM,EAANA,OAaF,GAAIgH,EAAKgB,OAAO,KAAOxH,EACrB,MAAO,CAAE2G,UAAU,GAIrB,IAAMjH,EAAoBD,EAApBC,IAAKC,EAAeF,EAAfE,MAAOlB,EAAQgB,EAARhB,IACbiB,IACHA,EAAM,GAEHC,IACHA,EAAQ,GAELlB,IACHA,EAAM,GAEO,QAAXe,IACFf,EAAM,GAYR,IAVA,IAAMgJ,EAAYpF,KAAKT,IAAIlC,EAAKC,EAAOlB,GAMnCiJ,EAAsB,EACtBC,EAAqB,EACrBC,EAAS,GACTlB,GAAmBmB,IACdxL,EAAI,EAAGA,EAAImK,EAAKtL,OAAS,EAAGmB,GAAK,EACxC,GAAgB,OAAZmK,EAAKnK,IAAeA,IAAMmK,EAAKtL,OAAQ,CACzC,GAAIwM,IAAwBhI,GAC1B,GACExD,KAAKmB,aAAamJ,EAAKlG,MAAMqH,EAAoBtL,MACjDgL,EAEA,MAAO,CAAEV,UAAU,QAEhB,GAAIe,IAAwB/H,EAAO,CAMxC,GALA+G,EAAkBoB,SAAStB,EAAKlG,MAAMqH,EAAoBtL,GAAI,IAEvC,mBAAnBkD,IACFmH,GAAmB,GAEjBA,GAAmBa,EACrB,MAAO,CAAEb,gBAAAA,EAAiBC,UAAU,GAEtC,IAAY,IAARlI,GAAaA,IAAQkB,IAEnB+G,EAAkB,GAAKY,EACzB,MAAO,CAAEZ,gBAAAA,EAAiBC,UAAU,E,MAGnC,GAAe,QAAXnH,GAA4C,IAAxBkI,EAC7BE,EAASpB,EAAKlG,MAAMqH,EAAoBtL,QACnC,GAAIqL,IAAwBjJ,EAAK,CAYtC,IATe,QAAXe,EACctD,KAAK6L,WACnBrB,EACAkB,EACApB,EAAKlG,MAAMqH,EAAoBtL,IAGjByL,SAAStB,EAAKlG,MAAMqH,EAAoBtL,GAAI,MAEzCiL,EACnB,MAAO,CAAEX,UAAU,E,CAKvB,GAFAgB,EAAqBtL,EAAI,GACzBqL,GAAuB,GACGD,EACxB,K,CAIN,MAAO,CAAEf,gBAAAA,EAAiBC,UAAU,EACtC,GAAC,wBAED,SAAWD,EAAyBkB,EAAgBI,GAClD,IAAIC,EAAgBvB,EAAkBkB,EAAO1M,OAMvCgN,GAAwC,IAAhCF,EAAKG,QAAQ,cAC3B,GAAgB,MAAZH,EAAK,IAAeE,GAajB,GAAIA,EACT,OAAOxB,EAAkB,OAZzB,IADA,IAAI0B,EAAW,IACNxH,EAAI,EAAGA,EAAIoH,EAAK9M,OAAQ0F,GAAK,EAAG,CACvC,GAAiB,MAAbwH,GAA6C,SAAzBJ,EAAK1H,MAAMM,EAAGA,EAAI,GAAe,CACvD,IAAIyH,EAAWL,EAAKG,QAAQ,IAAKvH,IACf,IAAdyH,IACFA,EAAWL,EAAK9M,QAElB+M,EAAgBH,SAASE,EAAK1H,MAAMM,EAAI,EAAGyH,GAAW,IACtD,K,CAEFD,EAAWJ,EAAKpH,E,CAKpB,OAAOqH,CACT,GAEA,gEAKA,WAAgBtJ,GAAe,wFAAoB,OAAlBrB,EAAAA,EAAAA,OAAAA,QAAAA,IAAAA,EAAAA,GAAAA,EAAAA,GAAgB,CAAC,EAAC,kBAC1CpB,KAAKyI,MAAM3F,UAAUL,EAASrB,IAAK,gDAC3C,mDAPD,IAOC,kEAED,WACEgL,EACAC,GAAsB,8FACJ,OAAlBjL,EAAAA,EAAAA,OAAAA,QAAAA,IAAAA,EAAAA,GAAAA,EAAAA,GAAgB,CAAC,EAAC,SAEkBpB,KAAKgB,WAAWsL,KAClDC,EAAOC,MAAMH,GACb,EACAA,EACAD,EACAhL,GACD,OANwB,OAMxB,SANOqL,EAAS,EAATA,UAAW7C,EAAM,EAANA,OAAM,kBAQlB6C,EAAYJ,EAAiBzC,EAAOxF,MAAM,EAAGqI,GAAa7C,GAAM,gDACxE,qDAhBA,IAkBD,gEAMA,WAAgBnK,GAAY,0FAAoB,OAAlB2B,EAAAA,EAAAA,OAAAA,QAAAA,IAAAA,EAAAA,GAAAA,EAAAA,GAAgB,CAAC,EAAC,SAIjBpB,KAAK4K,YAChCnL,EAAMJ,KAAKC,cACXG,EAAMkB,cACNS,GACD,OAJmB,OAAdsL,EAAiB,EAAH,iCAMXC,EAAAA,EAAAA,iBAAgBD,EAAgBjN,IAAM,sCAEvC,IAAIvB,MAAM,6BAAD,OAA8BuB,EAAMgG,WAAU,mBAAQ,yDAExE,mDApBD,MAoBC,EA7ekC,E","sources":["../../../node_modules/@gmod/tabix/src/util.ts","../../../node_modules/@gmod/tabix/src/virtualOffset.ts","../../../node_modules/@gmod/tabix/src/chunk.ts","../../../node_modules/@gmod/tabix/src/indexFile.ts","../../../node_modules/@gmod/tabix/src/tbi.ts","../../../node_modules/@gmod/tabix/src/csi.ts","../../../node_modules/@gmod/tabix/src/tabixIndexedFile.ts"],"sourcesContent":["import Chunk from './chunk'\nimport VirtualOffset from './virtualOffset'\n\nexport function longToNumber(long: Long) {\n  if (\n    long.greaterThan(Number.MAX_SAFE_INTEGER) ||\n    long.lessThan(Number.MIN_SAFE_INTEGER)\n  ) {\n    throw new Error('integer overflow')\n  }\n  return long.toNumber()\n}\n\nclass AbortError extends Error {\n  public code: string | undefined\n}\n/**\n * Properly check if the given AbortSignal is aborted.\n * Per the standard, if the signal reads as aborted,\n * this function throws either a DOMException AbortError, or a regular error\n * with a `code` attribute set to `ERR_ABORTED`.\n *\n * For convenience, passing `undefined` is a no-op\n *\n * @param {AbortSignal} [signal] an AbortSignal, or anything with an `aborted` attribute\n * @returns nothing\n */\nexport function checkAbortSignal(signal?: AbortSignal) {\n  if (!signal) {\n    return\n  }\n\n  if (signal.aborted) {\n    // console.log('bam aborted!')\n    if (typeof DOMException !== 'undefined') {\n      // eslint-disable-next-line  no-undef\n      throw new DOMException('aborted', 'AbortError')\n    } else {\n      const e = new AbortError('aborted')\n      e.code = 'ERR_ABORTED'\n      throw e\n    }\n  }\n}\n\n/**\n * Skips to the next tick, then runs `checkAbortSignal`.\n * Await this to inside an otherwise synchronous loop to\n * provide a place to break when an abort signal is received.\n * @param {AbortSignal} signal\n */\nexport async function abortBreakPoint(signal?: AbortSignal) {\n  await Promise.resolve()\n  checkAbortSignal(signal)\n}\n\nexport function canMergeBlocks(chunk1: Chunk, chunk2: Chunk) {\n  return (\n    chunk2.minv.blockPosition - chunk1.maxv.blockPosition < 65000 &&\n    chunk2.maxv.blockPosition - chunk1.minv.blockPosition < 5000000\n  )\n}\n\nexport function optimizeChunks(chunks: Chunk[], lowest: VirtualOffset) {\n  const mergedChunks: Chunk[] = []\n  let lastChunk: Chunk | null = null\n\n  if (chunks.length === 0) {\n    return chunks\n  }\n\n  chunks.sort(function (c0, c1) {\n    const dif = c0.minv.blockPosition - c1.minv.blockPosition\n    if (dif !== 0) {\n      return dif\n    } else {\n      return c0.minv.dataPosition - c1.minv.dataPosition\n    }\n  })\n\n  chunks.forEach(chunk => {\n    if (!lowest || chunk.maxv.compareTo(lowest) > 0) {\n      if (lastChunk === null) {\n        mergedChunks.push(chunk)\n        lastChunk = chunk\n      } else {\n        if (canMergeBlocks(lastChunk, chunk)) {\n          if (chunk.maxv.compareTo(lastChunk.maxv) > 0) {\n            lastChunk.maxv = chunk.maxv\n          }\n        } else {\n          mergedChunks.push(chunk)\n          lastChunk = chunk\n        }\n      }\n    }\n    // else {\n    //   console.log(`skipping chunk ${chunk}`)\n    // }\n  })\n\n  return mergedChunks\n}\n","export default class VirtualOffset {\n  public blockPosition: number\n  public dataPosition: number\n  constructor(blockPosition: number, dataPosition: number) {\n    this.blockPosition = blockPosition // < offset of the compressed data block\n    this.dataPosition = dataPosition // < offset into the uncompressed data\n  }\n\n  toString() {\n    return `${this.blockPosition}:${this.dataPosition}`\n  }\n\n  compareTo(b: VirtualOffset) {\n    return (\n      this.blockPosition - b.blockPosition || this.dataPosition - b.dataPosition\n    )\n  }\n\n  static min(...args: VirtualOffset[]) {\n    let min\n    let i = 0\n    for (; !min; i += 1) {\n      min = args[i]\n    }\n    for (; i < args.length; i += 1) {\n      if (min.compareTo(args[i]) > 0) {\n        min = args[i]\n      }\n    }\n    return min\n  }\n}\nexport function fromBytes(bytes: Buffer, offset = 0, bigendian = false) {\n  if (bigendian) {\n    throw new Error('big-endian virtual file offsets not implemented')\n  }\n\n  return new VirtualOffset(\n    bytes[offset + 7] * 0x10000000000 +\n      bytes[offset + 6] * 0x100000000 +\n      bytes[offset + 5] * 0x1000000 +\n      bytes[offset + 4] * 0x10000 +\n      bytes[offset + 3] * 0x100 +\n      bytes[offset + 2],\n    (bytes[offset + 1] << 8) | bytes[offset],\n  )\n}\n","import VirtualOffset from './virtualOffset'\n\n// little class representing a chunk in the index\nexport default class Chunk {\n  public minv: VirtualOffset\n  public maxv: VirtualOffset\n  public bin: number\n  public _fetchedSize?: number\n\n  /**\n   * @param {VirtualOffset} minv\n   * @param {VirtualOffset} maxv\n   * @param {number} bin\n   * @param {number} [fetchedSize]\n   */\n  constructor(\n    minv: VirtualOffset,\n    maxv: VirtualOffset,\n    bin: number,\n    fetchedSize = undefined,\n  ) {\n    this.minv = minv\n    this.maxv = maxv\n    this.bin = bin\n    this._fetchedSize = fetchedSize\n  }\n\n  toUniqueString() {\n    return `${this.minv}..${this.maxv} (bin ${\n      this.bin\n    }, fetchedSize ${this.fetchedSize()})`\n  }\n\n  toString() {\n    return this.toUniqueString()\n  }\n\n  compareTo(b: Chunk) {\n    return (\n      this.minv.compareTo(b.minv) ||\n      this.maxv.compareTo(b.maxv) ||\n      this.bin - b.bin\n    )\n  }\n\n  fetchedSize() {\n    if (this._fetchedSize !== undefined) {\n      return this._fetchedSize\n    }\n    return this.maxv.blockPosition + (1 << 16) - this.minv.blockPosition\n  }\n}\n","import AbortablePromiseCache from 'abortable-promise-cache'\nimport QuickLRU from 'quick-lru'\nimport { GenericFilehandle } from 'generic-filehandle'\nimport VirtualOffset from './virtualOffset'\nimport Chunk from './chunk'\n\nexport interface Options {\n  // support having some unknown parts of the options\n  [key: string]: unknown\n  signal?: AbortSignal\n}\n\nexport default abstract class IndexFile {\n  public filehandle: GenericFilehandle\n  public renameRefSeq: (arg0: string) => string\n  private _parseCache: any\n\n  /**\n   * @param {filehandle} filehandle\n   * @param {function} [renameRefSeqs]\n   */\n  constructor({\n    filehandle,\n    renameRefSeqs = (n: string) => n,\n  }: {\n    filehandle: GenericFilehandle\n    renameRefSeqs?: (a: string) => string\n  }) {\n    this.filehandle = filehandle\n    this.renameRefSeq = renameRefSeqs\n  }\n\n  public abstract lineCount(refName: string, args: Options): Promise<number>\n\n  protected abstract _parse(opts: Options): Promise<{\n    refNameToId: { [key: string]: number }\n    refIdToName: string[]\n  }>\n\n  public async getMetadata(opts: Options = {}) {\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    const { indices, ...rest } = await this.parse(opts)\n    return rest\n  }\n\n  public abstract blocksForRange(\n    refName: string,\n    start: number,\n    end: number,\n    opts: Options,\n  ): Promise<Chunk[]>\n\n  _findFirstData(\n    currentFdl: VirtualOffset | undefined,\n    virtualOffset: VirtualOffset,\n  ) {\n    if (currentFdl) {\n      return currentFdl.compareTo(virtualOffset) > 0\n        ? virtualOffset\n        : currentFdl\n    } else {\n      return virtualOffset\n    }\n  }\n\n  async parse(opts: Options = {}) {\n    if (!this._parseCache) {\n      this._parseCache = new AbortablePromiseCache({\n        cache: new QuickLRU({ maxSize: 1 }),\n        fill: () => this._parse(opts),\n      })\n    }\n    return this._parseCache.get('index', null, undefined)\n  }\n\n  async hasRefSeq(seqId: number, opts: Options = {}) {\n    return !!((await this.parse(opts)).indices[seqId] || {}).binIndex\n  }\n}\n","import Long from 'long'\nimport VirtualOffset, { fromBytes } from './virtualOffset'\nimport Chunk from './chunk'\nimport { unzip } from '@gmod/bgzf-filehandle'\nimport { longToNumber, optimizeChunks, checkAbortSignal } from './util'\nimport IndexFile, { Options } from './indexFile'\n\nconst TBI_MAGIC = 21578324 // TBI\\1\nconst TAD_LIDX_SHIFT = 14\n\n/**\n * calculate the list of bins that may overlap with region [beg,end) (zero-based half-open)\n */\nfunction reg2bins(beg: number, end: number) {\n  beg += 1 // < convert to 1-based closed\n  end -= 1\n  return [\n    [0, 0],\n    [1 + (beg >> 26), 1 + (end >> 26)],\n    [9 + (beg >> 23), 9 + (end >> 23)],\n    [73 + (beg >> 20), 73 + (end >> 20)],\n    [585 + (beg >> 17), 585 + (end >> 17)],\n    [4681 + (beg >> 14), 4681 + (end >> 14)],\n  ]\n}\n\nexport default class TabixIndex extends IndexFile {\n  async lineCount(refName: string, opts: Options = {}) {\n    const indexData = await this.parse(opts)\n    if (!indexData) {\n      return -1\n    }\n    const refId = indexData.refNameToId[refName]\n    const idx = indexData.indices[refId]\n    if (!idx) {\n      return -1\n    }\n    const { stats } = indexData.indices[refId]\n    if (stats) {\n      return stats.lineCount\n    }\n    return -1\n  }\n\n  // memoize\n  // fetch and parse the index\n  async _parse(opts: Options = {}) {\n    const bytes = await unzip((await this.filehandle.readFile(opts)) as Buffer)\n    checkAbortSignal(opts.signal)\n\n    // check TBI magic numbers\n    if (bytes.readUInt32LE(0) !== TBI_MAGIC /* \"TBI\\1\" */) {\n      throw new Error('Not a TBI file')\n      // TODO: do we need to support big-endian TBI files?\n    }\n\n    // number of reference sequences in the index\n    const refCount = bytes.readInt32LE(4)\n    const formatFlags = bytes.readInt32LE(8)\n    const coordinateType =\n      formatFlags & 0x10000 ? 'zero-based-half-open' : '1-based-closed'\n    const formatOpts: { [key: number]: string } = {\n      0: 'generic',\n      1: 'SAM',\n      2: 'VCF',\n    }\n    const format = formatOpts[formatFlags & 0xf]\n    if (!format) {\n      throw new Error(`invalid Tabix preset format flags ${formatFlags}`)\n    }\n    const columnNumbers = {\n      ref: bytes.readInt32LE(12),\n      start: bytes.readInt32LE(16),\n      end: bytes.readInt32LE(20),\n    }\n    const metaValue = bytes.readInt32LE(24)\n    const depth = 5\n    const maxBinNumber = ((1 << ((depth + 1) * 3)) - 1) / 7\n    const maxRefLength = 2 ** (14 + depth * 3)\n    const metaChar = metaValue ? String.fromCharCode(metaValue) : null\n    const skipLines = bytes.readInt32LE(28)\n\n    // read sequence dictionary\n    const nameSectionLength = bytes.readInt32LE(32)\n    const { refNameToId, refIdToName } = this._parseNameBytes(\n      bytes.slice(36, 36 + nameSectionLength),\n    )\n\n    // read the indexes for each reference sequence\n    let currOffset = 36 + nameSectionLength\n    let firstDataLine: VirtualOffset | undefined\n    const indices = new Array(refCount).fill(0).map(() => {\n      // the binning index\n      const binCount = bytes.readInt32LE(currOffset)\n      currOffset += 4\n      const binIndex: { [key: number]: Chunk[] } = {}\n      let stats\n      for (let j = 0; j < binCount; j += 1) {\n        const bin = bytes.readUInt32LE(currOffset)\n        currOffset += 4\n        if (bin > maxBinNumber + 1) {\n          throw new Error(\n            'tabix index contains too many bins, please use a CSI index',\n          )\n        } else if (bin === maxBinNumber + 1) {\n          const chunkCount = bytes.readInt32LE(currOffset)\n          currOffset += 4\n          if (chunkCount === 2) {\n            stats = this.parsePseudoBin(bytes, currOffset)\n          }\n          currOffset += 16 * chunkCount\n        } else {\n          const chunkCount = bytes.readInt32LE(currOffset)\n          currOffset += 4\n          const chunks = new Array(chunkCount)\n          for (let k = 0; k < chunkCount; k += 1) {\n            const u = fromBytes(bytes, currOffset)\n            const v = fromBytes(bytes, currOffset + 8)\n            currOffset += 16\n            firstDataLine = this._findFirstData(firstDataLine, u)\n            chunks[k] = new Chunk(u, v, bin)\n          }\n          binIndex[bin] = chunks\n        }\n      }\n\n      // the linear index\n      const linearCount = bytes.readInt32LE(currOffset)\n      currOffset += 4\n      const linearIndex = new Array(linearCount)\n      for (let k = 0; k < linearCount; k += 1) {\n        linearIndex[k] = fromBytes(bytes, currOffset)\n        currOffset += 8\n        firstDataLine = this._findFirstData(firstDataLine, linearIndex[k])\n      }\n      return { binIndex, linearIndex, stats }\n    })\n\n    return {\n      indices,\n      metaChar,\n      maxBinNumber,\n      maxRefLength,\n      skipLines,\n      firstDataLine,\n      columnNumbers,\n      coordinateType,\n      format,\n      refIdToName,\n      refNameToId,\n      maxBlockSize: 1 << 16,\n    }\n  }\n\n  parsePseudoBin(bytes: Buffer, offset: number) {\n    const lineCount = longToNumber(\n      Long.fromBytesLE(\n        bytes.slice(offset + 16, offset + 24) as unknown as number[],\n        true,\n      ),\n    )\n    return { lineCount }\n  }\n\n  _parseNameBytes(namesBytes: Buffer) {\n    let currRefId = 0\n    let currNameStart = 0\n    const refIdToName: string[] = []\n    const refNameToId: { [key: string]: number } = {}\n    for (let i = 0; i < namesBytes.length; i += 1) {\n      if (!namesBytes[i]) {\n        if (currNameStart < i) {\n          let refName = namesBytes.toString('utf8', currNameStart, i)\n          refName = this.renameRefSeq(refName)\n          refIdToName[currRefId] = refName\n          refNameToId[refName] = currRefId\n        }\n        currNameStart = i + 1\n        currRefId += 1\n      }\n    }\n    return { refNameToId, refIdToName }\n  }\n\n  async blocksForRange(\n    refName: string,\n    min: number,\n    max: number,\n    opts: Options = {},\n  ) {\n    if (min < 0) {\n      min = 0\n    }\n\n    const indexData = await this.parse(opts)\n    if (!indexData) {\n      return []\n    }\n    const refId = indexData.refNameToId[refName]\n    const ba = indexData.indices[refId]\n    if (!ba) {\n      return []\n    }\n\n    const minOffset = ba.linearIndex.length\n      ? ba.linearIndex[\n          min >> TAD_LIDX_SHIFT >= ba.linearIndex.length\n            ? ba.linearIndex.length - 1\n            : min >> TAD_LIDX_SHIFT\n        ]\n      : new VirtualOffset(0, 0)\n    if (!minOffset) {\n      console.warn('querying outside of possible tabix range')\n    }\n\n    // const { linearIndex, binIndex } = indexes\n\n    const overlappingBins = reg2bins(min, max) // List of bin #s that overlap min, max\n    const chunks: Chunk[] = []\n\n    // Find chunks in overlapping bins.  Leaf bins (< 4681) are not pruned\n    for (const [start, end] of overlappingBins) {\n      for (let bin = start; bin <= end; bin++) {\n        if (ba.binIndex[bin]) {\n          const binChunks = ba.binIndex[bin]\n          for (let c = 0; c < binChunks.length; ++c) {\n            chunks.push(new Chunk(binChunks[c].minv, binChunks[c].maxv, bin))\n          }\n        }\n      }\n    }\n\n    // Use the linear index to find minimum file position of chunks that could\n    // contain alignments in the region\n    const nintv = ba.linearIndex.length\n    let lowest = null\n    const minLin = Math.min(min >> 14, nintv - 1)\n    const maxLin = Math.min(max >> 14, nintv - 1)\n    for (let i = minLin; i <= maxLin; ++i) {\n      const vp = ba.linearIndex[i]\n      if (vp) {\n        if (!lowest || vp.compareTo(lowest) < 0) {\n          lowest = vp\n        }\n      }\n    }\n\n    return optimizeChunks(chunks, lowest)\n  }\n}\n","import Long from 'long'\nimport { unzip } from '@gmod/bgzf-filehandle'\n\nimport VirtualOffset, { fromBytes } from './virtualOffset'\nimport Chunk from './chunk'\nimport { longToNumber, optimizeChunks } from './util'\n\nimport IndexFile, { Options } from './indexFile'\n\nconst CSI1_MAGIC = 21582659 // CSI\\1\nconst CSI2_MAGIC = 38359875 // CSI\\2\n\nfunction lshift(num: number, bits: number) {\n  return num * 2 ** bits\n}\nfunction rshift(num: number, bits: number) {\n  return Math.floor(num / 2 ** bits)\n}\n\nexport default class CSI extends IndexFile {\n  private maxBinNumber: number\n  private depth: number\n  private minShift: number\n  constructor(args: any) {\n    super(args)\n    this.maxBinNumber = 0\n    this.depth = 0\n    this.minShift = 0\n  }\n  async lineCount(refName: string, opts: Options = {}): Promise<number> {\n    const indexData = await this.parse(opts)\n    if (!indexData) {\n      return -1\n    }\n    const refId = indexData.refNameToId[refName]\n    const idx = indexData.indices[refId]\n    if (!idx) {\n      return -1\n    }\n    const { stats } = indexData.indices[refId]\n    if (stats) {\n      return stats.lineCount\n    }\n    return -1\n  }\n  async indexCov() {\n    throw new Error('CSI indexes do not support indexcov')\n    return []\n  }\n\n  parseAuxData(bytes: Buffer, offset: number, auxLength: number) {\n    if (auxLength < 30) {\n      return {\n        refIdToName: [],\n        refNameToId: {},\n      }\n    }\n\n    const formatFlags = bytes.readInt32LE(offset)\n    const coordinateType =\n      formatFlags & 0x10000 ? 'zero-based-half-open' : '1-based-closed'\n    const format = (\n      { 0: 'generic', 1: 'SAM', 2: 'VCF' } as {\n        [key: number]: string\n      }\n    )[formatFlags & 0xf]\n    if (!format) {\n      throw new Error(`invalid Tabix preset format flags ${formatFlags}`)\n    }\n    const columnNumbers = {\n      ref: bytes.readInt32LE(offset + 4),\n      start: bytes.readInt32LE(offset + 8),\n      end: bytes.readInt32LE(offset + 12),\n    }\n    const metaValue = bytes.readInt32LE(offset + 16)\n    const metaChar = metaValue ? String.fromCharCode(metaValue) : ''\n    const skipLines = bytes.readInt32LE(offset + 20)\n    const nameSectionLength = bytes.readInt32LE(offset + 24)\n\n    const { refIdToName, refNameToId } = this._parseNameBytes(\n      bytes.slice(offset + 28, offset + 28 + nameSectionLength),\n    )\n\n    return {\n      refIdToName,\n      refNameToId,\n      skipLines,\n      metaChar,\n      columnNumbers,\n      format,\n      coordinateType,\n    }\n  }\n\n  _parseNameBytes(namesBytes: Buffer) {\n    let currRefId = 0\n    let currNameStart = 0\n    const refIdToName = []\n    const refNameToId: { [key: string]: number } = {}\n    for (let i = 0; i < namesBytes.length; i += 1) {\n      if (!namesBytes[i]) {\n        if (currNameStart < i) {\n          let refName = namesBytes.toString('utf8', currNameStart, i)\n          refName = this.renameRefSeq(refName)\n          refIdToName[currRefId] = refName\n          refNameToId[refName] = currRefId\n        }\n        currNameStart = i + 1\n        currRefId += 1\n      }\n    }\n    return { refNameToId, refIdToName }\n  }\n\n  // fetch and parse the index\n\n  async _parse(opts: Options = {}) {\n    const bytes = await unzip((await this.filehandle.readFile(opts)) as Buffer)\n\n    // check TBI magic numbers\n    let csiVersion\n    if (bytes.readUInt32LE(0) === CSI1_MAGIC) {\n      csiVersion = 1\n    } else if (bytes.readUInt32LE(0) === CSI2_MAGIC) {\n      csiVersion = 2\n    } else {\n      throw new Error('Not a CSI file')\n      // TODO: do we need to support big-endian CSI files?\n    }\n\n    this.minShift = bytes.readInt32LE(4)\n    this.depth = bytes.readInt32LE(8)\n    this.maxBinNumber = ((1 << ((this.depth + 1) * 3)) - 1) / 7\n    const maxRefLength = 2 ** (this.minShift + this.depth * 3)\n\n    const auxLength = bytes.readInt32LE(12)\n    let aux: {\n      refIdToName: string[]\n      refNameToId: { [key: string]: number }\n    } = {\n      refIdToName: [],\n      refNameToId: {},\n    }\n    if (auxLength) {\n      aux = this.parseAuxData(bytes, 16, auxLength)\n    }\n    const refCount = bytes.readInt32LE(16 + auxLength)\n\n    // read the indexes for each reference sequence\n    let firstDataLine: VirtualOffset | undefined\n    let currOffset = 16 + auxLength + 4\n    const indices = new Array(refCount).fill(0).map(() => {\n      // the binning index\n      const binCount = bytes.readInt32LE(currOffset)\n      currOffset += 4\n      const binIndex: { [key: string]: Chunk[] } = {}\n      let stats // < provided by parsing a pseudo-bin, if present\n      for (let j = 0; j < binCount; j += 1) {\n        const bin = bytes.readUInt32LE(currOffset)\n        if (bin > this.maxBinNumber) {\n          // this is a fake bin that actually has stats information\n          // about the reference sequence in it\n          stats = this.parsePseudoBin(bytes, currOffset + 4)\n          currOffset += 4 + 8 + 4 + 16 + 16\n        } else {\n          const loffset = fromBytes(bytes, currOffset + 4)\n          firstDataLine = this._findFirstData(firstDataLine, loffset)\n          const chunkCount = bytes.readInt32LE(currOffset + 12)\n          currOffset += 16\n          const chunks = new Array(chunkCount)\n          for (let k = 0; k < chunkCount; k += 1) {\n            const u = fromBytes(bytes, currOffset)\n            const v = fromBytes(bytes, currOffset + 8)\n            currOffset += 16\n            // this._findFirstData(data, u)\n            chunks[k] = new Chunk(u, v, bin)\n          }\n          binIndex[bin] = chunks\n        }\n      }\n\n      return { binIndex, stats }\n    })\n\n    return {\n      ...aux,\n      csi: true,\n      refCount,\n      maxBlockSize: 1 << 16,\n      firstDataLine,\n      csiVersion,\n      indices,\n      depth: this.depth,\n      maxBinNumber: this.maxBinNumber,\n      maxRefLength,\n    }\n  }\n\n  parsePseudoBin(bytes: Buffer, offset: number) {\n    const lineCount = longToNumber(\n      Long.fromBytesLE(\n        Array.prototype.slice.call(bytes, offset + 28, offset + 36),\n        true,\n      ),\n    )\n    return { lineCount }\n  }\n\n  async blocksForRange(\n    refName: string,\n    min: number,\n    max: number,\n    opts: Options = {},\n  ) {\n    if (min < 0) {\n      min = 0\n    }\n\n    const indexData = await this.parse(opts)\n    if (!indexData) {\n      return []\n    }\n    const refId = indexData.refNameToId[refName]\n    const ba = indexData.indices[refId]\n    if (!ba) {\n      return []\n    }\n\n    // const { linearIndex, binIndex } = indexes\n\n    const overlappingBins = this.reg2bins(min, max) // List of bin #s that overlap min, max\n    const chunks: Chunk[] = []\n\n    // Find chunks in overlapping bins.  Leaf bins (< 4681) are not pruned\n    for (const [start, end] of overlappingBins) {\n      for (let bin = start; bin <= end; bin++) {\n        if (ba.binIndex[bin]) {\n          const binChunks = ba.binIndex[bin]\n          for (let c = 0; c < binChunks.length; ++c) {\n            chunks.push(new Chunk(binChunks[c].minv, binChunks[c].maxv, bin))\n          }\n        }\n      }\n    }\n\n    return optimizeChunks(chunks, new VirtualOffset(0, 0))\n  }\n\n  /**\n   * calculate the list of bins that may overlap with region [beg,end) (zero-based half-open)\n   */\n  reg2bins(beg: number, end: number) {\n    beg -= 1 // < convert to 1-based closed\n    if (beg < 1) {\n      beg = 1\n    }\n    if (end > 2 ** 50) {\n      end = 2 ** 34\n    } // 17 GiB ought to be enough for anybody\n    end -= 1\n    let l = 0\n    let t = 0\n    let s = this.minShift + this.depth * 3\n    const bins = []\n    for (; l <= this.depth; s -= 3, t += lshift(1, l * 3), l += 1) {\n      const b = t + rshift(beg, s)\n      const e = t + rshift(end, s)\n      if (e - b + bins.length > this.maxBinNumber) {\n        throw new Error(\n          `query ${beg}-${end} is too large for current binning scheme (shift ${this.minShift}, depth ${this.depth}), try a smaller query or a coarser index binning scheme`,\n        )\n      }\n      bins.push([b, e])\n    }\n    return bins\n  }\n}\n","import AbortablePromiseCache from 'abortable-promise-cache'\nimport LRU from 'quick-lru'\nimport { GenericFilehandle, LocalFile } from 'generic-filehandle'\nimport { unzip, unzipChunkSlice } from '@gmod/bgzf-filehandle'\nimport { checkAbortSignal } from './util'\nimport IndexFile, { Options } from './indexFile'\n\nimport Chunk from './chunk'\nimport TBI from './tbi'\nimport CSI from './csi'\n\ntype GetLinesCallback = (line: string, fileOffset: number) => void\n\ninterface GetLinesOpts {\n  [key: string]: unknown\n  signal?: AbortSignal\n  lineCallback: GetLinesCallback\n}\n\nfunction timeout(time: number) {\n  return new Promise(resolve => {\n    setTimeout(resolve, time)\n  })\n}\nexport default class TabixIndexedFile {\n  private filehandle: GenericFilehandle\n  private index: IndexFile\n  private chunkSizeLimit: number\n  private renameRefSeq: (n: string) => string\n  private chunkCache: any\n  /**\n   * @param {object} args\n   * @param {string} [args.path]\n   * @param {filehandle} [args.filehandle]\n   * @param {string} [args.tbiPath]\n   * @param {filehandle} [args.tbiFilehandle]\n   * @param {string} [args.csiPath]\n   * @param {filehandle} [args.csiFilehandle]\n   * @param {chunkSizeLimit} default 50MiB\n   * @param {function} [args.renameRefSeqs] optional function with sig `string => string` to transform\n   * reference sequence names for the purpose of indexing and querying. note that the data that is returned is\n   * not altered, just the names of the reference sequences that are used for querying.\n   * @param {number} [args.chunkCacheSize] maximum size in bytes of the chunk cache. default 5MB\n   * @param {number} [args.blockCacheSize] maximum size in bytes of the block cache. default 5MB\n   */\n  constructor({\n    path,\n    filehandle,\n    tbiPath,\n    tbiFilehandle,\n    csiPath,\n    csiFilehandle,\n    chunkSizeLimit = 50000000,\n    renameRefSeqs = n => n,\n    chunkCacheSize = 5 * 2 ** 20,\n  }: {\n    path?: string\n    filehandle?: GenericFilehandle\n    tbiPath?: string\n    tbiFilehandle?: GenericFilehandle\n    csiPath?: string\n    csiFilehandle?: GenericFilehandle\n    chunkSizeLimit?: number\n    renameRefSeqs?: (n: string) => string\n    chunkCacheSize?: number\n  }) {\n    if (filehandle) {\n      this.filehandle = filehandle\n    } else if (path) {\n      this.filehandle = new LocalFile(path)\n    } else {\n      throw new TypeError('must provide either filehandle or path')\n    }\n\n    if (tbiFilehandle) {\n      this.index = new TBI({\n        filehandle: tbiFilehandle,\n        renameRefSeqs,\n      })\n    } else if (csiFilehandle) {\n      this.index = new CSI({\n        filehandle: csiFilehandle,\n        renameRefSeqs,\n      })\n    } else if (tbiPath) {\n      this.index = new TBI({\n        filehandle: new LocalFile(tbiPath),\n        renameRefSeqs,\n      })\n    } else if (csiPath) {\n      this.index = new CSI({\n        filehandle: new LocalFile(csiPath),\n        renameRefSeqs,\n      })\n    } else if (path) {\n      this.index = new TBI({\n        filehandle: new LocalFile(`${path}.tbi`),\n        renameRefSeqs,\n      })\n    } else {\n      throw new TypeError(\n        'must provide one of tbiFilehandle, tbiPath, csiFilehandle, or csiPath',\n      )\n    }\n\n    this.chunkSizeLimit = chunkSizeLimit\n    this.renameRefSeq = renameRefSeqs\n    this.chunkCache = new AbortablePromiseCache({\n      cache: new LRU({\n        maxSize: Math.floor(chunkCacheSize / (1 << 16)),\n      }),\n\n      fill: this.readChunk.bind(this),\n    })\n  }\n\n  /**\n   * @param {string} refName name of the reference sequence\n   * @param {number} start start of the region (in 0-based half-open coordinates)\n   * @param {number} end end of the region (in 0-based half-open coordinates)\n   * @param {function|object} lineCallback callback called for each line in the region. can also pass a object param containing obj.lineCallback, obj.signal, etc\n   * @returns {Promise} resolved when the whole read is finished, rejected on error\n   */\n  async getLines(\n    refName: string,\n    start: number,\n    end: number,\n    opts: GetLinesOpts | GetLinesCallback,\n  ) {\n    let signal: AbortSignal | undefined\n    let options: Options = {}\n    let callback: (line: string, lineOffset: number) => void\n    if (typeof opts === 'undefined') {\n      throw new TypeError('line callback must be provided')\n    }\n    if (typeof opts === 'function') {\n      callback = opts\n    } else {\n      options = opts\n      callback = opts.lineCallback\n    }\n    if (refName === undefined) {\n      throw new TypeError('must provide a reference sequence name')\n    }\n    if (!callback) {\n      throw new TypeError('line callback must be provided')\n    }\n\n    const metadata = await this.index.getMetadata(options)\n    checkAbortSignal(signal)\n    if (!start) {\n      start = 0\n    }\n    if (!end) {\n      end = metadata.maxRefLength\n    }\n    if (!(start <= end)) {\n      throw new TypeError(\n        'invalid start and end coordinates. start must be less than or equal to end',\n      )\n    }\n    if (start === end) {\n      return\n    }\n\n    const chunks = await this.index.blocksForRange(refName, start, end, options)\n    checkAbortSignal(signal)\n\n    // check the chunks for any that are over the size limit.  if\n    // any are, don't fetch any of them\n    for (let i = 0; i < chunks.length; i += 1) {\n      const size = chunks[i].fetchedSize()\n      if (size > this.chunkSizeLimit) {\n        throw new Error(\n          `Too much data. Chunk size ${size.toLocaleString()} bytes exceeds chunkSizeLimit of ${this.chunkSizeLimit.toLocaleString()}.`,\n        )\n      }\n    }\n\n    // now go through each chunk and parse and filter the lines out of it\n    let last = Date.now()\n    for (let chunkNum = 0; chunkNum < chunks.length; chunkNum += 1) {\n      let previousStartCoordinate: number | undefined\n      const c = chunks[chunkNum]\n      const { buffer, cpositions, dpositions } = await this.chunkCache.get(\n        c.toString(),\n        c,\n        signal,\n      )\n\n      const lines = (\n        typeof TextDecoder !== 'undefined'\n          ? new TextDecoder('utf-8').decode(buffer)\n          : buffer.toString()\n      ).split('\\n')\n      lines.pop()\n\n      checkAbortSignal(signal)\n      let blockStart = c.minv.dataPosition\n      let pos\n\n      for (let i = 0; i < lines.length; i += 1) {\n        const line = lines[i]\n\n        for (pos = 0; blockStart >= dpositions[pos]; pos += 1) {}\n\n        // filter the line for whether it is within the requested range\n        const { startCoordinate, overlaps } = this.checkLine(\n          metadata,\n          refName,\n          start,\n          end,\n          line,\n        )\n\n        // do a small check just to make sure that the lines are really sorted by start coordinate\n        if (\n          previousStartCoordinate !== undefined &&\n          startCoordinate !== undefined &&\n          previousStartCoordinate > startCoordinate\n        ) {\n          throw new Error(\n            `Lines not sorted by start coordinate (${previousStartCoordinate} > ${startCoordinate}), this file is not usable with Tabix.`,\n          )\n        }\n        previousStartCoordinate = startCoordinate\n\n        if (overlaps) {\n          callback(\n            line.trim(),\n            // cpositions[pos] refers to actual file offset of a bgzip block boundaries\n            //\n            // we multiply by (1 <<8) in order to make sure each block has a \"unique\"\n            // address space so that data in that block could never overlap\n            //\n            // then the blockStart-dpositions is an uncompressed file offset from\n            // that bgzip block boundary, and since the cpositions are multiplied by\n            // (1 << 8) these uncompressed offsets get a unique space\n            cpositions[pos] * (1 << 8) + (blockStart - dpositions[pos]),\n          )\n        } else if (startCoordinate !== undefined && startCoordinate >= end) {\n          // the lines were overlapping the region, but now have stopped, so\n          // we must be at the end of the relevant data and we can stop\n          // processing data now\n          return\n        }\n        blockStart += line.length + 1\n\n        // yield if we have emitted beyond the yield limit\n        if (last - Date.now() > 500) {\n          last = Date.now()\n          checkAbortSignal(signal)\n          await timeout(1)\n        }\n      }\n    }\n  }\n\n  async getMetadata(opts: Options = {}) {\n    return this.index.getMetadata(opts)\n  }\n\n  /**\n   * get a buffer containing the \"header\" region of\n   * the file, which are the bytes up to the first\n   * non-meta line\n   *\n   * @returns {Promise} for a buffer\n   */\n  async getHeaderBuffer(opts: Options = {}) {\n    const { firstDataLine, metaChar, maxBlockSize } = await this.getMetadata(\n      opts,\n    )\n    checkAbortSignal(opts.signal)\n    const maxFetch =\n      firstDataLine && firstDataLine.blockPosition\n        ? firstDataLine.blockPosition + maxBlockSize\n        : maxBlockSize\n    // TODO: what if we don't have a firstDataLine, and the header\n    // actually takes up more than one block? this case is not covered here\n\n    let bytes = await this._readRegion(0, maxFetch, opts)\n    checkAbortSignal(opts.signal)\n    try {\n      bytes = await unzip(bytes)\n    } catch (e) {\n      console.error(e)\n      throw new Error(\n        //@ts-ignore\n        `error decompressing block ${e.code} at 0 (length ${maxFetch}) ${e}`,\n      )\n    }\n\n    // trim off lines after the last non-meta line\n    if (metaChar) {\n      // trim backward from the end\n      let lastNewline = -1\n      const newlineByte = '\\n'.charCodeAt(0)\n      const metaByte = metaChar.charCodeAt(0)\n      for (let i = 0; i < bytes.length; i += 1) {\n        if (i === lastNewline + 1 && bytes[i] !== metaByte) {\n          break\n        }\n        if (bytes[i] === newlineByte) {\n          lastNewline = i\n        }\n      }\n      bytes = bytes.slice(0, lastNewline + 1)\n    }\n    return bytes\n  }\n\n  /**\n   * get a string containing the \"header\" region of the\n   * file, is the portion up to the first non-meta line\n   *\n   * @returns {Promise} for a string\n   */\n  async getHeader(opts: Options = {}) {\n    const bytes = await this.getHeaderBuffer(opts)\n    checkAbortSignal(opts.signal)\n    return bytes.toString('utf8')\n  }\n\n  /**\n   * get an array of reference sequence names, in the order in which\n   * they occur in the file.\n   *\n   * reference sequence renaming is not applied to these names.\n   *\n   * @returns {Promise} for an array of string sequence names\n   */\n  async getReferenceSequenceNames(opts: Options = {}) {\n    const metadata = await this.getMetadata(opts)\n    return metadata.refIdToName\n  }\n\n  /**\n   * @param {object} metadata metadata object from the parsed index,\n   * containing columnNumbers, metaChar, and format\n   * @param {string} regionRefName\n   * @param {number} regionStart region start coordinate (0-based-half-open)\n   * @param {number} regionEnd region end coordinate (0-based-half-open)\n   * @param {array[string]} line\n   * @returns {object} like `{startCoordinate, overlaps}`. overlaps is boolean,\n   * true if line is a data line that overlaps the given region\n   */\n  checkLine(\n    {\n      columnNumbers,\n      metaChar,\n      coordinateType,\n      format,\n    }: {\n      columnNumbers: { ref: number; start: number; end: number }\n      metaChar: string\n      coordinateType: string\n      format: string\n    },\n    regionRefName: string,\n    regionStart: number,\n    regionEnd: number,\n    line: string,\n  ) {\n    // skip meta lines\n    if (line.charAt(0) === metaChar) {\n      return { overlaps: false }\n    }\n\n    // check ref/start/end using column metadata from index\n    let { ref, start, end } = columnNumbers\n    if (!ref) {\n      ref = 0\n    }\n    if (!start) {\n      start = 0\n    }\n    if (!end) {\n      end = 0\n    }\n    if (format === 'VCF') {\n      end = 8\n    }\n    const maxColumn = Math.max(ref, start, end)\n\n    // this code is kind of complex, but it is fairly fast.\n    // basically, we want to avoid doing a split, because if the lines are really long\n    // that could lead to us allocating a bunch of extra memory, which is slow\n\n    let currentColumnNumber = 1 // cols are numbered starting at 1 in the index metadata\n    let currentColumnStart = 0\n    let refSeq = ''\n    let startCoordinate = -Infinity\n    for (let i = 0; i < line.length + 1; i += 1) {\n      if (line[i] === '\\t' || i === line.length) {\n        if (currentColumnNumber === ref) {\n          if (\n            this.renameRefSeq(line.slice(currentColumnStart, i)) !==\n            regionRefName\n          ) {\n            return { overlaps: false }\n          }\n        } else if (currentColumnNumber === start) {\n          startCoordinate = parseInt(line.slice(currentColumnStart, i), 10)\n          // we convert to 0-based-half-open\n          if (coordinateType === '1-based-closed') {\n            startCoordinate -= 1\n          }\n          if (startCoordinate >= regionEnd) {\n            return { startCoordinate, overlaps: false }\n          }\n          if (end === 0 || end === start) {\n            // if we have no end, we assume the feature is 1 bp long\n            if (startCoordinate + 1 <= regionStart) {\n              return { startCoordinate, overlaps: false }\n            }\n          }\n        } else if (format === 'VCF' && currentColumnNumber === 4) {\n          refSeq = line.slice(currentColumnStart, i)\n        } else if (currentColumnNumber === end) {\n          let endCoordinate\n          // this will never match if there is no end column\n          if (format === 'VCF') {\n            endCoordinate = this._getVcfEnd(\n              startCoordinate,\n              refSeq,\n              line.slice(currentColumnStart, i),\n            )\n          } else {\n            endCoordinate = parseInt(line.slice(currentColumnStart, i), 10)\n          }\n          if (endCoordinate <= regionStart) {\n            return { overlaps: false }\n          }\n        }\n        currentColumnStart = i + 1\n        currentColumnNumber += 1\n        if (currentColumnNumber > maxColumn) {\n          break\n        }\n      }\n    }\n    return { startCoordinate, overlaps: true }\n  }\n\n  _getVcfEnd(startCoordinate: number, refSeq: string, info: any) {\n    let endCoordinate = startCoordinate + refSeq.length\n    // ignore TRA features as they specify CHR2 and END\n    // as being on a different chromosome\n    // if CHR2 is on the same chromosome, still ignore it\n    // because there should be another pairwise feature\n    // at the end of this one\n    const isTRA = info.indexOf('SVTYPE=TRA') !== -1\n    if (info[0] !== '.' && !isTRA) {\n      let prevChar = ';'\n      for (let j = 0; j < info.length; j += 1) {\n        if (prevChar === ';' && info.slice(j, j + 4) === 'END=') {\n          let valueEnd = info.indexOf(';', j)\n          if (valueEnd === -1) {\n            valueEnd = info.length\n          }\n          endCoordinate = parseInt(info.slice(j + 4, valueEnd), 10)\n          break\n        }\n        prevChar = info[j]\n      }\n    } else if (isTRA) {\n      return startCoordinate + 1\n    }\n    return endCoordinate\n  }\n\n  /**\n   * return the approximate number of data lines in the given reference sequence\n   * @param {string} refSeq reference sequence name\n   * @returns {Promise} for number of data lines present on that reference sequence\n   */\n  async lineCount(refName: string, opts: Options = {}) {\n    return this.index.lineCount(refName, opts)\n  }\n\n  async _readRegion(\n    position: number,\n    compressedSize: number,\n    opts: Options = {},\n  ) {\n    const { bytesRead, buffer } = await this.filehandle.read(\n      Buffer.alloc(compressedSize),\n      0,\n      compressedSize,\n      position,\n      opts,\n    )\n\n    return bytesRead < compressedSize ? buffer.slice(0, bytesRead) : buffer\n  }\n\n  /**\n   * read and uncompress the data in a chunk (composed of one or more\n   * contiguous bgzip blocks) of the file\n   * @param {Chunk} chunk\n   * @returns {Promise} for a string chunk of the file\n   */\n  async readChunk(chunk: Chunk, opts: Options = {}) {\n    // fetch the uncompressed data, uncompress carefully a block at a time,\n    // and stop when done\n\n    const compressedData = await this._readRegion(\n      chunk.minv.blockPosition,\n      chunk.fetchedSize(),\n      opts,\n    )\n    try {\n      return unzipChunkSlice(compressedData, chunk)\n    } catch (e) {\n      throw new Error(`error decompressing chunk ${chunk.toString()} ${e}`)\n    }\n  }\n}\n"],"names":["longToNumber","long","greaterThan","Number","MAX_SAFE_INTEGER","lessThan","MIN_SAFE_INTEGER","Error","toNumber","AbortError","checkAbortSignal","signal","aborted","DOMException","e","code","optimizeChunks","chunks","lowest","mergedChunks","lastChunk","length","sort","c0","c1","dif","minv","blockPosition","dataPosition","forEach","chunk","chunk1","chunk2","maxv","compareTo","push","VirtualOffset","this","b","min","i","args","fromBytes","bytes","offset","bigendian","Chunk","bin","fetchedSize","undefined","_fetchedSize","toUniqueString","IndexFile","filehandle","renameRefSeqs","n","renameRefSeq","opts","parse","indices","rest","currentFdl","virtualOffset","_parseCache","AbortablePromiseCache","cache","QuickLRU","maxSize","fill","_parse","get","seqId","binIndex","TBI_MAGIC","reg2bins","beg","end","TabixIndex","refName","indexData","refId","refNameToId","stats","lineCount","unzip","readFile","readUInt32LE","refCount","readInt32LE","formatFlags","coordinateType","format","columnNumbers","ref","start","metaValue","maxBinNumber","depth","maxRefLength","metaChar","String","fromCharCode","skipLines","nameSectionLength","_parseNameBytes","slice","refIdToName","currOffset","Array","map","binCount","j","chunkCount","parsePseudoBin","k","u","v","firstDataLine","_findFirstData","linearCount","linearIndex","maxBlockSize","Long","namesBytes","currRefId","currNameStart","toString","max","ba","console","warn","overlappingBins","binChunks","c","nintv","minLin","Math","maxLin","vp","CSI1_MAGIC","CSI2_MAGIC","rshift","num","bits","floor","CSI","minShift","auxLength","csiVersion","aux","parseAuxData","loffset","csi","prototype","call","l","t","s","bins","timeout","time","Promise","resolve","setTimeout","TabixIndexedFile","path","tbiPath","tbiFilehandle","csiPath","csiFilehandle","chunkSizeLimit","chunkCacheSize","TypeError","LocalFile","index","TBI","chunkCache","LRU","readChunk","bind","options","callback","lineCallback","getMetadata","metadata","blocksForRange","size","toLocaleString","last","Date","now","chunkNum","previousStartCoordinate","buffer","cpositions","dpositions","lines","TextDecoder","decode","split","pop","blockStart","pos","line","checkLine","startCoordinate","overlaps","trim","maxFetch","_readRegion","error","lastNewline","newlineByte","charCodeAt","metaByte","getHeaderBuffer","regionRefName","regionStart","regionEnd","charAt","maxColumn","currentColumnNumber","currentColumnStart","refSeq","Infinity","parseInt","_getVcfEnd","info","endCoordinate","isTRA","indexOf","prevChar","valueEnd","position","compressedSize","read","Buffer","alloc","bytesRead","compressedData","unzipChunkSlice"],"sourceRoot":""}
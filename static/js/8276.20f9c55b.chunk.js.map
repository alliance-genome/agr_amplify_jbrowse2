{"version":3,"file":"static/js/8276.20f9c55b.chunk.js","mappings":"0MAiBAA,eAAeC,EAAMC,GACnB,IACE,IAAIC,EACAC,EAAM,EACNC,EAAI,EACR,MAAMC,EAAS,GACf,IACIC,EADAC,EAAY,EAEhB,EAAG,CACD,MAAMC,EAAiBP,EAAUQ,SAASN,GAK1C,GAJAG,EAAW,IAAI,EAAAI,UAEXR,QAASI,GACbA,EAASK,KAAKH,EAAgB,EAAAI,cAC1BN,EAASO,IACX,MAAM,IAAIC,MAAMR,EAASS,KAG3BZ,GAAOD,EAAKc,QACZX,EAAOD,GAAKE,EAASW,OACrBV,GAAaF,EAAOD,GAAGc,OACvBd,GAAK,C,OACEF,EAAKiB,UAEd,MAAMF,EAAS,IAAIG,WAAWb,GAC9B,IAAK,IAAIH,EAAI,EAAGiB,EAAS,EAAGjB,EAAIC,EAAOa,OAAQd,IAC7Ca,EAAOK,IAAIjB,EAAOD,GAAIiB,GACtBA,GAAUhB,EAAOD,GAAGc,OAEtB,OAAO,EAAAK,OAAOC,KAAKP,E,CACnB,MAAOQ,GAEP,GAAI,GAAGA,IAAIC,MAAM,0BACf,MAAM,IAAIZ,MACR,4DAGJ,MAAMW,C,CAEV,CAgDA1B,eAAe4B,EAAgB1B,EAAmB2B,GAChD,IACE,IAAI1B,EACJ,MAAM,KAAE2B,EAAI,KAAEC,GAASF,EACvB,IAAIG,EAAOF,EAAKG,cACZC,EAAOJ,EAAKK,aAChB,MAAM7B,EAAS,GACT8B,EAAa,GACbC,EAAa,GAEnB,IAAI7B,EAAY,EACZH,EAAI,EACR,EAAG,CACD,MAAMI,EAAiBP,EAAUQ,SAASsB,EAAOF,EAAKG,eAChD1B,EAAW,IAAI,EAAAI,QAIrB,KAFIR,QAASI,GACbA,EAASK,KAAKH,EAAgB,EAAAI,cAC1BN,EAASO,IACX,MAAM,IAAIC,MAAMR,EAASS,KAG3B,MAAMsB,EAAS/B,EAASW,OACxBZ,EAAOM,KAAK0B,GACZ,IAAIC,EAAMD,EAAOnB,OAEjBiB,EAAWxB,KAAKoB,GAChBK,EAAWzB,KAAKsB,GACM,IAAlB5B,EAAOa,QAAgBW,EAAKK,eAE9B7B,EAAO,GAAKA,EAAO,GAAGI,SAASoB,EAAKK,cACpCI,EAAMjC,EAAO,GAAGa,QAElB,MAAMqB,EAAWR,EAIjB,GAHAA,GAAQ7B,EAAKc,QACbiB,GAAQK,EAEJC,GAAYT,EAAKE,cAAe,CAKlC3B,EAAOD,GAAKC,EAAOD,GAAGK,SACpB,EACAqB,EAAKE,gBAAkBH,EAAKG,cACxBF,EAAKI,aAAeL,EAAKK,aAAe,EACxCJ,EAAKI,aAAe,GAG1BC,EAAWxB,KAAKoB,GAChBK,EAAWzB,KAAKsB,GAChB1B,GAAaF,EAAOD,GAAGc,OACvB,K,CAEFX,GAAaF,EAAOD,GAAGc,OACvBd,G,OACOF,EAAKiB,UAEd,MAAMF,EAAS,IAAIG,WAAWb,GAC9B,IAAK,IAAIH,EAAI,EAAGiB,EAAS,EAAGjB,EAAIC,EAAOa,OAAQd,IAC7Ca,EAAOK,IAAIjB,EAAOD,GAAIiB,GACtBA,GAAUhB,EAAOD,GAAGc,OAItB,MAAO,CAAEmB,OAFM,EAAAd,OAAOC,KAAKP,GAEVkB,aAAYC,a,CAC7B,MAAOX,GAEP,GAAI,GAAGA,IAAIC,MAAM,0BACf,MAAM,IAAIZ,MACR,4DAGJ,MAAMW,C,CAEV,C,wBC5Ke,MAAMe,EAKnB,WAAAC,EAAY,WACVC,EAAU,KACVC,IAKA,GAAID,EACFE,KAAKF,WAAaA,MACb,KAAIC,EAGT,MAAM,IAAIE,UAAU,6CAFpBD,KAAKF,WAAa,IAAI,KAAUC,E,CAIpC,CAEA,qBAAAG,CAAsBC,EAAa1B,EAAS,EAAG2B,GAAW,GAExD,MAAMC,EAAO,gBAAiBF,EAAIG,MAAM7B,EAAQA,EAAS,GAAI2B,GAC7D,GACEC,EAAKE,YAAYC,OAAOC,mBACxBJ,EAAKK,SAASF,OAAOG,kBAErB,MAAM,IAAIV,UAAU,oBAGtB,OAAOI,EAAKO,UACd,CAEA,SAAAC,GAIE,OAHKb,KAAKc,QACRd,KAAKc,MAAQd,KAAKe,cAEbf,KAAKc,KACd,CAEA,gBAAMC,GACJ,IAAIZ,EAAM,EAAAxB,OAAOqC,YAAY,SACvBhB,KAAKF,WAAWmB,KAAKd,EAAK,EAAG,EAAG,GACtC,MAAMe,EAAalB,KAAKE,sBAAsBC,EAAK,GAAG,GACtD,IAAKe,EACH,MAAO,CAAC,CAAC,EAAG,IAGd,MAAMC,EAAU,IAAIC,MAAMF,EAAa,GACvCC,EAAQ,GAAK,CAAC,EAAG,GAGjB,MAAME,EAAU,GAAQH,EACxB,GAAIG,EAAUb,OAAOC,iBACnB,MAAM,IAAIR,UAAU,oBAEtBE,EAAM,EAAAxB,OAAOqC,YAAYK,SACnBrB,KAAKF,WAAWmB,KAAKd,EAAK,EAAGkB,EAAS,GAC5C,IAAK,IAAIC,EAAc,EAAGA,EAAcJ,EAAYI,GAAe,EAAG,CACpE,MAAMC,EAAqBvB,KAAKE,sBAC9BC,EACc,GAAdmB,GAEIE,EAAuBxB,KAAKE,sBAChCC,EACc,GAAdmB,EAAmB,GAErBH,EAAQG,EAAc,GAAK,CAACC,EAAoBC,E,CAGlD,OAAOL,CACT,CAEA,kBAAMM,GACJ,MAAMN,QAAgBnB,KAAKa,YAC3B,GAAKM,EAAQ7C,OAGb,OAAO6C,EAAQA,EAAQ7C,OAAS,EAClC,CAEA,8BAAMoD,CAAyBpD,EAAgBqD,GAC7C,MAAMC,EAAcD,EAAWrD,EAC/B,GAAe,IAAXA,EACF,MAAO,GAET,MAAM6C,QAAgBnB,KAAKa,YACrBgB,EAAW,GAIXC,EAAU,CAACC,EAAYC,KAC3B,MAAMR,EAAuBO,EA/FL,GAgGlBE,EAA2BD,EAC7BA,EAjGoB,GAkGpBE,IAEJ,OACEV,GAAwBG,GACxBM,EAA2BN,EAEpB,EAGLH,EAAuBG,GACjB,EAGH,CAAC,EAGV,IAAIQ,EAAa,EACbC,EAAajB,EAAQ7C,OAAS,EAC9B+D,EAAiBC,KAAKC,MAAMpB,EAAQ7C,OAAS,GAE7CkE,EAAaV,EACfX,EAAQkB,GACRlB,EAAQkB,EAAiB,IAE3B,KAAsB,IAAfG,GACDA,EAAa,EACfJ,EAAaC,EAAiB,EACrBG,EAAa,IACtBL,EAAaE,EAAiB,GAEhCA,EAAiBC,KAAKG,MAAML,EAAaD,GAAc,GAAKA,EAC5DK,EAAaV,EAAQX,EAAQkB,GAAiBlB,EAAQkB,EAAiB,IAIzER,EAAS9D,KAAKoD,EAAQkB,IACtB,IAAI7E,EAAI6E,EAAiB,EACzB,KAAO7E,EAAI2D,EAAQ7C,SACjBuD,EAAS9D,KAAKoD,EAAQ3D,MAClB2D,EAAQ3D,GAzIY,IAyIiBoE,IAFhBpE,GAAK,GAShC,OAHIqE,EAASA,EAASvD,OAAS,GA7IL,GA6IiCsD,GACzDC,EAAS9D,KAAK,IAET8D,CACT,EC/Ia,MAAMa,EAInB,WAAA7C,EAAY,WACVC,EAAU,KACVC,EAAI,cACJ4C,EAAa,QACbC,IAOA,GAAI9C,EACFE,KAAKF,WAAaA,MACb,KAAIC,EAGT,MAAM,IAAIE,UAAU,6CAFpBD,KAAKF,WAAa,IAAI,KAAUC,E,CAKlC,IAAK4C,IAAkBC,IAAY7C,EACjC,MAAM,IAAIE,UAAU,mDAGtBD,KAAK6C,IAAM,IAAIjD,EAAS,CACtBE,WAAY6C,EACZ5C,KAAO4C,GAAkBC,IAAW7C,EAAiB,GAAGA,QAAb6C,GAE/C,CAEA,UAAME,GACJ,MAAMC,QAAuB/C,KAAKF,WAAWgD,OAC7C,OAAOE,OAAOC,OAAOF,EAAgB,CACnCG,WAAYlD,KAAKmD,0BACjBC,YAAQC,EACRC,aAASD,GAEb,CAEA,6BAAMF,GAGJ,MAAO,CAAE3B,SAA8BxB,KAAK6C,IAAIpB,gBAE1C,KAAEyB,SAAelD,KAAKF,WAAWgD,OAEjC3C,EAAM,EAAAxB,OAAOqC,YAAY,IAGzB,UAAEuC,SAAoBvD,KAAKF,WAAWmB,KAAKd,EAAK,EAAG,EAAG+C,EAAO,GAAK,GACxE,GAAkB,IAAdK,EACF,MAAM,IAAIrF,MAAM,cAGlB,OAAOsD,EAD2BrB,EAAIqD,aAAa,EAErD,CAEA,6BAAMC,CACJC,GACCnC,IACAoC,IAED,IAAIC,EAAOD,EACNC,IACHA,SAAc5D,KAAKF,WAAWgD,QAAQI,MAIxC,MAAMW,EAAwBD,EAAOrC,EAcrC,aAZMvB,KAAKF,WAAWmB,KACpByC,EACA,EACAG,EACAtC,SAI2BnE,EAC3BsG,EAAYpD,MAAM,EAAGuD,GAIzB,CAEA,UAAM5C,CAAKd,EAAa1B,EAAgBH,EAAgBqD,GAEtD,MAAMmC,QAAuB9D,KAAK6C,IAAInB,yBACpCpD,EACAqD,GAEI+B,EAAc,EAAA/E,OAAOqC,YAAY,OAEvC,IAAI+C,EAAoBtF,EACpB8E,EAAY,EAChB,IACE,IAAIS,EAAW,EACfA,EAAWF,EAAexF,OAAS,EACnC0F,GAAY,EACZ,CAEA,MAAMC,QAA2BjE,KAAKyD,wBACpCC,EACAI,EAAeE,GACfF,EAAeE,EAAW,KAErB,CAAExC,GAAwBsC,EAAeE,GAC1CE,EACJ1C,GAAwBG,EAAW,EAAIA,EAAWH,EAC9C2C,EACJ7B,KAAK8B,IACHzC,EAAWrD,EACXkD,EAAuByC,EAAmB3F,QACxCkD,EACF0C,GAAgB,GAAKA,EAAeD,EAAmB3F,SACzD2F,EAAmBI,KAAKlE,EAAK4D,EAAmBG,EAAcC,GAC9DJ,GAAqBI,EAAYD,EACjCX,GAAaY,EAAYD,E,CAI7B,MAAO,CAAEX,YAAW9D,OAAQU,EAC9B,E,+GCjIK,SAASmE,EAAajE,GAC3B,GACEA,EAAKE,YAAYC,OAAOC,mBACxBJ,EAAKK,SAASF,OAAOG,kBAErB,MAAM,IAAIzC,MAAM,oBAElB,OAAOmC,EAAKO,UACd,CAEA,MAAM2D,UAAmBrG,OAclB,SAASsG,EAAiBC,GAC/B,GAAKA,GAIDA,EAAOC,QAAS,CAClB,GAA4B,oBAAjBC,aAET,MAAM,IAAIA,aAAa,UAAW,cAC7B,CACL,MAAM9F,EAAI,IAAI0F,EAAW,WAEzB,MADA1F,EAAE+F,KAAO,cACH/F,C,EAGZ,CAoBO,SAASgG,EAAepH,EAAiBqH,GAC9C,MAAMC,EAAwB,GAC9B,IAAIC,EAA0B,KAE9B,OAAsB,IAAlBvH,EAAOa,OACFb,GAGTA,EAAOwH,MAAK,SAAUC,EAAIC,GACxB,MAAMC,EAAMF,EAAGjG,KAAKG,cAAgB+F,EAAGlG,KAAKG,cAC5C,OAAY,IAARgG,EACKA,EAEAF,EAAGjG,KAAKK,aAAe6F,EAAGlG,KAAKK,YAE1C,IAEA7B,EAAO4H,SAAQrG,IAxBV,IAAwBsG,EAAeC,IAyBrCT,GAAU9F,EAAME,KAAKsG,UAAUV,GAAU,KAC1B,OAAdE,GACFD,EAAahH,KAAKiB,GAClBgG,EAAYhG,IA5BWsG,EA8BJN,GA9BmBO,EA8BRvG,GA5B3BC,KAAKG,cAAgBkG,EAAOpG,KAAKE,cAAgB,MACxDmG,EAAOrG,KAAKE,cAAgBkG,EAAOrG,KAAKG,cAAgB,IA4B9CJ,EAAME,KAAKsG,UAAUR,EAAU9F,MAAQ,IACzC8F,EAAU9F,KAAOF,EAAME,OAGzB6F,EAAahH,KAAKiB,GAClBgG,EAAYhG,I,IAMb+F,EACT,C,wBCjGe,MAAMU,EAGnB,WAAA5F,CAAYT,EAAuBE,GACjCU,KAAKZ,cAAgBA,EACrBY,KAAKV,aAAeA,CACtB,CAEA,QAAAoG,GACE,MAAO,GAAG1F,KAAKZ,iBAAiBY,KAAKV,cACvC,CAEA,SAAAkG,CAAUG,GACR,OACE3F,KAAKZ,cAAgBuG,EAAEvG,eAAiBY,KAAKV,aAAeqG,EAAErG,YAElE,CAEA,UAAO8E,IAAOwB,GACZ,IAAIxB,EACA5G,EAAI,EACR,MAAQ4G,EAAK5G,GAAK,EAChB4G,EAAMwB,EAAKpI,GAEb,KAAOA,EAAIoI,EAAKtH,OAAQd,GAAK,EACvB4G,EAAIoB,UAAUI,EAAKpI,IAAM,IAC3B4G,EAAMwB,EAAKpI,IAGf,OAAO4G,CACT,EAEK,SAASyB,EAAUC,EAAerH,EAAS,EAAGsH,GAAY,GAC/D,GAAIA,EACF,MAAM,IAAI7H,MAAM,mDAGlB,OAAO,IAAIuH,EACW,cAApBK,EAAMrH,EAAS,GACO,WAApBqH,EAAMrH,EAAS,GACK,SAApBqH,EAAMrH,EAAS,GACK,MAApBqH,EAAMrH,EAAS,GACK,IAApBqH,EAAMrH,EAAS,GACfqH,EAAMrH,EAAS,GAChBqH,EAAMrH,EAAS,IAAM,EAAKqH,EAAMrH,GAErC,CC5Ce,MAAMuH,EAYnB,WAAAnG,CACEZ,EACAC,EACA+G,EACAC,OAAc7C,GAEdrD,KAAKf,KAAOA,EACZe,KAAKd,KAAOA,EACZc,KAAKiG,IAAMA,EACXjG,KAAKmG,aAAeD,CACtB,CAEA,cAAAE,GACE,MAAO,GAAGpG,KAAKf,SAASe,KAAKd,aAC3Bc,KAAKiG,oBACUjG,KAAKkG,gBACxB,CAEA,QAAAR,GACE,OAAO1F,KAAKoG,gBACd,CAEA,SAAAZ,CAAUG,GACR,OACE3F,KAAKf,KAAKuG,UAAUG,EAAE1G,OACtBe,KAAKd,KAAKsG,UAAUG,EAAEzG,OACtBc,KAAKiG,IAAMN,EAAEM,GAEjB,CAEA,WAAAC,GACE,YAA0B7C,IAAtBrD,KAAKmG,aACAnG,KAAKmG,aAEPnG,KAAKd,KAAKE,cAAgB,MAAYY,KAAKf,KAAKG,aACzD,EC9Ba,MAAeiH,EAK5B,WAAAxG,EAAY,WACVC,EAAU,cACVwG,EAAgB,CAACC,GAAcA,KAK/BvG,KAAKF,WAAaA,EAClBE,KAAKwG,aAAeF,CACtB,CAMO,iBAAMG,CAAYC,EAAgB,CAAC,GAExC,MAAM,QAAEC,KAAYC,SAAe5G,KAAK6G,MAAMH,GAC9C,OAAOE,CACT,CASA,cAAAE,CACEC,EACAC,GAEA,OAAID,EACKA,EAAWvB,UAAUwB,GAAiB,EACzCA,EACAD,EAEGC,CAEX,CAEA,WAAMH,CAAMH,EAAgB,CAAC,GAO3B,OANK1G,KAAKiH,SACRjH,KAAKiH,OAASjH,KAAKkH,OAAOR,GAAMS,OAAMtI,IAEpC,MADAmB,KAAKiH,YAAS5D,EACRxE,CAAC,KAGJmB,KAAKiH,MACd,CAEA,eAAMG,CAAUC,EAAeX,EAAgB,CAAC,GAC9C,gBAAiB1G,KAAK6G,MAAMH,IAAOC,QAAQU,IAAU,CAAC,GAAGC,QAC3D,ECnDa,MAAMC,UAAmBlB,EACtC,eAAMmB,CAAUC,EAAiBf,EAAgB,CAAC,GAChD,MAAMgB,QAAkB1H,KAAK6G,MAAMH,GACnC,IAAKgB,EACH,OAAQ,EAEV,MAAMC,EAAQD,EAAUE,YAAYH,GAEpC,IADYC,EAAUf,QAAQgB,GAE5B,OAAQ,EAEV,MAAM,MAAEE,GAAUH,EAAUf,QAAQgB,GACpC,OAAIE,EACKA,EAAML,WAEP,CACV,CAGA,YAAMN,CAAOR,EAAgB,CAAC,GAC5B,MAAMvG,QAAYH,KAAKF,WAAWgI,SAASpB,GACrCZ,QAAc,IAAA1I,OAAM+C,GAI1B,GAHAqE,EAAiBkC,EAAKjC,QAzCR,WA4CVqB,EAAMtC,aAAa,GACrB,MAAM,IAAItF,MAAM,kBAKlB,MAAM6J,EAAWjC,EAAMkC,YAAY,GAC7BC,EAAcnC,EAAMkC,YAAY,GAChCE,EACU,MAAdD,EAAwB,uBAAyB,iBAM7CE,EALwC,CAC5C,EAAG,UACH,EAAG,MACH,EAAG,OAEmC,GAAdF,GAC1B,IAAKE,EACH,MAAM,IAAIjK,MAAM,qCAAqC+J,KAEvD,MAAMG,EAAgB,CACpBC,IAAKvC,EAAMkC,YAAY,IACvBM,MAAOxC,EAAMkC,YAAY,IACzBO,IAAKzC,EAAMkC,YAAY,KAEnBQ,EAAY1C,EAAMkC,YAAY,IAI9BS,EAAWD,EAAYE,OAAOC,aAAaH,GAAa,KACxDI,EAAY9C,EAAMkC,YAAY,IAG9Ba,EAAoB/C,EAAMkC,YAAY,KACtC,YAAEJ,EAAW,YAAEkB,GAAgB9I,KAAK+I,gBACxCjD,EAAMxF,MAAM,GAAI,GAAKuI,IAIvB,IACIG,EADAC,EAAa,GAAKJ,EAiDtB,MAAO,CACLlC,QAhDc,IAAIvF,MAAM2G,GAAUmB,KAAK,GAAGC,KAAI,KAE9C,MAAMC,EAAWtD,EAAMkC,YAAYiB,GACnCA,GAAc,EACd,MAAM3B,EAAuC,CAAC,EAC9C,IAAIO,EACJ,IAAK,IAAIwB,EAAI,EAAGA,EAAID,EAAUC,GAAK,EAAG,CACpC,MAAMpD,EAAMH,EAAMtC,aAAayF,GAE/B,GADAA,GAAc,EACVhD,EAAMqD,MACR,MAAM,IAAIpL,MACR,8DAEG,GAAYoL,QAARrD,EAA0B,CACnC,MAAMsD,EAAazD,EAAMkC,YAAYiB,GACrCA,GAAc,EACK,IAAfM,IACF1B,EAAQ7H,KAAKwJ,eAAe1D,EAAOmD,IAErCA,GAAc,GAAKM,C,KACd,CACL,MAAMA,EAAazD,EAAMkC,YAAYiB,GACrCA,GAAc,EACd,MAAMxL,EAAS,IAAI2D,MAAMmI,GACzB,IAAK,IAAIE,EAAI,EAAGA,EAAIF,EAAYE,GAAK,EAAG,CACtC,MAAMC,EAAI7D,EAAUC,EAAOmD,GACrBU,EAAI9D,EAAUC,EAAOmD,EAAa,GACxCA,GAAc,GACdD,EAAgBhJ,KAAK8G,eAAekC,EAAeU,GACnDjM,EAAOgM,GAAK,IAAIzD,EAAM0D,EAAGC,EAAG1D,E,CAE9BqB,EAASrB,GAAOxI,C,EAKpB,MAAMmM,EAAc9D,EAAMkC,YAAYiB,GACtCA,GAAc,EACd,MAAMY,EAAc,IAAIzI,MAAMwI,GAC9B,IAAK,IAAIH,EAAI,EAAGA,EAAIG,EAAaH,GAAK,EACpCI,EAAYJ,GAAK5D,EAAUC,EAAOmD,GAClCA,GAAc,EACdD,EAAgBhJ,KAAK8G,eAAekC,EAAea,EAAYJ,IAEjE,MAAO,CAAEnC,WAAUuC,cAAahC,QAAO,IAKvCY,WACAa,aAhEmB,MAiEnBQ,aAhEmB,UAiEnBlB,YACAI,gBACAZ,gBACAF,iBACAC,SACAW,cACAlB,cACAmC,aAAc,MAElB,CAEA,cAAAP,CAAe1D,EAAerH,GAO5B,MAAO,CAAE+I,UANSlD,EAChB,gBACEwB,EAAMxF,MAAM7B,EAAS,GAAIA,EAAS,KAClC,IAIN,CAEA,eAAAsK,CAAgBiB,GACd,IAAIC,EAAY,EACZC,EAAgB,EACpB,MAAMpB,EAAwB,GACxBlB,EAAyC,CAAC,EAChD,IAAK,IAAIpK,EAAI,EAAGA,EAAIwM,EAAW1L,OAAQd,GAAK,EAC1C,IAAKwM,EAAWxM,GAAI,CAClB,GAAI0M,EAAgB1M,EAAG,CACrB,IAAIiK,EAAUuC,EAAWtE,SAAS,OAAQwE,EAAe1M,GACzDiK,EAAUzH,KAAKwG,aAAaiB,GAC5BqB,EAAYmB,GAAaxC,EACzBG,EAAYH,GAAWwC,C,CAEzBC,EAAgB1M,EAAI,EACpByM,GAAa,C,CAGjB,MAAO,CAAErC,cAAakB,cACxB,CAEA,oBAAMqB,CACJ1C,EACArD,EACAgG,EACA1D,EAAgB,CAAC,GAEbtC,EAAM,IACRA,EAAM,GAGR,MAAMsD,QAAkB1H,KAAK6G,MAAMH,GACnC,IAAKgB,EACH,MAAO,GAET,MAAMC,EAAQD,EAAUE,YAAYH,GAC9B4C,EAAK3C,EAAUf,QAAQgB,GAC7B,IAAK0C,EACH,MAAO,IAGSA,EAAGR,YAAYvL,OAC7B+L,EAAGR,YACDzF,GAtMa,IAsMYiG,EAAGR,YAAYvL,OACpC+L,EAAGR,YAAYvL,OAAS,EACxB8F,GAxMS,IA0Mf,IAAIqB,EAAc,EAAG,KAEvB6E,QAAQC,KAAK,4CAKf,MAAMC,GA5MQC,EA4MmBrG,EA5MNmE,EA4MW6B,EAzMjC,CACL,CAAC,EAAG,GACJ,CAAC,IAJHK,GAAO,IAIQ,IAAK,IAHpBlC,GAAO,IAGyB,KAC9B,CAAC,GAAKkC,GAAO,IAAK,GAAKlC,GAAO,KAC9B,CAAC,IAAMkC,GAAO,IAAK,IAAMlC,GAAO,KAChC,CAAC,KAAOkC,GAAO,IAAK,KAAOlC,GAAO,KAClC,CAAC,MAAQkC,GAAO,IAAK,MAAQlC,GAAO,OATxC,IAAkBkC,EAAalC,EA6M3B,MAAM9K,EAAkB,GAGxB,IAAK,MAAO6K,EAAOC,KAAQiC,EACzB,IAAK,IAAIvE,EAAMqC,EAAOrC,GAAOsC,EAAKtC,IAChC,GAAIoE,EAAG/C,SAASrB,GAAM,CACpB,MAAMyE,EAAYL,EAAG/C,SAASrB,GAC9B,IAAK,IAAI0E,EAAI,EAAGA,EAAID,EAAUpM,SAAUqM,EACtClN,EAAOM,KAAK,IAAIiI,EAAM0E,EAAUC,GAAG1L,KAAMyL,EAAUC,GAAGzL,KAAM+G,G,CAQpE,MAAM2E,EAAQP,EAAGR,YAAYvL,OAC7B,IAAIwG,EAAS,KACb,MAAM+F,EAASvI,KAAK8B,IAAIA,GAAO,GAAIwG,EAAQ,GACrCE,EAASxI,KAAK8B,IAAIgG,GAAO,GAAIQ,EAAQ,GAC3C,IAAK,IAAIpN,EAAIqN,EAAQrN,GAAKsN,IAAUtN,EAAG,CACrC,MAAMuN,EAAKV,EAAGR,YAAYrM,GACtBuN,KACGjG,GAAUiG,EAAGvF,UAAUV,GAAU,KACpCA,EAASiG,E,CAKf,OAAOlG,EAAepH,EAAQqH,EAChC,ECzOF,SAASkG,EAAOC,EAAaC,GAC3B,OAAO5I,KAAKC,MAAM0I,EAAM,GAAKC,EAC/B,CAEe,MAAMC,UAAY9E,EAI/B,WAAAxG,CAAY+F,GACVwF,MAAMxF,GACN5F,KAAKsJ,aAAe,EACpBtJ,KAAKqL,MAAQ,EACbrL,KAAKsL,SAAW,CAClB,CACA,eAAM9D,CAAUC,EAAiBf,EAAgB,CAAC,GAChD,MAAMgB,QAAkB1H,KAAK6G,MAAMH,GACnC,IAAKgB,EACH,OAAQ,EAEV,MAAMC,EAAQD,EAAUE,YAAYH,GAEpC,IADYC,EAAUf,QAAQgB,GAE5B,OAAQ,EAEV,MAAM,MAAEE,GAAUH,EAAUf,QAAQgB,GACpC,OAAIE,EACKA,EAAML,WAEP,CACV,CACA,cAAM+D,GACJ,MAAM,IAAIrN,MAAM,sCAClB,CAEA,YAAAsN,CAAa1F,EAAerH,GAC1B,MAAMwJ,EAAcnC,EAAMkC,YAAYvJ,GAChCyJ,EACU,MAAdD,EAAwB,uBAAyB,iBAC7CE,EAAS,CAAE,EAAG,UAAW,EAAG,MAAO,EAAG,OAAsB,GAAdF,GACpD,IAAKE,EACH,MAAM,IAAIjK,MAAM,qCAAqC+J,KAEvD,MAAMG,EAAgB,CACpBC,IAAKvC,EAAMkC,YAAYvJ,EAAS,GAChC6J,MAAOxC,EAAMkC,YAAYvJ,EAAS,GAClC8J,IAAKzC,EAAMkC,YAAYvJ,EAAS,KAE5B+J,EAAY1C,EAAMkC,YAAYvJ,EAAS,IACvCgK,EAAWD,EAAYE,OAAOC,aAAaH,GAAa,KACxDI,EAAY9C,EAAMkC,YAAYvJ,EAAS,IACvCoK,EAAoB/C,EAAMkC,YAAYvJ,EAAS,KAE/C,YAAEqK,EAAW,YAAElB,GAAgB5H,KAAK+I,gBACxCjD,EAAMxF,MAAM7B,EAAS,GAAIA,EAAS,GAAKoK,IAGzC,MAAO,CACLC,cACAlB,cACAgB,YACAH,WACAL,gBACAD,SACAD,iBAEJ,CAEA,eAAAa,CAAgBiB,GACd,IAAIC,EAAY,EACZC,EAAgB,EACpB,MAAMpB,EAAc,GACdlB,EAAyC,CAAC,EAChD,IAAK,IAAIpK,EAAI,EAAGA,EAAIwM,EAAW1L,OAAQd,GAAK,EAC1C,IAAKwM,EAAWxM,GAAI,CAClB,GAAI0M,EAAgB1M,EAAG,CACrB,IAAIiK,EAAUuC,EAAWtE,SAAS,OAAQwE,EAAe1M,GACzDiK,EAAUzH,KAAKwG,aAAaiB,GAC5BqB,EAAYmB,GAAaxC,EACzBG,EAAYH,GAAWwC,C,CAEzBC,EAAgB1M,EAAI,EACpByM,GAAa,C,CAGjB,MAAO,CAAErC,cAAakB,cACxB,CAIA,YAAM5B,CAAOR,EAAgB,CAAC,GAC5B,MAAMZ,QAAc,IAAA1I,aAAa4C,KAAKF,WAAWgI,SAASpB,IAG1D,IAAI+E,EACJ,GApGe,WAoGX3F,EAAMtC,aAAa,GACrBiI,EAAa,MACR,IArGQ,WAqGJ3F,EAAMtC,aAAa,GAG5B,MAAM,IAAItF,MAAM,kBAFhBuN,EAAa,C,CAMfzL,KAAKsL,SAAWxF,EAAMkC,YAAY,GAClChI,KAAKqL,MAAQvF,EAAMkC,YAAY,GAC/BhI,KAAKsJ,eAAiB,GAAyB,GAAlBtJ,KAAKqL,MAAQ,IAAW,GAAK,EAC1D,MAAMvB,EAAe,IAAM9J,KAAKsL,SAAwB,EAAbtL,KAAKqL,OAC1CK,EAAY5F,EAAMkC,YAAY,IAC9B2D,EACJD,GAAaA,GAAa,GACtB1L,KAAKwL,aAAa1F,EAAO,IACzB,CACEgD,YAAa,GACblB,YAAa,CAAC,EACda,SAAU,KACVL,cAAe,CAAEC,IAAK,EAAGC,MAAO,EAAGC,IAAK,GACxCL,eAAgB,uBAChBC,OAAQ,WAEVJ,EAAWjC,EAAMkC,YAAY,GAAK0D,GAGxC,IAAI1C,EACAC,EAAa,GAAKyC,EAAY,EAClC,MAAM/E,EAAU,IAAIvF,MAAM2G,GAAUmB,KAAK,GAAGC,KAAI,KAE9C,MAAMC,EAAWtD,EAAMkC,YAAYiB,GACnCA,GAAc,EACd,MAAM3B,EAAuC,CAAC,EAC9C,IAAIO,EACJ,IAAK,IAAIwB,EAAI,EAAGA,EAAID,EAAUC,GAAK,EAAG,CACpC,MAAMpD,EAAMH,EAAMtC,aAAayF,GAC/B,GAAIhD,EAAMjG,KAAKsJ,aAGbzB,EAAQ7H,KAAKwJ,eAAe1D,EAAOmD,EAAa,GAChDA,GAAc,OACT,CACL,MAAM2C,EAAU/F,EAAUC,EAAOmD,EAAa,GAC9CD,EAAgBhJ,KAAK8G,eAAekC,EAAe4C,GACnD,MAAMrC,EAAazD,EAAMkC,YAAYiB,EAAa,IAClDA,GAAc,GACd,MAAMxL,EAAS,IAAI2D,MAAMmI,GACzB,IAAK,IAAIE,EAAI,EAAGA,EAAIF,EAAYE,GAAK,EAAG,CACtC,MAAMC,EAAI7D,EAAUC,EAAOmD,GACrBU,EAAI9D,EAAUC,EAAOmD,EAAa,GACxCA,GAAc,GAEdxL,EAAOgM,GAAK,IAAIzD,EAAM0D,EAAGC,EAAG1D,E,CAE9BqB,EAASrB,GAAOxI,C,EAIpB,MAAO,CAAE6J,WAAUO,QAAO,IAG5B,MAAO,IACF8D,EACHE,KAAK,EACL9D,WACAgC,aAAc,MACdf,gBACAyC,aACA9E,UACA0E,MAAOrL,KAAKqL,MACZ/B,aAActJ,KAAKsJ,aACnBQ,eAEJ,CAEA,cAAAN,CAAe1D,EAAerH,GAO5B,MAAO,CAAE+I,UANSlD,EAChB,gBACEwB,EAAMxF,MAAM7B,EAAS,GAAIA,EAAS,KAClC,IAIN,CAEA,oBAAM0L,CACJ1C,EACArD,EACAgG,EACA1D,EAAgB,CAAC,GAEbtC,EAAM,IACRA,EAAM,GAGR,MAAMsD,QAAkB1H,KAAK6G,MAAMH,GACnC,IAAKgB,EACH,MAAO,GAET,MAAMC,EAAQD,EAAUE,YAAYH,GAC9B4C,EAAK3C,EAAUf,QAAQgB,GAC7B,IAAK0C,EACH,MAAO,GAKT,MAAMG,EAAkBxK,KAAK8L,SAAS1H,EAAKgG,GACrC3M,EAAkB,GAGxB,IAAK,MAAO6K,EAAOC,KAAQiC,EACzB,IAAK,IAAIvE,EAAMqC,EAAOrC,GAAOsC,EAAKtC,IAChC,GAAIoE,EAAG/C,SAASrB,GAAM,CACpB,MAAMyE,EAAYL,EAAG/C,SAASrB,GAC9B,IAAK,IAAI0E,EAAI,EAAGA,EAAID,EAAUpM,SAAUqM,EACtClN,EAAOM,KAAK,IAAIiI,EAAM0E,EAAUC,GAAG1L,KAAMyL,EAAUC,GAAGzL,KAAM+G,G,CAMpE,OAAOpB,EAAepH,EAAQ,IAAIgI,EAAc,EAAG,GACrD,CAKA,QAAAqG,CAASrB,EAAalC,IACpBkC,GAAO,GACG,IACRA,EAAM,GAEJlC,EAAM,GAAK,KACbA,EAAM,GAAK,IAEbA,GAAO,EACP,IAAIwD,EAAI,EACJC,EAAI,EACJC,EAAIjM,KAAKsL,SAAwB,EAAbtL,KAAKqL,MAC7B,MAAMa,EAAO,GACb,KAAOH,GAAK/L,KAAKqL,MAAOY,GAAK,EAAGD,GAAY,EA/OjC,IA+OwC,EAAJD,GAAQA,GAAK,EAAG,CAC7D,MAAMpG,EAAIqG,EAAIhB,EAAOP,EAAKwB,GACpBpN,EAAImN,EAAIhB,EAAOzC,EAAK0D,GAC1B,GAAIpN,EAAI8G,EAAIuG,EAAK5N,OAAS0B,KAAKsJ,aAC7B,MAAM,IAAIpL,MACR,SAASuM,KAAOlC,oDAAsDvI,KAAKsL,mBAAmBtL,KAAKqL,iEAGvGa,EAAKnO,KAAK,CAAC4H,EAAG9G,G,CAEhB,OAAOqN,CACT,EC1PF,MAAMC,EACmB,oBAAhBC,YAA8B,IAAIA,YAAY,cAAW/I,EAclE,SAASgJ,EAAQC,GACf,OAAO,IAAIC,SAAQC,GAAWC,WAAWD,EAASF,IACpD,CACe,MAAMI,EAqBnB,WAAA7M,EAAY,KACVE,EAAI,WACJD,EAAU,QACV6M,EAAO,cACPC,EAAa,QACbC,EAAO,cACPC,EAAa,UACbC,EAAY,IAAG,eACfC,EAAiB,IAAQ,cACzB1G,EAAgBC,IAAKA,GAAC,eACtB0G,EAAiB,UAajB,GAAInN,EACFE,KAAKF,WAAaA,MACb,KAAIC,EAGT,MAAM,IAAIE,UAAU,0CAFpBD,KAAKF,WAAa,IAAI,KAAUC,E,CAKlC,GAAI6M,EACF5M,KAAKc,MAAQ,IAAI,EAAI,CACnBhB,WAAY8M,EACZtG,uBAEG,GAAIwG,EACT9M,KAAKc,MAAQ,IAAIqK,EAAI,CACnBrL,WAAYgN,EACZxG,uBAEG,GAAIqG,EACT3M,KAAKc,MAAQ,IAAI,EAAI,CACnBhB,WAAY,IAAI,KAAU6M,GAC1BrG,uBAEG,GAAIuG,EACT7M,KAAKc,MAAQ,IAAIqK,EAAI,CACnBrL,WAAY,IAAI,KAAU+M,GAC1BvG,sBAEG,KAAIvG,EAMT,MAAM,IAAIE,UACR,yEANFD,KAAKc,MAAQ,IAAI,EAAI,CACnBhB,WAAY,IAAI,KAAU,GAAGC,SAC7BuG,iB,CAQJtG,KAAKgN,eAAiBA,EACtBhN,KAAKwG,aAAeF,EACpBtG,KAAK+M,UAAYA,EACjB/M,KAAKkN,WAAa,IAAI,IAAJ,CAA4C,CAC5DC,MAAO,IAAI,IAAJ,CAAQ,CAAEC,QAAS9K,KAAKC,MAAM0K,EAAiB,SACtD/D,KAAM,CAACtD,EAAanB,IAClBzE,KAAKqN,UAAUzH,EAAM,CAAEnB,YAE7B,CASA,cAAM6I,CACJ7F,EACAa,EACAC,EACA7B,GAEA,IAAIjC,EAEA8I,EADAC,EAAmB,CAAC,EAExB,QAAoB,IAAT9G,EACT,MAAM,IAAIzG,UAAU,kCAQtB,GANoB,mBAATyG,EACT6G,EAAW7G,GAEX8G,EAAU9G,EACV6G,EAAW7G,EAAK+G,mBAEFpK,IAAZoE,EACF,MAAM,IAAIxH,UAAU,0CAEtB,IAAKsN,EACH,MAAM,IAAItN,UAAU,kCAGtB,MAAMyN,QAAiB1N,KAAKc,MAAM2F,YAAY+G,GAQ9C,GAPAhJ,EAAiBC,GACZ6D,IACHA,EAAQ,GAELC,IACHA,EAAMmF,EAAS5D,gBAEXxB,GAASC,GACb,MAAM,IAAItI,UACR,8EAGJ,GAAIqI,IAAUC,EACZ,OAGF,MAAM9K,QAAeuC,KAAKc,MAAMqJ,eAAe1C,EAASa,EAAOC,EAAKiF,GACpEhJ,EAAiBC,GAIjB,IAAK,IAAIjH,EAAI,EAAGA,EAAIC,EAAOa,OAAQd,GAAK,EAAG,CACzC,MAAM0F,EAAOzF,EAAOD,GAAG0I,cACvB,GAAIhD,EAAOlD,KAAKgN,eACd,MAAM,IAAI9O,MACR,6BAA6BgF,EAAKyK,oDAAoD3N,KAAKgN,eAAeW,oB,CAMhH,IAAIC,EAAOC,KAAKC,MAChB,IAAK,IAAIC,EAAW,EAAGA,EAAWtQ,EAAOa,OAAQyP,GAAY,EAAG,CAC9D,IAAIC,EACJ,MAAMrD,EAAIlN,EAAOsQ,IACX,OAAEtO,EAAM,WAAEF,EAAU,WAAEC,SAAqBQ,KAAKkN,WAAWe,IAC/DtD,EAAEjF,WACFiF,GAGFnG,EAAiBC,GACjB,IAAIyJ,EAAa,EACb3Q,EAAM,EACV,KAAO2Q,EAAazO,EAAOnB,QAAQ,CACjC,MAAMiI,EAAI9G,EAAO0O,QAAQ,KAAMD,GAC/B,IAAW,IAAP3H,EACF,MAEF,MAAMZ,EAAIlG,EAAOa,MAAM4N,EAAY3H,GAC7B6H,GAAOjC,aAAO,EAAPA,EAASkC,OAAO1I,KAAMA,EAAED,WAErC,GAAIlG,EAAY,CACd,KAAO0O,EAAavD,EAAE1L,KAAKK,cAAgBE,EAAWjC,OACtDA,G,CAIF,MAAM,gBAAE+Q,EAAe,SAAEC,GAAavO,KAAKwO,UACzCd,EACAjG,EACAa,EACAC,EACA6F,GAKF,QAC8B/K,IAA5B2K,QACoB3K,IAApBiL,GACAN,EAA0BM,EAE1B,MAAM,IAAIpQ,MACR,yCAAyC8P,OAA6BM,2CAK1E,GAFAN,EAA0BM,EAEtBC,EACFhB,EACEa,EAAKK,OASa,IAAlBlP,EAAWhC,IACR2Q,EAAa1O,EAAWjC,IACzBoN,EAAE1L,KAAKK,aACP,QAEC,QAAwB+D,IAApBiL,GAAiCA,GAAmB/F,EAI7D,OAIEvI,KAAK+M,WAAaa,EAAOC,KAAKC,MAAQ9N,KAAK+M,YAC7Ca,EAAOC,KAAKC,MACZtJ,EAAiBC,SACX4H,EAAQ,IAEhB6B,EAAa3H,EAAI,C,EAGvB,CAEA,iBAAME,CAAYC,EAAgB,CAAC,GACjC,OAAO1G,KAAKc,MAAM2F,YAAYC,EAChC,CAOA,qBAAMgI,CAAgBhI,EAAgB,CAAC,GACrC,MAAM,cAAEsC,EAAa,SAAEP,EAAQ,aAAEsB,SAAuB/J,KAAKyG,YAC3DC,GAEFlC,EAAiBkC,EAAKjC,QACtB,MAAMkK,IAAY3F,aAAa,EAAbA,EAAe5J,gBAAiB,GAAK2K,EAIvD,IAAIjE,QAAc9F,KAAK4O,YAAY,EAAGD,EAAUjI,GAChDlC,EAAiBkC,EAAKjC,QACtB,IACEqB,QAAc,IAAA1I,OAAM0I,E,CACpB,MAAOjH,GAEP,MADAyL,QAAQuE,MAAMhQ,GACR,IAAIX,MAER,6BAA6BW,EAAE+F,qBAAqB+J,MAAa9P,I,CAKrE,GAAI4J,EAAU,CAEZ,IAAIqG,GAAe,EACnB,MAAMC,EAAc,KAAKC,WAAW,GAC9BC,EAAWxG,EAASuG,WAAW,GACrC,IAAK,IAAIxR,EAAI,EAAGA,EAAIsI,EAAMxH,SACpBd,IAAMsR,EAAc,GAAKhJ,EAAMtI,KAAOyR,GADVzR,GAAK,EAIjCsI,EAAMtI,KAAOuR,IACfD,EAActR,GAGlBsI,EAAQA,EAAMxF,MAAM,EAAGwO,EAAc,E,CAEvC,OAAOhJ,CACT,CAQA,eAAMoJ,CAAUxI,EAAgB,CAAC,GAE/B,aADoB1G,KAAK0O,gBAAgBhI,IAC5BhB,SAAS,OACxB,CAOA,+BAAMyJ,CAA0BzI,EAAgB,CAAC,GAE/C,aADuB1G,KAAKyG,YAAYC,IACxBoC,WAClB,CAYA,SAAA0F,CACEd,EACA0B,EACAC,EACAC,EACAlB,GAEA,MAAM,cAAEhG,EAAa,SAAEK,EAAQ,eAAEP,EAAc,OAAEC,GAAWuF,EAE5D,GAAIU,EAAKmB,OAAO,KAAO9G,EACrB,MAAO,CAAE8F,UAAU,GAIrB,IAAI,IAAElG,EAAG,MAAEC,EAAK,IAAEC,GAAQH,EACrBC,IACHA,EAAM,GAEHC,IACHA,EAAQ,GAELC,IACHA,EAAM,GAEO,QAAXJ,IACFI,EAAM,GAER,MAAMiH,EAAYlN,KAAK8H,IAAI/B,EAAKC,EAAOC,GAMvC,IAAIkH,EAAsB,EACtBC,EAAqB,EACrBC,EAAS,GACTrB,GAAkB,IACtB,IAAK,IAAI9Q,EAAI,EAAGA,EAAI4Q,EAAK9P,OAAS,EAAGd,GAAK,EACxC,GAAgB,OAAZ4Q,EAAK5Q,IAAeA,IAAM4Q,EAAK9P,OAAQ,CACzC,GAAImR,IAAwBpH,GAC1B,GACErI,KAAKwG,aAAa4H,EAAK9N,MAAMoP,EAAoBlS,MACjD4R,EAEA,MAAO,CAAEb,UAAU,QAEhB,GAAIkB,IAAwBnH,EAAO,CAMxC,GALAgG,EAAkBsB,SAASxB,EAAK9N,MAAMoP,EAAoBlS,GAAI,IAEvC,mBAAnB0K,IACFoG,GAAmB,GAEjBA,GAAmBgB,EACrB,MAAO,CAAEhB,kBAAiBC,UAAU,GAEtC,IAAY,IAARhG,GAAaA,IAAQD,IAEnBgG,EAAkB,GAAKe,EACzB,MAAO,CAAEf,kBAAiBC,UAAU,E,MAGnC,GAAe,QAAXpG,GAA4C,IAAxBsH,EAC7BE,EAASvB,EAAK9N,MAAMoP,EAAoBlS,QACnC,GAAIiS,IAAwBlH,EAAK,CACtC,IAAIsH,EAWJ,GAREA,EADa,QAAX1H,EACcnI,KAAK8P,WACnBxB,EACAqB,EACAvB,EAAK9N,MAAMoP,EAAoBlS,IAGjBoS,SAASxB,EAAK9N,MAAMoP,EAAoBlS,GAAI,IAE1DqS,GAAiBR,EACnB,MAAO,CAAEd,UAAU,E,CAKvB,GAFAmB,EAAqBlS,EAAI,EACzBiS,GAAuB,EACnBA,EAAsBD,EACxB,K,CAIN,MAAO,CAAElB,kBAAiBC,UAAU,EACtC,CAEA,UAAAuB,CAAWxB,EAAyBqB,EAAgBI,GAClD,IAAIF,EAAgBvB,EAAkBqB,EAAOrR,OAM7C,MAAM0R,GAAwC,IAAhCD,EAAK5B,QAAQ,cAC3B,GAAgB,MAAZ4B,EAAK,IAAeC,GAajB,GAAIA,EACT,OAAO1B,EAAkB,MAdI,CAC7B,IAAI2B,EAAW,IACf,IAAK,IAAI5G,EAAI,EAAGA,EAAI0G,EAAKzR,OAAQ+K,GAAK,EAAG,CACvC,GAAiB,MAAb4G,GAA6C,SAAzBF,EAAKzP,MAAM+I,EAAGA,EAAI,GAAe,CACvD,IAAI6G,EAAWH,EAAK5B,QAAQ,IAAK9E,IACf,IAAd6G,IACFA,EAAWH,EAAKzR,QAElBuR,EAAgBD,SAASG,EAAKzP,MAAM+I,EAAI,EAAG6G,GAAW,IACtD,K,CAEFD,EAAWF,EAAK1G,E,EAKpB,OAAOwG,CACT,CAOA,eAAMrI,CAAUC,EAAiBf,EAAgB,CAAC,GAChD,OAAO1G,KAAKc,MAAM0G,UAAUC,EAASf,EACvC,CAEA,iBAAMkI,CAAYrR,EAAa2F,EAAcwD,EAAgB,CAAC,GAC5D,MAAMf,EAAI,EAAAhH,OAAOwR,MAAMjN,IACjB,UAAEK,EAAS,OAAE9D,SAAiBO,KAAKF,WAAWmB,KAClD0E,EACA,EACAzC,EACA3F,EACAmJ,GAGF,OAAOjH,EAAOa,MAAM,EAAGiD,EACzB,CAMA,eAAM8J,CAAU1C,EAAUjE,EAAgB,CAAC,GAIzC,MAAM0J,QAAapQ,KAAK4O,YACtBjE,EAAE1L,KAAKG,cACPuL,EAAEzE,cACFQ,GAEF,IACE,OAAO,QAAgB0J,EAAMzF,E,CAC7B,MAAO9L,GACP,MAAM,IAAIX,MAAM,yBAAyByM,EAAEjF,cAAc7G,I,CAE7D,E,wBCzfF,IAAIwR,EAAmBrQ,MAAQA,KAAKqQ,iBAAoB,SAAUC,GAC9D,OAAQA,GAAOA,EAAIC,WAAcD,EAAM,CAAE,QAAWA,EACxD,EACAtN,OAAOwN,eAAeC,EAAS,aAAc,CAAEC,OAAO,IACtD,MAAMC,EAA6B,EAAQ,OACrCC,EAA6BP,EAAgB,EAAQ,QACrDQ,EAA4BR,EAAgB,EAAQ,OAC1D,MAAMS,EACF,WAAAjR,EAAY,KAAEqJ,EAAI,MAAEiE,IAChB,GAAoB,mBAATjE,EACP,MAAM,IAAIjJ,UAAU,6BAExB,GAAqB,iBAAVkN,EACP,MAAM,IAAIlN,UAAU,4BAExB,GAAyB,mBAAdkN,EAAMc,KACQ,mBAAdd,EAAMzO,KACW,mBAAjByO,EAAM4D,OACb,MAAM,IAAI9Q,UAAU,qEAExBD,KAAKmN,MAAQA,EACbnN,KAAKgR,aAAe9H,CACxB,CACA,uBAAO+H,CAAiBC,GACpB,MAEmB,eAAnBA,EAAUC,MAGa,gBAAnBD,EAAUtM,MAEY,wBAAtBsM,EAAUE,SAEY,mBAAtBF,EAAUE,OAClB,CACA,KAAAC,CAAMC,EAAKvP,GACH/B,KAAKmN,MAAMc,IAAIqD,KAASvP,GACxB/B,KAAKmN,MAAM4D,OAAOO,EAE1B,CACA,IAAApI,CAAKoI,EAAKlB,EAAM3L,EAAQ8M,GACpB,MAAMC,EAAU,IAAIZ,EAA2Ba,QACzCC,EAAiB,IAAIb,EAA0BY,QACrDC,EAAeC,YAAYJ,GAC3B,MAAMK,EAAW,CACbJ,QAASA,EACTK,QAAS7R,KAAKgR,aAAaZ,EAAMoB,EAAQ/M,QAAS2M,IAC9CM,EAAenE,SAAS6D,EAAQ,IAEpCU,SAAS,EACTJ,iBACA,WAAIhN,GACA,OAAO1E,KAAKwR,QAAQ/M,OAAOC,OAC/B,GAEJkN,EAASJ,QAAQO,UAAUtN,GAE3BmN,EAASJ,QAAQ/M,OAAOuN,iBAAiB,SAAS,KACzCJ,EAASE,SACV9R,KAAKqR,MAAMC,EAAKM,EACpB,IAGJA,EAASC,QACJI,MAAK,KACNL,EAASE,SAAU,CAAI,IACxB,KACCF,EAASE,SAAU,EAEnB9R,KAAKqR,MAAMC,EAAKM,EAAS,IAExBzK,OAAMtI,IAIP,MADAyL,QAAQuE,MAAMhQ,GACRA,CAAC,IAEXmB,KAAKmN,MAAMzO,IAAI4S,EAAKM,EACxB,CACA,yBAAOM,CAAmBL,EAASpN,GAI/B,SAAS0N,IACL,GAAI1N,GAAUA,EAAOC,QACjB,MAAM1B,OAAOC,OAAO,IAAI/E,MAAM,WAAY,CAAE0G,KAAM,eAE1D,CACA,OAAOiN,EAAQI,MAAK5T,IAChB8T,IACO9T,KACRwQ,IAEC,MADAsD,IACMtD,CAAK,GAEnB,CACA,GAAAuD,CAAId,GACA,OAAOtR,KAAKmN,MAAMiF,IAAId,EAC1B,CAaA,GAAArD,CAAIqD,EAAKlB,EAAM3L,EAAQ8M,GACnB,IAAK9M,GAAU2L,aAAgBO,EAA2B0B,YACtD,MAAM,IAAIpS,UAAU,yGAExB,MAAMqS,EAAatS,KAAKmN,MAAMc,IAAIqD,GAClC,OAAIgB,EACIA,EAAW5N,UAAY4N,EAAWR,SAElC9R,KAAKqR,MAAMC,EAAKgB,GACTtS,KAAKiO,IAAIqD,EAAKlB,EAAM3L,EAAQ8M,IAEnCe,EAAWR,QAEJQ,EAAWT,SAItBS,EAAWd,QAAQO,UAAUtN,GAC7B6N,EAAWZ,eAAeC,YAAYJ,GAC/BT,EAAsBoB,mBAAmBI,EAAWT,QAASpN,KAGxEzE,KAAKkJ,KAAKoI,EAAKlB,EAAM3L,EAAQ8M,GACtBT,EAAsBoB,mBAG7BlS,KAAKmN,MAAMc,IAAIqD,GAAKO,QAASpN,GACjC,CAOA,OAAO6M,GACH,MAAMiB,EAAcvS,KAAKmN,MAAMc,IAAIqD,GAC/BiB,IACKA,EAAYT,SACbS,EAAYf,QAAQgB,QAExBxS,KAAKmN,MAAM4D,OAAOO,GAE1B,CAKA,KAAAmB,GAEI,MAAMC,EAAU1S,KAAKmN,MAAMwF,OAC3B,IAAIC,EAAc,EAClB,IAAK,IAAIvU,EAASqU,EAAQ9O,QAASvF,EAAOwU,KAAMxU,EAASqU,EAAQ9O,OAC7D5D,KAAK+Q,OAAO1S,EAAOqS,OACnBkC,GAAe,EAEnB,OAAOA,CACX,EAEJnC,EAAA,QAAkBK,C,kBCzKlB9N,OAAOwN,eAAeC,EAAS,aAAc,CAAEC,OAAO,IACtD,MAAMC,EAA6B,EAAQ,OAC3C,MAAMmC,GAgDNrC,EAAA,QA1CA,MACI,WAAA5Q,GACIG,KAAK+S,QAAU,IAAIC,IACnBhT,KAAKiT,gBAAkB,IAAItC,EAA2BuC,eAC1D,CAOA,SAAAnB,CAAUtN,EAAS,IAAIqO,GACnB,GAAI9S,KAAKyE,OAAOC,QACZ,MAAM,IAAIxG,MAAM,yCAIpB8B,KAAK+S,QAAQI,IAAI1O,GACbA,EAAOC,QAGP1E,KAAKoT,cAAc3O,GAEqB,mBAA5BA,EAAOuN,kBACnBvN,EAAOuN,iBAAiB,SAAS,KAC7BhS,KAAKoT,cAAc3O,EAAO,GAGtC,CACA,aAAA2O,CAAc3O,GACVzE,KAAK+S,QAAQhC,OAAOtM,GACM,IAAtBzE,KAAK+S,QAAQ7P,MACblD,KAAKiT,gBAAgBT,OAE7B,CACA,UAAI/N,GACA,OAAOzE,KAAKiT,gBAAgBxO,MAChC,CACA,KAAA+N,GACIxS,KAAKiT,gBAAgBT,OACzB,E,eChDJxP,OAAOwN,eAAeC,EAAS,aAAc,CAAEC,OAAO,IAgBtDD,EAAA,QAfA,MACI,WAAA5Q,GACIG,KAAKqT,UAAY,IAAIL,GACzB,CACA,WAAArB,CAAYpE,EAAW,UACnBvN,KAAKqT,UAAUF,IAAI5F,GACnBA,EAASvN,KAAKsT,eAClB,CACA,QAAA/F,CAAS6D,GACLpR,KAAKsT,eAAiBlC,EACtBpR,KAAKqT,UAAUhO,SAAQkO,IACnBA,EAAInC,EAAQ,GAEpB,E,kBCbJpO,OAAOwN,eAAeC,EAAS,aAAc,CAAEC,OAAO,IACtDD,EAAQ4B,YAAc5B,EAAQyC,qBAAkB,EAChD,MAAMM,EAAiB,EAAQ,OAC/B,IAAIC,EAAY,WAIZ,GAAoB,oBAATC,KACP,OAAOA,KAEX,GAAsB,oBAAXC,OACP,OAAOA,OAEX,QAAsB,IAAX,EAAAC,EACP,OAAO,EAAAA,EAEX,MAAM,IAAI1V,MAAM,iCACpB,EAEA,IAAIgV,OAAyD,IAAhCO,IAAYP,gBAAkCM,EAAeN,gBAAkBO,IAAYP,gBACxHzC,EAAQyC,gBAAkBA,EAE1B,IAAIb,OAAqD,IAAhCoB,IAAYP,gBAAkCM,EAAenB,YAAcoB,IAAYpB,YAChH5B,EAAQ4B,YAAcA,C,uBCxBtB,IAAIhC,EAAmBrQ,MAAQA,KAAKqQ,iBAAoB,SAAUC,GAC9D,OAAQA,GAAOA,EAAIC,WAAcD,EAAM,CAAE,QAAWA,EACxD,EACAtN,OAAOwN,eAAeC,EAAS,aAAc,CAAEC,OAAO,IACtD,MAAMmD,EAA0BxD,EAAgB,EAAQ,QACxDI,EAAA,QAAkBoD,EAAwBpC,O","sources":["../../../node_modules/@gmod/bgzf-filehandle/src/unzip-pako.ts","../../../node_modules/@gmod/bgzf-filehandle/src/gziIndex.ts","../../../node_modules/@gmod/bgzf-filehandle/src/bgzFilehandle.ts","../../../node_modules/@gmod/tabix/src/util.ts","../../../node_modules/@gmod/tabix/src/virtualOffset.ts","../../../node_modules/@gmod/tabix/src/chunk.ts","../../../node_modules/@gmod/tabix/src/indexFile.ts","../../../node_modules/@gmod/tabix/src/tbi.ts","../../../node_modules/@gmod/tabix/src/csi.ts","../../../node_modules/@gmod/tabix/src/tabixIndexedFile.ts","../../../node_modules/abortable-promise-cache/esm/AbortablePromiseCache.js","../../../node_modules/abortable-promise-cache/esm/AggregateAbortController.js","../../../node_modules/abortable-promise-cache/esm/AggregateStatusReporter.js","../../../node_modules/abortable-promise-cache/esm/abortcontroller-ponyfill.js","../../../node_modules/abortable-promise-cache/esm/index.js"],"sourcesContent":["import { Buffer } from 'buffer'\n//@ts-ignore\nimport { Z_SYNC_FLUSH, Inflate } from 'pako'\n\ninterface VirtualOffset {\n  blockPosition: number\n  dataPosition: number\n}\ninterface Chunk {\n  minv: VirtualOffset\n  maxv: VirtualOffset\n}\n\n// browserify-zlib, which is the zlib shim used by default in webpacked code,\n// does not properly uncompress bgzf chunks that contain more than\n// one bgzf block, so export an unzip function that uses pako directly\n// if we are running in a browser.\nasync function unzip(inputData: Buffer) {\n  try {\n    let strm\n    let pos = 0\n    let i = 0\n    const chunks = []\n    let totalSize = 0\n    let inflator\n    do {\n      const remainingInput = inputData.subarray(pos)\n      inflator = new Inflate()\n      //@ts-ignore\n      ;({ strm } = inflator)\n      inflator.push(remainingInput, Z_SYNC_FLUSH)\n      if (inflator.err) {\n        throw new Error(inflator.msg)\n      }\n\n      pos += strm.next_in\n      chunks[i] = inflator.result as Uint8Array\n      totalSize += chunks[i].length\n      i += 1\n    } while (strm.avail_in)\n\n    const result = new Uint8Array(totalSize)\n    for (let i = 0, offset = 0; i < chunks.length; i++) {\n      result.set(chunks[i], offset)\n      offset += chunks[i].length\n    }\n    return Buffer.from(result)\n  } catch (e) {\n    //cleanup error message\n    if (`${e}`.match(/incorrect header check/)) {\n      throw new Error(\n        'problem decompressing block: incorrect gzip header check',\n      )\n    }\n    throw e\n  }\n}\n\n// similar to pakounzip, except it does extra counting\n// to return the positions of compressed and decompressed\n// data offsets\nasync function unzipChunk(inputData: Buffer) {\n  try {\n    let strm\n    let cpos = 0\n    let dpos = 0\n    const blocks = []\n    const cpositions = []\n    const dpositions = []\n    do {\n      const remainingInput = inputData.slice(cpos)\n      const inflator = new Inflate()\n      // @ts-ignore\n      ;({ strm } = inflator)\n      inflator.push(remainingInput, Z_SYNC_FLUSH)\n      if (inflator.err) {\n        throw new Error(inflator.msg)\n      }\n\n      const buffer = Buffer.from(inflator.result)\n      blocks.push(buffer)\n\n      cpositions.push(cpos)\n      dpositions.push(dpos)\n\n      cpos += strm.next_in\n      dpos += buffer.length\n    } while (strm.avail_in)\n\n    const buffer = Buffer.concat(blocks)\n    return { buffer, cpositions, dpositions }\n  } catch (e) {\n    //cleanup error message\n    if (`${e}`.match(/incorrect header check/)) {\n      throw new Error(\n        'problem decompressing block: incorrect gzip header check',\n      )\n    }\n    throw e\n  }\n}\n\n// similar to unzipChunk above but slices (0,minv.dataPosition) and\n// (maxv.dataPosition,end) off\nasync function unzipChunkSlice(inputData: Buffer, chunk: Chunk) {\n  try {\n    let strm\n    const { minv, maxv } = chunk\n    let cpos = minv.blockPosition\n    let dpos = minv.dataPosition\n    const chunks = []\n    const cpositions = []\n    const dpositions = []\n\n    let totalSize = 0\n    let i = 0\n    do {\n      const remainingInput = inputData.subarray(cpos - minv.blockPosition)\n      const inflator = new Inflate()\n      // @ts-ignore\n      ;({ strm } = inflator)\n      inflator.push(remainingInput, Z_SYNC_FLUSH)\n      if (inflator.err) {\n        throw new Error(inflator.msg)\n      }\n\n      const buffer = inflator.result\n      chunks.push(buffer as Uint8Array)\n      let len = buffer.length\n\n      cpositions.push(cpos)\n      dpositions.push(dpos)\n      if (chunks.length === 1 && minv.dataPosition) {\n        // this is the first chunk, trim it\n        chunks[0] = chunks[0].subarray(minv.dataPosition)\n        len = chunks[0].length\n      }\n      const origCpos = cpos\n      cpos += strm.next_in\n      dpos += len\n\n      if (origCpos >= maxv.blockPosition) {\n        // this is the last chunk, trim it and stop decompressing\n        // note if it is the same block is minv it subtracts that already\n        // trimmed part of the slice length\n\n        chunks[i] = chunks[i].subarray(\n          0,\n          maxv.blockPosition === minv.blockPosition\n            ? maxv.dataPosition - minv.dataPosition + 1\n            : maxv.dataPosition + 1,\n        )\n\n        cpositions.push(cpos)\n        dpositions.push(dpos)\n        totalSize += chunks[i].length\n        break\n      }\n      totalSize += chunks[i].length\n      i++\n    } while (strm.avail_in)\n\n    const result = new Uint8Array(totalSize)\n    for (let i = 0, offset = 0; i < chunks.length; i++) {\n      result.set(chunks[i], offset)\n      offset += chunks[i].length\n    }\n    const buffer = Buffer.from(result)\n\n    return { buffer, cpositions, dpositions }\n  } catch (e) {\n    //cleanup error message\n    if (`${e}`.match(/incorrect header check/)) {\n      throw new Error(\n        'problem decompressing block: incorrect gzip header check',\n      )\n    }\n    throw e\n  }\n}\n\nfunction nodeUnzip() {\n  throw new Error('nodeUnzip not implemented.')\n}\n\nexport { unzip, unzipChunk, unzipChunkSlice, unzip as pakoUnzip, nodeUnzip }\n","import Long from 'long'\nimport { Buffer } from 'buffer'\nimport { LocalFile, GenericFilehandle } from 'generic-filehandle'\n\n// const COMPRESSED_POSITION = 0\nconst UNCOMPRESSED_POSITION = 1\n\nexport default class GziIndex {\n  filehandle: GenericFilehandle\n\n  index?: any\n\n  constructor({\n    filehandle,\n    path,\n  }: {\n    filehandle?: GenericFilehandle\n    path?: string\n  }) {\n    if (filehandle) {\n      this.filehandle = filehandle\n    } else if (path) {\n      this.filehandle = new LocalFile(path)\n    } else {\n      throw new TypeError('either filehandle or path must be defined')\n    }\n  }\n\n  _readLongWithOverflow(buf: Buffer, offset = 0, unsigned = true) {\n    //@ts-ignore\n    const long = Long.fromBytesLE(buf.slice(offset, offset + 8), unsigned)\n    if (\n      long.greaterThan(Number.MAX_SAFE_INTEGER) ||\n      long.lessThan(Number.MIN_SAFE_INTEGER)\n    ) {\n      throw new TypeError('integer overflow')\n    }\n\n    return long.toNumber()\n  }\n\n  _getIndex() {\n    if (!this.index) {\n      this.index = this._readIndex()\n    }\n    return this.index\n  }\n\n  async _readIndex() {\n    let buf = Buffer.allocUnsafe(8)\n    await this.filehandle.read(buf, 0, 8, 0)\n    const numEntries = this._readLongWithOverflow(buf, 0, true)\n    if (!numEntries) {\n      return [[0, 0]]\n    }\n\n    const entries = new Array(numEntries + 1)\n    entries[0] = [0, 0]\n\n    // TODO rewrite this to make an index-index that stays in memory\n    const bufSize = 8 * 2 * numEntries\n    if (bufSize > Number.MAX_SAFE_INTEGER) {\n      throw new TypeError('integer overflow')\n    }\n    buf = Buffer.allocUnsafe(bufSize)\n    await this.filehandle.read(buf, 0, bufSize, 8)\n    for (let entryNumber = 0; entryNumber < numEntries; entryNumber += 1) {\n      const compressedPosition = this._readLongWithOverflow(\n        buf,\n        entryNumber * 16,\n      )\n      const uncompressedPosition = this._readLongWithOverflow(\n        buf,\n        entryNumber * 16 + 8,\n      )\n      entries[entryNumber + 1] = [compressedPosition, uncompressedPosition]\n    }\n\n    return entries\n  }\n\n  async getLastBlock() {\n    const entries = await this._getIndex()\n    if (!entries.length) {\n      return undefined\n    }\n    return entries[entries.length - 1]\n  }\n\n  async getRelevantBlocksForRead(length: number, position: number) {\n    const endPosition = position + length\n    if (length === 0) {\n      return []\n    }\n    const entries = await this._getIndex()\n    const relevant = []\n\n    // binary search to find the block that the\n    // read starts in and extend forward from that\n    const compare = (entry: any, nextEntry: any) => {\n      const uncompressedPosition = entry[UNCOMPRESSED_POSITION]\n      const nextUncompressedPosition = nextEntry\n        ? nextEntry[UNCOMPRESSED_POSITION]\n        : Infinity\n      // block overlaps read start\n      if (\n        uncompressedPosition <= position &&\n        nextUncompressedPosition > position\n      ) {\n        return 0\n        // block is before read start\n      }\n      if (uncompressedPosition < position) {\n        return -1\n      }\n      // block is after read start\n      return 1\n    }\n\n    let lowerBound = 0\n    let upperBound = entries.length - 1\n    let searchPosition = Math.floor(entries.length / 2)\n\n    let comparison = compare(\n      entries[searchPosition],\n      entries[searchPosition + 1],\n    )\n    while (comparison !== 0) {\n      if (comparison > 0) {\n        upperBound = searchPosition - 1\n      } else if (comparison < 0) {\n        lowerBound = searchPosition + 1\n      }\n      searchPosition = Math.ceil((upperBound - lowerBound) / 2) + lowerBound\n      comparison = compare(entries[searchPosition], entries[searchPosition + 1])\n    }\n\n    // here's where we read forward\n    relevant.push(entries[searchPosition])\n    let i = searchPosition + 1\n    for (; i < entries.length; i += 1) {\n      relevant.push(entries[i])\n      if (entries[i][UNCOMPRESSED_POSITION] >= endPosition) {\n        break\n      }\n    }\n    if (relevant[relevant.length - 1][UNCOMPRESSED_POSITION] < endPosition) {\n      relevant.push([])\n    }\n    return relevant\n  }\n}\n","import { Buffer } from 'buffer'\nimport { LocalFile, GenericFilehandle } from 'generic-filehandle'\n\n// locals\nimport { unzip } from './unzip'\nimport GziIndex from './gziIndex'\n\nexport default class BgzFilehandle {\n  filehandle: GenericFilehandle\n  gzi: GziIndex\n\n  constructor({\n    filehandle,\n    path,\n    gziFilehandle,\n    gziPath,\n  }: {\n    filehandle?: GenericFilehandle\n    path?: string\n    gziFilehandle?: GenericFilehandle\n    gziPath?: string\n  }) {\n    if (filehandle) {\n      this.filehandle = filehandle\n    } else if (path) {\n      this.filehandle = new LocalFile(path)\n    } else {\n      throw new TypeError('either filehandle or path must be defined')\n    }\n\n    if (!gziFilehandle && !gziPath && !path) {\n      throw new TypeError('either gziFilehandle or gziPath must be defined')\n    }\n\n    this.gzi = new GziIndex({\n      filehandle: gziFilehandle,\n      path: !gziFilehandle && !gziPath && path ? gziPath : `${path}.gzi`,\n    })\n  }\n\n  async stat() {\n    const compressedStat = await this.filehandle.stat()\n    return Object.assign(compressedStat, {\n      size: await this.getUncompressedFileSize(),\n      blocks: undefined,\n      blksize: undefined,\n    })\n  }\n\n  async getUncompressedFileSize() {\n    // read the last block's ISIZE (see gzip RFC),\n    // and add it to its uncompressedPosition\n    const [, uncompressedPosition] = await this.gzi.getLastBlock()\n\n    const { size } = await this.filehandle.stat()\n\n    const buf = Buffer.allocUnsafe(4)\n    // note: there should be a 28-byte EOF marker (an empty block) at\n    // the end of the file, so we skip backward past that\n    const { bytesRead } = await this.filehandle.read(buf, 0, 4, size - 28 - 4)\n    if (bytesRead !== 4) {\n      throw new Error('read error')\n    }\n    const lastBlockUncompressedSize = buf.readUInt32LE(0)\n    return uncompressedPosition + lastBlockUncompressedSize\n  }\n\n  async _readAndUncompressBlock(\n    blockBuffer: Buffer,\n    [compressedPosition]: [number],\n    [nextCompressedPosition]: [number],\n  ) {\n    let next = nextCompressedPosition\n    if (!next) {\n      next = (await this.filehandle.stat()).size\n    }\n\n    // read the compressed data into the block buffer\n    const blockCompressedLength = next - compressedPosition\n\n    await this.filehandle.read(\n      blockBuffer,\n      0,\n      blockCompressedLength,\n      compressedPosition,\n    )\n\n    // uncompress it\n    const unzippedBuffer = await unzip(\n      blockBuffer.slice(0, blockCompressedLength),\n    )\n\n    return unzippedBuffer as Buffer\n  }\n\n  async read(buf: Buffer, offset: number, length: number, position: number) {\n    // get the block positions for this read\n    const blockPositions = await this.gzi.getRelevantBlocksForRead(\n      length,\n      position,\n    )\n    const blockBuffer = Buffer.allocUnsafe(32768 * 2)\n    // uncompress the blocks and read from them one at a time to keep memory usage down\n    let destinationOffset = offset\n    let bytesRead = 0\n    for (\n      let blockNum = 0;\n      blockNum < blockPositions.length - 1;\n      blockNum += 1\n    ) {\n      // eslint-disable-next-line no-await-in-loop\n      const uncompressedBuffer = await this._readAndUncompressBlock(\n        blockBuffer,\n        blockPositions[blockNum],\n        blockPositions[blockNum + 1],\n      )\n      const [, uncompressedPosition] = blockPositions[blockNum]\n      const sourceOffset =\n        uncompressedPosition >= position ? 0 : position - uncompressedPosition\n      const sourceEnd =\n        Math.min(\n          position + length,\n          uncompressedPosition + uncompressedBuffer.length,\n        ) - uncompressedPosition\n      if (sourceOffset >= 0 && sourceOffset < uncompressedBuffer.length) {\n        uncompressedBuffer.copy(buf, destinationOffset, sourceOffset, sourceEnd)\n        destinationOffset += sourceEnd - sourceOffset\n        bytesRead += sourceEnd - sourceOffset\n      }\n    }\n\n    return { bytesRead, buffer: buf }\n  }\n}\n","import Chunk from './chunk'\nimport VirtualOffset from './virtualOffset'\n\nexport function longToNumber(long: Long) {\n  if (\n    long.greaterThan(Number.MAX_SAFE_INTEGER) ||\n    long.lessThan(Number.MIN_SAFE_INTEGER)\n  ) {\n    throw new Error('integer overflow')\n  }\n  return long.toNumber()\n}\n\nclass AbortError extends Error {\n  public code: string | undefined\n}\n/**\n * Properly check if the given AbortSignal is aborted.\n * Per the standard, if the signal reads as aborted,\n * this function throws either a DOMException AbortError, or a regular error\n * with a `code` attribute set to `ERR_ABORTED`.\n *\n * For convenience, passing `undefined` is a no-op\n *\n * @param {AbortSignal} [signal] an AbortSignal, or anything with an `aborted` attribute\n * @returns nothing\n */\nexport function checkAbortSignal(signal?: AbortSignal) {\n  if (!signal) {\n    return\n  }\n\n  if (signal.aborted) {\n    if (typeof DOMException !== 'undefined') {\n      // eslint-disable-next-line  no-undef\n      throw new DOMException('aborted', 'AbortError')\n    } else {\n      const e = new AbortError('aborted')\n      e.code = 'ERR_ABORTED'\n      throw e\n    }\n  }\n}\n\n/**\n * Skips to the next tick, then runs `checkAbortSignal`.\n * Await this to inside an otherwise synchronous loop to\n * provide a place to break when an abort signal is received.\n * @param {AbortSignal} signal\n */\nexport async function abortBreakPoint(signal?: AbortSignal) {\n  await Promise.resolve()\n  checkAbortSignal(signal)\n}\n\nexport function canMergeBlocks(chunk1: Chunk, chunk2: Chunk) {\n  return (\n    chunk2.minv.blockPosition - chunk1.maxv.blockPosition < 65000 &&\n    chunk2.maxv.blockPosition - chunk1.minv.blockPosition < 5000000\n  )\n}\n\nexport function optimizeChunks(chunks: Chunk[], lowest: VirtualOffset) {\n  const mergedChunks: Chunk[] = []\n  let lastChunk: Chunk | null = null\n\n  if (chunks.length === 0) {\n    return chunks\n  }\n\n  chunks.sort(function (c0, c1) {\n    const dif = c0.minv.blockPosition - c1.minv.blockPosition\n    if (dif !== 0) {\n      return dif\n    } else {\n      return c0.minv.dataPosition - c1.minv.dataPosition\n    }\n  })\n\n  chunks.forEach(chunk => {\n    if (!lowest || chunk.maxv.compareTo(lowest) > 0) {\n      if (lastChunk === null) {\n        mergedChunks.push(chunk)\n        lastChunk = chunk\n      } else {\n        if (canMergeBlocks(lastChunk, chunk)) {\n          if (chunk.maxv.compareTo(lastChunk.maxv) > 0) {\n            lastChunk.maxv = chunk.maxv\n          }\n        } else {\n          mergedChunks.push(chunk)\n          lastChunk = chunk\n        }\n      }\n    }\n  })\n\n  return mergedChunks\n}\n","import { Buffer } from 'buffer'\nexport default class VirtualOffset {\n  public blockPosition: number\n  public dataPosition: number\n  constructor(blockPosition: number, dataPosition: number) {\n    this.blockPosition = blockPosition // < offset of the compressed data block\n    this.dataPosition = dataPosition // < offset into the uncompressed data\n  }\n\n  toString() {\n    return `${this.blockPosition}:${this.dataPosition}`\n  }\n\n  compareTo(b: VirtualOffset) {\n    return (\n      this.blockPosition - b.blockPosition || this.dataPosition - b.dataPosition\n    )\n  }\n\n  static min(...args: VirtualOffset[]) {\n    let min\n    let i = 0\n    for (; !min; i += 1) {\n      min = args[i]\n    }\n    for (; i < args.length; i += 1) {\n      if (min.compareTo(args[i]) > 0) {\n        min = args[i]\n      }\n    }\n    return min\n  }\n}\nexport function fromBytes(bytes: Buffer, offset = 0, bigendian = false) {\n  if (bigendian) {\n    throw new Error('big-endian virtual file offsets not implemented')\n  }\n\n  return new VirtualOffset(\n    bytes[offset + 7] * 0x10000000000 +\n      bytes[offset + 6] * 0x100000000 +\n      bytes[offset + 5] * 0x1000000 +\n      bytes[offset + 4] * 0x10000 +\n      bytes[offset + 3] * 0x100 +\n      bytes[offset + 2],\n    (bytes[offset + 1] << 8) | bytes[offset],\n  )\n}\n","import VirtualOffset from './virtualOffset'\n\n// little class representing a chunk in the index\nexport default class Chunk {\n  public minv: VirtualOffset\n  public maxv: VirtualOffset\n  public bin: number\n  public _fetchedSize?: number\n\n  /**\n   * @param {VirtualOffset} minv\n   * @param {VirtualOffset} maxv\n   * @param {number} bin\n   * @param {number} [fetchedSize]\n   */\n  constructor(\n    minv: VirtualOffset,\n    maxv: VirtualOffset,\n    bin: number,\n    fetchedSize = undefined,\n  ) {\n    this.minv = minv\n    this.maxv = maxv\n    this.bin = bin\n    this._fetchedSize = fetchedSize\n  }\n\n  toUniqueString() {\n    return `${this.minv}..${this.maxv} (bin ${\n      this.bin\n    }, fetchedSize ${this.fetchedSize()})`\n  }\n\n  toString() {\n    return this.toUniqueString()\n  }\n\n  compareTo(b: Chunk) {\n    return (\n      this.minv.compareTo(b.minv) ||\n      this.maxv.compareTo(b.maxv) ||\n      this.bin - b.bin\n    )\n  }\n\n  fetchedSize() {\n    if (this._fetchedSize !== undefined) {\n      return this._fetchedSize\n    }\n    return this.maxv.blockPosition + (1 << 16) - this.minv.blockPosition\n  }\n}\n","import { GenericFilehandle } from 'generic-filehandle'\nimport VirtualOffset from './virtualOffset'\nimport Chunk from './chunk'\n\nexport interface Options {\n  // support having some unknown parts of the options\n  [key: string]: unknown\n  signal?: AbortSignal\n}\n\nexport interface IndexData {\n  refNameToId: { [key: string]: number }\n  refIdToName: string[]\n  metaChar: string | null\n  columnNumbers: { ref: number; start: number; end: number }\n  coordinateType: string\n  format: string\n  [key: string]: any\n}\n\nexport default abstract class IndexFile {\n  public filehandle: GenericFilehandle\n  public renameRefSeq: (arg0: string) => string\n  private parseP?: Promise<IndexData>\n\n  constructor({\n    filehandle,\n    renameRefSeqs = (n: string) => n,\n  }: {\n    filehandle: GenericFilehandle\n    renameRefSeqs?: (a: string) => string\n  }) {\n    this.filehandle = filehandle\n    this.renameRefSeq = renameRefSeqs\n  }\n\n  public abstract lineCount(refName: string, args: Options): Promise<number>\n\n  protected abstract _parse(opts: Options): Promise<IndexData>\n\n  public async getMetadata(opts: Options = {}) {\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    const { indices, ...rest } = await this.parse(opts)\n    return rest\n  }\n\n  public abstract blocksForRange(\n    refName: string,\n    start: number,\n    end: number,\n    opts: Options,\n  ): Promise<Chunk[]>\n\n  _findFirstData(\n    currentFdl: VirtualOffset | undefined,\n    virtualOffset: VirtualOffset,\n  ) {\n    if (currentFdl) {\n      return currentFdl.compareTo(virtualOffset) > 0\n        ? virtualOffset\n        : currentFdl\n    } else {\n      return virtualOffset\n    }\n  }\n\n  async parse(opts: Options = {}) {\n    if (!this.parseP) {\n      this.parseP = this._parse(opts).catch(e => {\n        this.parseP = undefined\n        throw e\n      })\n    }\n    return this.parseP\n  }\n\n  async hasRefSeq(seqId: number, opts: Options = {}) {\n    return !!((await this.parse(opts)).indices[seqId] || {}).binIndex\n  }\n}\n","import Long from 'long'\nimport { Buffer } from 'buffer'\nimport VirtualOffset, { fromBytes } from './virtualOffset'\nimport Chunk from './chunk'\nimport { unzip } from '@gmod/bgzf-filehandle'\nimport { longToNumber, optimizeChunks, checkAbortSignal } from './util'\nimport IndexFile, { Options } from './indexFile'\n\nconst TBI_MAGIC = 21578324 // TBI\\1\nconst TAD_LIDX_SHIFT = 14\n\n/**\n * calculate the list of bins that may overlap with region [beg,end) (zero-based half-open)\n */\nfunction reg2bins(beg: number, end: number) {\n  beg += 1 // < convert to 1-based closed\n  end -= 1\n  return [\n    [0, 0],\n    [1 + (beg >> 26), 1 + (end >> 26)],\n    [9 + (beg >> 23), 9 + (end >> 23)],\n    [73 + (beg >> 20), 73 + (end >> 20)],\n    [585 + (beg >> 17), 585 + (end >> 17)],\n    [4681 + (beg >> 14), 4681 + (end >> 14)],\n  ]\n}\n\nexport default class TabixIndex extends IndexFile {\n  async lineCount(refName: string, opts: Options = {}) {\n    const indexData = await this.parse(opts)\n    if (!indexData) {\n      return -1\n    }\n    const refId = indexData.refNameToId[refName]\n    const idx = indexData.indices[refId]\n    if (!idx) {\n      return -1\n    }\n    const { stats } = indexData.indices[refId]\n    if (stats) {\n      return stats.lineCount\n    }\n    return -1\n  }\n\n  // fetch and parse the index\n  async _parse(opts: Options = {}) {\n    const buf = await this.filehandle.readFile(opts)\n    const bytes = await unzip(buf)\n    checkAbortSignal(opts.signal)\n\n    // check TBI magic numbers\n    if (bytes.readUInt32LE(0) !== TBI_MAGIC /* \"TBI\\1\" */) {\n      throw new Error('Not a TBI file')\n      // TODO: do we need to support big-endian TBI files?\n    }\n\n    // number of reference sequences in the index\n    const refCount = bytes.readInt32LE(4)\n    const formatFlags = bytes.readInt32LE(8)\n    const coordinateType =\n      formatFlags & 0x10000 ? 'zero-based-half-open' : '1-based-closed'\n    const formatOpts: { [key: number]: string } = {\n      0: 'generic',\n      1: 'SAM',\n      2: 'VCF',\n    }\n    const format = formatOpts[formatFlags & 0xf]\n    if (!format) {\n      throw new Error(`invalid Tabix preset format flags ${formatFlags}`)\n    }\n    const columnNumbers = {\n      ref: bytes.readInt32LE(12),\n      start: bytes.readInt32LE(16),\n      end: bytes.readInt32LE(20),\n    }\n    const metaValue = bytes.readInt32LE(24)\n    const depth = 5\n    const maxBinNumber = ((1 << ((depth + 1) * 3)) - 1) / 7\n    const maxRefLength = 2 ** (14 + depth * 3)\n    const metaChar = metaValue ? String.fromCharCode(metaValue) : null\n    const skipLines = bytes.readInt32LE(28)\n\n    // read sequence dictionary\n    const nameSectionLength = bytes.readInt32LE(32)\n    const { refNameToId, refIdToName } = this._parseNameBytes(\n      bytes.slice(36, 36 + nameSectionLength),\n    )\n\n    // read the indexes for each reference sequence\n    let currOffset = 36 + nameSectionLength\n    let firstDataLine: VirtualOffset | undefined\n    const indices = new Array(refCount).fill(0).map(() => {\n      // the binning index\n      const binCount = bytes.readInt32LE(currOffset)\n      currOffset += 4\n      const binIndex: { [key: number]: Chunk[] } = {}\n      let stats\n      for (let j = 0; j < binCount; j += 1) {\n        const bin = bytes.readUInt32LE(currOffset)\n        currOffset += 4\n        if (bin > maxBinNumber + 1) {\n          throw new Error(\n            'tabix index contains too many bins, please use a CSI index',\n          )\n        } else if (bin === maxBinNumber + 1) {\n          const chunkCount = bytes.readInt32LE(currOffset)\n          currOffset += 4\n          if (chunkCount === 2) {\n            stats = this.parsePseudoBin(bytes, currOffset)\n          }\n          currOffset += 16 * chunkCount\n        } else {\n          const chunkCount = bytes.readInt32LE(currOffset)\n          currOffset += 4\n          const chunks = new Array(chunkCount)\n          for (let k = 0; k < chunkCount; k += 1) {\n            const u = fromBytes(bytes, currOffset)\n            const v = fromBytes(bytes, currOffset + 8)\n            currOffset += 16\n            firstDataLine = this._findFirstData(firstDataLine, u)\n            chunks[k] = new Chunk(u, v, bin)\n          }\n          binIndex[bin] = chunks\n        }\n      }\n\n      // the linear index\n      const linearCount = bytes.readInt32LE(currOffset)\n      currOffset += 4\n      const linearIndex = new Array(linearCount)\n      for (let k = 0; k < linearCount; k += 1) {\n        linearIndex[k] = fromBytes(bytes, currOffset)\n        currOffset += 8\n        firstDataLine = this._findFirstData(firstDataLine, linearIndex[k])\n      }\n      return { binIndex, linearIndex, stats }\n    })\n\n    return {\n      indices,\n      metaChar,\n      maxBinNumber,\n      maxRefLength,\n      skipLines,\n      firstDataLine,\n      columnNumbers,\n      coordinateType,\n      format,\n      refIdToName,\n      refNameToId,\n      maxBlockSize: 1 << 16,\n    }\n  }\n\n  parsePseudoBin(bytes: Buffer, offset: number) {\n    const lineCount = longToNumber(\n      Long.fromBytesLE(\n        bytes.slice(offset + 16, offset + 24) as unknown as number[],\n        true,\n      ),\n    )\n    return { lineCount }\n  }\n\n  _parseNameBytes(namesBytes: Buffer) {\n    let currRefId = 0\n    let currNameStart = 0\n    const refIdToName: string[] = []\n    const refNameToId: { [key: string]: number } = {}\n    for (let i = 0; i < namesBytes.length; i += 1) {\n      if (!namesBytes[i]) {\n        if (currNameStart < i) {\n          let refName = namesBytes.toString('utf8', currNameStart, i)\n          refName = this.renameRefSeq(refName)\n          refIdToName[currRefId] = refName\n          refNameToId[refName] = currRefId\n        }\n        currNameStart = i + 1\n        currRefId += 1\n      }\n    }\n    return { refNameToId, refIdToName }\n  }\n\n  async blocksForRange(\n    refName: string,\n    min: number,\n    max: number,\n    opts: Options = {},\n  ) {\n    if (min < 0) {\n      min = 0\n    }\n\n    const indexData = await this.parse(opts)\n    if (!indexData) {\n      return []\n    }\n    const refId = indexData.refNameToId[refName]\n    const ba = indexData.indices[refId]\n    if (!ba) {\n      return []\n    }\n\n    const minOffset = ba.linearIndex.length\n      ? ba.linearIndex[\n          min >> TAD_LIDX_SHIFT >= ba.linearIndex.length\n            ? ba.linearIndex.length - 1\n            : min >> TAD_LIDX_SHIFT\n        ]\n      : new VirtualOffset(0, 0)\n    if (!minOffset) {\n      console.warn('querying outside of possible tabix range')\n    }\n\n    // const { linearIndex, binIndex } = indexes\n\n    const overlappingBins = reg2bins(min, max) // List of bin #s that overlap min, max\n    const chunks: Chunk[] = []\n\n    // Find chunks in overlapping bins.  Leaf bins (< 4681) are not pruned\n    for (const [start, end] of overlappingBins) {\n      for (let bin = start; bin <= end; bin++) {\n        if (ba.binIndex[bin]) {\n          const binChunks = ba.binIndex[bin]\n          for (let c = 0; c < binChunks.length; ++c) {\n            chunks.push(new Chunk(binChunks[c].minv, binChunks[c].maxv, bin))\n          }\n        }\n      }\n    }\n\n    // Use the linear index to find minimum file position of chunks that could\n    // contain alignments in the region\n    const nintv = ba.linearIndex.length\n    let lowest = null\n    const minLin = Math.min(min >> 14, nintv - 1)\n    const maxLin = Math.min(max >> 14, nintv - 1)\n    for (let i = minLin; i <= maxLin; ++i) {\n      const vp = ba.linearIndex[i]\n      if (vp) {\n        if (!lowest || vp.compareTo(lowest) < 0) {\n          lowest = vp\n        }\n      }\n    }\n\n    return optimizeChunks(chunks, lowest)\n  }\n}\n","import Long from 'long'\nimport { Buffer } from 'buffer'\nimport { unzip } from '@gmod/bgzf-filehandle'\n\nimport VirtualOffset, { fromBytes } from './virtualOffset'\nimport Chunk from './chunk'\nimport { longToNumber, optimizeChunks } from './util'\n\nimport IndexFile, { Options } from './indexFile'\n\nconst CSI1_MAGIC = 21582659 // CSI\\1\nconst CSI2_MAGIC = 38359875 // CSI\\2\n\nfunction lshift(num: number, bits: number) {\n  return num * 2 ** bits\n}\nfunction rshift(num: number, bits: number) {\n  return Math.floor(num / 2 ** bits)\n}\n\nexport default class CSI extends IndexFile {\n  private maxBinNumber: number\n  private depth: number\n  private minShift: number\n  constructor(args: any) {\n    super(args)\n    this.maxBinNumber = 0\n    this.depth = 0\n    this.minShift = 0\n  }\n  async lineCount(refName: string, opts: Options = {}): Promise<number> {\n    const indexData = await this.parse(opts)\n    if (!indexData) {\n      return -1\n    }\n    const refId = indexData.refNameToId[refName]\n    const idx = indexData.indices[refId]\n    if (!idx) {\n      return -1\n    }\n    const { stats } = indexData.indices[refId]\n    if (stats) {\n      return stats.lineCount\n    }\n    return -1\n  }\n  async indexCov() {\n    throw new Error('CSI indexes do not support indexcov')\n  }\n\n  parseAuxData(bytes: Buffer, offset: number) {\n    const formatFlags = bytes.readInt32LE(offset)\n    const coordinateType =\n      formatFlags & 0x10000 ? 'zero-based-half-open' : '1-based-closed'\n    const format = { 0: 'generic', 1: 'SAM', 2: 'VCF' }[formatFlags & 0xf]\n    if (!format) {\n      throw new Error(`invalid Tabix preset format flags ${formatFlags}`)\n    }\n    const columnNumbers = {\n      ref: bytes.readInt32LE(offset + 4),\n      start: bytes.readInt32LE(offset + 8),\n      end: bytes.readInt32LE(offset + 12),\n    }\n    const metaValue = bytes.readInt32LE(offset + 16)\n    const metaChar = metaValue ? String.fromCharCode(metaValue) : null\n    const skipLines = bytes.readInt32LE(offset + 20)\n    const nameSectionLength = bytes.readInt32LE(offset + 24)\n\n    const { refIdToName, refNameToId } = this._parseNameBytes(\n      bytes.slice(offset + 28, offset + 28 + nameSectionLength),\n    )\n\n    return {\n      refIdToName,\n      refNameToId,\n      skipLines,\n      metaChar,\n      columnNumbers,\n      format,\n      coordinateType,\n    }\n  }\n\n  _parseNameBytes(namesBytes: Buffer) {\n    let currRefId = 0\n    let currNameStart = 0\n    const refIdToName = []\n    const refNameToId: { [key: string]: number } = {}\n    for (let i = 0; i < namesBytes.length; i += 1) {\n      if (!namesBytes[i]) {\n        if (currNameStart < i) {\n          let refName = namesBytes.toString('utf8', currNameStart, i)\n          refName = this.renameRefSeq(refName)\n          refIdToName[currRefId] = refName\n          refNameToId[refName] = currRefId\n        }\n        currNameStart = i + 1\n        currRefId += 1\n      }\n    }\n    return { refNameToId, refIdToName }\n  }\n\n  // fetch and parse the index\n\n  async _parse(opts: Options = {}) {\n    const bytes = await unzip((await this.filehandle.readFile(opts)) as Buffer)\n\n    // check TBI magic numbers\n    let csiVersion\n    if (bytes.readUInt32LE(0) === CSI1_MAGIC) {\n      csiVersion = 1\n    } else if (bytes.readUInt32LE(0) === CSI2_MAGIC) {\n      csiVersion = 2\n    } else {\n      throw new Error('Not a CSI file')\n      // TODO: do we need to support big-endian CSI files?\n    }\n\n    this.minShift = bytes.readInt32LE(4)\n    this.depth = bytes.readInt32LE(8)\n    this.maxBinNumber = ((1 << ((this.depth + 1) * 3)) - 1) / 7\n    const maxRefLength = 2 ** (this.minShift + this.depth * 3)\n    const auxLength = bytes.readInt32LE(12)\n    const aux =\n      auxLength && auxLength >= 30\n        ? this.parseAuxData(bytes, 16)\n        : {\n            refIdToName: [],\n            refNameToId: {},\n            metaChar: null,\n            columnNumbers: { ref: 0, start: 1, end: 2 },\n            coordinateType: 'zero-based-half-open',\n            format: 'generic',\n          }\n    const refCount = bytes.readInt32LE(16 + auxLength)\n\n    // read the indexes for each reference sequence\n    let firstDataLine: VirtualOffset | undefined\n    let currOffset = 16 + auxLength + 4\n    const indices = new Array(refCount).fill(0).map(() => {\n      // the binning index\n      const binCount = bytes.readInt32LE(currOffset)\n      currOffset += 4\n      const binIndex: { [key: string]: Chunk[] } = {}\n      let stats // < provided by parsing a pseudo-bin, if present\n      for (let j = 0; j < binCount; j += 1) {\n        const bin = bytes.readUInt32LE(currOffset)\n        if (bin > this.maxBinNumber) {\n          // this is a fake bin that actually has stats information\n          // about the reference sequence in it\n          stats = this.parsePseudoBin(bytes, currOffset + 4)\n          currOffset += 4 + 8 + 4 + 16 + 16\n        } else {\n          const loffset = fromBytes(bytes, currOffset + 4)\n          firstDataLine = this._findFirstData(firstDataLine, loffset)\n          const chunkCount = bytes.readInt32LE(currOffset + 12)\n          currOffset += 16\n          const chunks = new Array(chunkCount)\n          for (let k = 0; k < chunkCount; k += 1) {\n            const u = fromBytes(bytes, currOffset)\n            const v = fromBytes(bytes, currOffset + 8)\n            currOffset += 16\n            // this._findFirstData(data, u)\n            chunks[k] = new Chunk(u, v, bin)\n          }\n          binIndex[bin] = chunks\n        }\n      }\n\n      return { binIndex, stats }\n    })\n\n    return {\n      ...aux,\n      csi: true,\n      refCount,\n      maxBlockSize: 1 << 16,\n      firstDataLine,\n      csiVersion,\n      indices,\n      depth: this.depth,\n      maxBinNumber: this.maxBinNumber,\n      maxRefLength,\n    }\n  }\n\n  parsePseudoBin(bytes: Buffer, offset: number) {\n    const lineCount = longToNumber(\n      Long.fromBytesLE(\n        bytes.slice(offset + 28, offset + 36) as unknown as number[],\n        true,\n      ),\n    )\n    return { lineCount }\n  }\n\n  async blocksForRange(\n    refName: string,\n    min: number,\n    max: number,\n    opts: Options = {},\n  ) {\n    if (min < 0) {\n      min = 0\n    }\n\n    const indexData = await this.parse(opts)\n    if (!indexData) {\n      return []\n    }\n    const refId = indexData.refNameToId[refName]\n    const ba = indexData.indices[refId]\n    if (!ba) {\n      return []\n    }\n\n    // const { linearIndex, binIndex } = indexes\n\n    const overlappingBins = this.reg2bins(min, max) // List of bin #s that overlap min, max\n    const chunks: Chunk[] = []\n\n    // Find chunks in overlapping bins.  Leaf bins (< 4681) are not pruned\n    for (const [start, end] of overlappingBins) {\n      for (let bin = start; bin <= end; bin++) {\n        if (ba.binIndex[bin]) {\n          const binChunks = ba.binIndex[bin]\n          for (let c = 0; c < binChunks.length; ++c) {\n            chunks.push(new Chunk(binChunks[c].minv, binChunks[c].maxv, bin))\n          }\n        }\n      }\n    }\n\n    return optimizeChunks(chunks, new VirtualOffset(0, 0))\n  }\n\n  /**\n   * calculate the list of bins that may overlap with region [beg,end) (zero-based half-open)\n   */\n  reg2bins(beg: number, end: number) {\n    beg -= 1 // < convert to 1-based closed\n    if (beg < 1) {\n      beg = 1\n    }\n    if (end > 2 ** 50) {\n      end = 2 ** 34\n    } // 17 GiB ought to be enough for anybody\n    end -= 1\n    let l = 0\n    let t = 0\n    let s = this.minShift + this.depth * 3\n    const bins = []\n    for (; l <= this.depth; s -= 3, t += lshift(1, l * 3), l += 1) {\n      const b = t + rshift(beg, s)\n      const e = t + rshift(end, s)\n      if (e - b + bins.length > this.maxBinNumber) {\n        throw new Error(\n          `query ${beg}-${end} is too large for current binning scheme (shift ${this.minShift}, depth ${this.depth}), try a smaller query or a coarser index binning scheme`,\n        )\n      }\n      bins.push([b, e])\n    }\n    return bins\n  }\n}\n","import AbortablePromiseCache from 'abortable-promise-cache'\nimport LRU from 'quick-lru'\nimport { Buffer } from 'buffer'\nimport { GenericFilehandle, LocalFile } from 'generic-filehandle'\nimport { unzip, unzipChunkSlice } from '@gmod/bgzf-filehandle'\nimport { checkAbortSignal } from './util'\nimport IndexFile, { Options, IndexData } from './indexFile'\n\nimport Chunk from './chunk'\nimport TBI from './tbi'\nimport CSI from './csi'\n\ntype GetLinesCallback = (line: string, fileOffset: number) => void\n\nconst decoder =\n  typeof TextDecoder !== 'undefined' ? new TextDecoder('utf-8') : undefined\n\ninterface GetLinesOpts {\n  [key: string]: unknown\n  signal?: AbortSignal\n  lineCallback: GetLinesCallback\n}\n\ninterface ReadChunk {\n  buffer: Buffer\n  cpositions: number[]\n  dpositions: number[]\n}\n\nfunction timeout(time: number) {\n  return new Promise(resolve => setTimeout(resolve, time))\n}\nexport default class TabixIndexedFile {\n  private filehandle: GenericFilehandle\n  private index: IndexFile\n  private chunkSizeLimit: number\n  private yieldTime: number\n  private renameRefSeq: (n: string) => string\n  private chunkCache: AbortablePromiseCache<Chunk, ReadChunk>\n\n  /**\n   * @param {object} args\n   * @param {string} [args.path]\n   * @param {filehandle} [args.filehandle]\n   * @param {string} [args.tbiPath]\n   * @param {filehandle} [args.tbiFilehandle]\n   * @param {string} [args.csiPath]\n   * @param {filehandle} [args.csiFilehandle]\n   * @param {number} [args.yieldTime] yield to main thread after N milliseconds if reading features is taking a long time to avoid hanging main thread\n   * @param {function} [args.renameRefSeqs] optional function with sig `string => string` to transform\n   * reference sequence names for the purpose of indexing and querying. note that the data that is returned is\n   * not altered, just the names of the reference sequences that are used for querying.\n   */\n  constructor({\n    path,\n    filehandle,\n    tbiPath,\n    tbiFilehandle,\n    csiPath,\n    csiFilehandle,\n    yieldTime = 500,\n    chunkSizeLimit = 50000000,\n    renameRefSeqs = n => n,\n    chunkCacheSize = 5 * 2 ** 20,\n  }: {\n    path?: string\n    filehandle?: GenericFilehandle\n    tbiPath?: string\n    tbiFilehandle?: GenericFilehandle\n    csiPath?: string\n    csiFilehandle?: GenericFilehandle\n    yieldTime?: number\n    chunkSizeLimit?: number\n    renameRefSeqs?: (n: string) => string\n    chunkCacheSize?: number\n  }) {\n    if (filehandle) {\n      this.filehandle = filehandle\n    } else if (path) {\n      this.filehandle = new LocalFile(path)\n    } else {\n      throw new TypeError('must provide either filehandle or path')\n    }\n\n    if (tbiFilehandle) {\n      this.index = new TBI({\n        filehandle: tbiFilehandle,\n        renameRefSeqs,\n      })\n    } else if (csiFilehandle) {\n      this.index = new CSI({\n        filehandle: csiFilehandle,\n        renameRefSeqs,\n      })\n    } else if (tbiPath) {\n      this.index = new TBI({\n        filehandle: new LocalFile(tbiPath),\n        renameRefSeqs,\n      })\n    } else if (csiPath) {\n      this.index = new CSI({\n        filehandle: new LocalFile(csiPath),\n        renameRefSeqs,\n      })\n    } else if (path) {\n      this.index = new TBI({\n        filehandle: new LocalFile(`${path}.tbi`),\n        renameRefSeqs,\n      })\n    } else {\n      throw new TypeError(\n        'must provide one of tbiFilehandle, tbiPath, csiFilehandle, or csiPath',\n      )\n    }\n\n    this.chunkSizeLimit = chunkSizeLimit\n    this.renameRefSeq = renameRefSeqs\n    this.yieldTime = yieldTime\n    this.chunkCache = new AbortablePromiseCache<Chunk, ReadChunk>({\n      cache: new LRU({ maxSize: Math.floor(chunkCacheSize / (1 << 16)) }),\n      fill: (args: Chunk, signal?: AbortSignal) =>\n        this.readChunk(args, { signal }),\n    })\n  }\n\n  /**\n   * @param refName name of the reference sequence\n   * @param start start of the region (in 0-based half-open coordinates)\n   * @param end end of the region (in 0-based half-open coordinates)\n   * @param opts callback called for each line in the region. can also pass a object param containing obj.lineCallback, obj.signal, etc\n   * @returns promise that is resolved when the whole read is finished, rejected on error\n   */\n  async getLines(\n    refName: string,\n    start: number,\n    end: number,\n    opts: GetLinesOpts | GetLinesCallback,\n  ) {\n    let signal: AbortSignal | undefined\n    let options: Options = {}\n    let callback: (line: string, lineOffset: number) => void\n    if (typeof opts === 'undefined') {\n      throw new TypeError('line callback must be provided')\n    }\n    if (typeof opts === 'function') {\n      callback = opts\n    } else {\n      options = opts\n      callback = opts.lineCallback\n    }\n    if (refName === undefined) {\n      throw new TypeError('must provide a reference sequence name')\n    }\n    if (!callback) {\n      throw new TypeError('line callback must be provided')\n    }\n\n    const metadata = await this.index.getMetadata(options)\n    checkAbortSignal(signal)\n    if (!start) {\n      start = 0\n    }\n    if (!end) {\n      end = metadata.maxRefLength\n    }\n    if (!(start <= end)) {\n      throw new TypeError(\n        'invalid start and end coordinates. start must be less than or equal to end',\n      )\n    }\n    if (start === end) {\n      return\n    }\n\n    const chunks = await this.index.blocksForRange(refName, start, end, options)\n    checkAbortSignal(signal)\n\n    // check the chunks for any that are over the size limit.  if\n    // any are, don't fetch any of them\n    for (let i = 0; i < chunks.length; i += 1) {\n      const size = chunks[i].fetchedSize()\n      if (size > this.chunkSizeLimit) {\n        throw new Error(\n          `Too much data. Chunk size ${size.toLocaleString()} bytes exceeds chunkSizeLimit of ${this.chunkSizeLimit.toLocaleString()}.`,\n        )\n      }\n    }\n\n    // now go through each chunk and parse and filter the lines out of it\n    let last = Date.now()\n    for (let chunkNum = 0; chunkNum < chunks.length; chunkNum += 1) {\n      let previousStartCoordinate: number | undefined\n      const c = chunks[chunkNum]\n      const { buffer, cpositions, dpositions } = await this.chunkCache.get(\n        c.toString(),\n        c,\n      )\n\n      checkAbortSignal(signal)\n      let blockStart = 0\n      let pos = 0\n      while (blockStart < buffer.length) {\n        const n = buffer.indexOf('\\n', blockStart)\n        if (n === -1) {\n          break\n        }\n        const b = buffer.slice(blockStart, n)\n        const line = decoder?.decode(b) || b.toString()\n\n        if (dpositions) {\n          while (blockStart + c.minv.dataPosition >= dpositions[pos++]) {}\n          pos--\n        }\n\n        // filter the line for whether it is within the requested range\n        const { startCoordinate, overlaps } = this.checkLine(\n          metadata,\n          refName,\n          start,\n          end,\n          line,\n        )\n\n        // do a small check just to make sure that the lines are really sorted\n        // by start coordinate\n        if (\n          previousStartCoordinate !== undefined &&\n          startCoordinate !== undefined &&\n          previousStartCoordinate > startCoordinate\n        ) {\n          throw new Error(\n            `Lines not sorted by start coordinate (${previousStartCoordinate} > ${startCoordinate}), this file is not usable with Tabix.`,\n          )\n        }\n        previousStartCoordinate = startCoordinate\n\n        if (overlaps) {\n          callback(\n            line.trim(),\n            // cpositions[pos] refers to actual file offset of a bgzip block boundaries\n            //\n            // we multiply by (1 <<8) in order to make sure each block has a \"unique\"\n            // address space so that data in that block could never overlap\n            //\n            // then the blockStart-dpositions is an uncompressed file offset from\n            // that bgzip block boundary, and since the cpositions are multiplied by\n            // (1 << 8) these uncompressed offsets get a unique space\n            cpositions[pos] * (1 << 8) +\n              (blockStart - dpositions[pos]) +\n              c.minv.dataPosition +\n              1,\n          )\n        } else if (startCoordinate !== undefined && startCoordinate >= end) {\n          // the lines were overlapping the region, but now have stopped, so\n          // we must be at the end of the relevant data and we can stop\n          // processing data now\n          return\n        }\n\n        // yield if we have emitted beyond the yield limit\n        if (this.yieldTime && last - Date.now() > this.yieldTime) {\n          last = Date.now()\n          checkAbortSignal(signal)\n          await timeout(1)\n        }\n        blockStart = n + 1\n      }\n    }\n  }\n\n  async getMetadata(opts: Options = {}) {\n    return this.index.getMetadata(opts)\n  }\n\n  /**\n   * get a buffer containing the \"header\" region of\n   * the file, which are the bytes up to the first\n   * non-meta line\n   */\n  async getHeaderBuffer(opts: Options = {}) {\n    const { firstDataLine, metaChar, maxBlockSize } = await this.getMetadata(\n      opts,\n    )\n    checkAbortSignal(opts.signal)\n    const maxFetch = (firstDataLine?.blockPosition || 0) + maxBlockSize\n    // TODO: what if we don't have a firstDataLine, and the header\n    // actually takes up more than one block? this case is not covered here\n\n    let bytes = await this._readRegion(0, maxFetch, opts)\n    checkAbortSignal(opts.signal)\n    try {\n      bytes = await unzip(bytes)\n    } catch (e) {\n      console.error(e)\n      throw new Error(\n        //@ts-ignore\n        `error decompressing block ${e.code} at 0 (length ${maxFetch}) ${e}`,\n      )\n    }\n\n    // trim off lines after the last non-meta line\n    if (metaChar) {\n      // trim backward from the end\n      let lastNewline = -1\n      const newlineByte = '\\n'.charCodeAt(0)\n      const metaByte = metaChar.charCodeAt(0)\n      for (let i = 0; i < bytes.length; i += 1) {\n        if (i === lastNewline + 1 && bytes[i] !== metaByte) {\n          break\n        }\n        if (bytes[i] === newlineByte) {\n          lastNewline = i\n        }\n      }\n      bytes = bytes.slice(0, lastNewline + 1)\n    }\n    return bytes\n  }\n\n  /**\n   * get a string containing the \"header\" region of the\n   * file, is the portion up to the first non-meta line\n   *\n   * @returns {Promise} for a string\n   */\n  async getHeader(opts: Options = {}) {\n    const bytes = await this.getHeaderBuffer(opts)\n    return bytes.toString('utf8')\n  }\n\n  /**\n   * get an array of reference sequence names, in the order in which\n   * they occur in the file. reference sequence renaming is not applied\n   * to these names.\n   */\n  async getReferenceSequenceNames(opts: Options = {}) {\n    const metadata = await this.getMetadata(opts)\n    return metadata.refIdToName\n  }\n\n  /**\n   * @param {object} metadata metadata object from the parsed index,\n   * containing columnNumbers, metaChar, and format\n   * @param {string} regionRefName\n   * @param {number} regionStart region start coordinate (0-based-half-open)\n   * @param {number} regionEnd region end coordinate (0-based-half-open)\n   * @param {array[string]} line\n   * @returns {object} like `{startCoordinate, overlaps}`. overlaps is boolean,\n   * true if line is a data line that overlaps the given region\n   */\n  checkLine(\n    metadata: IndexData,\n    regionRefName: string,\n    regionStart: number,\n    regionEnd: number,\n    line: string,\n  ) {\n    const { columnNumbers, metaChar, coordinateType, format } = metadata\n    // skip meta lines\n    if (line.charAt(0) === metaChar) {\n      return { overlaps: false }\n    }\n\n    // check ref/start/end using column metadata from index\n    let { ref, start, end } = columnNumbers\n    if (!ref) {\n      ref = 0\n    }\n    if (!start) {\n      start = 0\n    }\n    if (!end) {\n      end = 0\n    }\n    if (format === 'VCF') {\n      end = 8\n    }\n    const maxColumn = Math.max(ref, start, end)\n\n    // this code is kind of complex, but it is fairly fast.\n    // basically, we want to avoid doing a split, because if the lines are really long\n    // that could lead to us allocating a bunch of extra memory, which is slow\n\n    let currentColumnNumber = 1 // cols are numbered starting at 1 in the index metadata\n    let currentColumnStart = 0\n    let refSeq = ''\n    let startCoordinate = -Infinity\n    for (let i = 0; i < line.length + 1; i += 1) {\n      if (line[i] === '\\t' || i === line.length) {\n        if (currentColumnNumber === ref) {\n          if (\n            this.renameRefSeq(line.slice(currentColumnStart, i)) !==\n            regionRefName\n          ) {\n            return { overlaps: false }\n          }\n        } else if (currentColumnNumber === start) {\n          startCoordinate = parseInt(line.slice(currentColumnStart, i), 10)\n          // we convert to 0-based-half-open\n          if (coordinateType === '1-based-closed') {\n            startCoordinate -= 1\n          }\n          if (startCoordinate >= regionEnd) {\n            return { startCoordinate, overlaps: false }\n          }\n          if (end === 0 || end === start) {\n            // if we have no end, we assume the feature is 1 bp long\n            if (startCoordinate + 1 <= regionStart) {\n              return { startCoordinate, overlaps: false }\n            }\n          }\n        } else if (format === 'VCF' && currentColumnNumber === 4) {\n          refSeq = line.slice(currentColumnStart, i)\n        } else if (currentColumnNumber === end) {\n          let endCoordinate\n          // this will never match if there is no end column\n          if (format === 'VCF') {\n            endCoordinate = this._getVcfEnd(\n              startCoordinate,\n              refSeq,\n              line.slice(currentColumnStart, i),\n            )\n          } else {\n            endCoordinate = parseInt(line.slice(currentColumnStart, i), 10)\n          }\n          if (endCoordinate <= regionStart) {\n            return { overlaps: false }\n          }\n        }\n        currentColumnStart = i + 1\n        currentColumnNumber += 1\n        if (currentColumnNumber > maxColumn) {\n          break\n        }\n      }\n    }\n    return { startCoordinate, overlaps: true }\n  }\n\n  _getVcfEnd(startCoordinate: number, refSeq: string, info: any) {\n    let endCoordinate = startCoordinate + refSeq.length\n    // ignore TRA features as they specify CHR2 and END\n    // as being on a different chromosome\n    // if CHR2 is on the same chromosome, still ignore it\n    // because there should be another pairwise feature\n    // at the end of this one\n    const isTRA = info.indexOf('SVTYPE=TRA') !== -1\n    if (info[0] !== '.' && !isTRA) {\n      let prevChar = ';'\n      for (let j = 0; j < info.length; j += 1) {\n        if (prevChar === ';' && info.slice(j, j + 4) === 'END=') {\n          let valueEnd = info.indexOf(';', j)\n          if (valueEnd === -1) {\n            valueEnd = info.length\n          }\n          endCoordinate = parseInt(info.slice(j + 4, valueEnd), 10)\n          break\n        }\n        prevChar = info[j]\n      }\n    } else if (isTRA) {\n      return startCoordinate + 1\n    }\n    return endCoordinate\n  }\n\n  /**\n   * return the approximate number of data lines in the given reference sequence\n   * @param refSeq reference sequence name\n   * @returns number of data lines present on that reference sequence\n   */\n  async lineCount(refName: string, opts: Options = {}) {\n    return this.index.lineCount(refName, opts)\n  }\n\n  async _readRegion(pos: number, size: number, opts: Options = {}) {\n    const b = Buffer.alloc(size)\n    const { bytesRead, buffer } = await this.filehandle.read(\n      b,\n      0,\n      size,\n      pos,\n      opts,\n    )\n\n    return buffer.slice(0, bytesRead)\n  }\n\n  /**\n   * read and uncompress the data in a chunk (composed of one or more\n   * contiguous bgzip blocks) of the file\n   */\n  async readChunk(c: Chunk, opts: Options = {}) {\n    // fetch the uncompressed data, uncompress carefully a block at a time,\n    // and stop when done\n\n    const data = await this._readRegion(\n      c.minv.blockPosition,\n      c.fetchedSize(),\n      opts,\n    )\n    try {\n      return unzipChunkSlice(data, c)\n    } catch (e) {\n      throw new Error(`error decompressing c ${c.toString()} ${e}`)\n    }\n  }\n}\n","\"use strict\";\nvar __importDefault = (this && this.__importDefault) || function (mod) {\n    return (mod && mod.__esModule) ? mod : { \"default\": mod };\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nconst abortcontroller_ponyfill_1 = require(\"./abortcontroller-ponyfill\");\nconst AggregateAbortController_1 = __importDefault(require(\"./AggregateAbortController\"));\nconst AggregateStatusReporter_1 = __importDefault(require(\"./AggregateStatusReporter\"));\nclass AbortablePromiseCache {\n    constructor({ fill, cache, }) {\n        if (typeof fill !== 'function') {\n            throw new TypeError('must pass a fill function');\n        }\n        if (typeof cache !== 'object') {\n            throw new TypeError('must pass a cache object');\n        }\n        if (typeof cache.get !== 'function' ||\n            typeof cache.set !== 'function' ||\n            typeof cache.delete !== 'function') {\n            throw new TypeError('cache must implement get(key), set(key, val), and and delete(key)');\n        }\n        this.cache = cache;\n        this.fillCallback = fill;\n    }\n    static isAbortException(exception) {\n        return (\n        // DOMException\n        exception.name === 'AbortError' ||\n            // standard-ish non-DOM abort exception\n            //@ts-ignore\n            exception.code === 'ERR_ABORTED' ||\n            // stringified DOMException\n            exception.message === 'AbortError: aborted' ||\n            // stringified standard-ish exception\n            exception.message === 'Error: aborted');\n    }\n    evict(key, entry) {\n        if (this.cache.get(key) === entry) {\n            this.cache.delete(key);\n        }\n    }\n    fill(key, data, signal, statusCallback) {\n        const aborter = new AggregateAbortController_1.default();\n        const statusReporter = new AggregateStatusReporter_1.default();\n        statusReporter.addCallback(statusCallback);\n        const newEntry = {\n            aborter: aborter,\n            promise: this.fillCallback(data, aborter.signal, (message) => {\n                statusReporter.callback(message);\n            }),\n            settled: false,\n            statusReporter,\n            get aborted() {\n                return this.aborter.signal.aborted;\n            },\n        };\n        newEntry.aborter.addSignal(signal);\n        // remove the fill from the cache when its abortcontroller fires, if still in there\n        newEntry.aborter.signal.addEventListener('abort', () => {\n            if (!newEntry.settled) {\n                this.evict(key, newEntry);\n            }\n        });\n        // chain off the cached promise to record when it settles\n        newEntry.promise\n            .then(() => {\n            newEntry.settled = true;\n        }, () => {\n            newEntry.settled = true;\n            // if the fill throws an error (including abort) and is still in the cache, remove it\n            this.evict(key, newEntry);\n        })\n            .catch(e => {\n            // this will only be reached if there is some kind of\n            // bad bug in this library\n            console.error(e);\n            throw e;\n        });\n        this.cache.set(key, newEntry);\n    }\n    static checkSinglePromise(promise, signal) {\n        // check just this signal for having been aborted, and abort the\n        // promise if it was, regardless of what happened with the cached\n        // response\n        function checkForSingleAbort() {\n            if (signal && signal.aborted) {\n                throw Object.assign(new Error('aborted'), { code: 'ERR_ABORTED' });\n            }\n        }\n        return promise.then(result => {\n            checkForSingleAbort();\n            return result;\n        }, error => {\n            checkForSingleAbort();\n            throw error;\n        });\n    }\n    has(key) {\n        return this.cache.has(key);\n    }\n    /**\n     * Callback for getting status of the pending async\n     *\n     * @callback statusCallback\n     * @param {any} status, current status string or message object\n     */\n    /**\n     * @param {any} key cache key to use for this request\n     * @param {any} data data passed as the first argument to the fill callback\n     * @param {AbortSignal} [signal] optional AbortSignal object that aborts the request\n     * @param {statusCallback} a callback to get the current status of a pending async operation\n     */\n    get(key, data, signal, statusCallback) {\n        if (!signal && data instanceof abortcontroller_ponyfill_1.AbortSignal) {\n            throw new TypeError('second get argument appears to be an AbortSignal, perhaps you meant to pass `null` for the fill data?');\n        }\n        const cacheEntry = this.cache.get(key);\n        if (cacheEntry) {\n            if (cacheEntry.aborted && !cacheEntry.settled) {\n                // if it's aborted but has not realized it yet, evict it and redispatch\n                this.evict(key, cacheEntry);\n                return this.get(key, data, signal, statusCallback);\n            }\n            if (cacheEntry.settled) {\n                // too late to abort, just return it\n                return cacheEntry.promise;\n            }\n            // request is in-flight, add this signal to its list of signals,\n            // or if there is no signal, the aborter will become non-abortable\n            cacheEntry.aborter.addSignal(signal);\n            cacheEntry.statusReporter.addCallback(statusCallback);\n            return AbortablePromiseCache.checkSinglePromise(cacheEntry.promise, signal);\n        }\n        // if we got here, it is not in the cache. fill.\n        this.fill(key, data, signal, statusCallback);\n        return AbortablePromiseCache.checkSinglePromise(\n        //see https://www.typescriptlang.org/docs/handbook/2/everyday-types.html#non-null-assertion-operator-postfix-\n        //eslint-disable-next-line @typescript-eslint/no-non-null-assertion\n        this.cache.get(key).promise, signal);\n    }\n    /**\n     * delete the given entry from the cache. if it exists and its fill request has\n     * not yet settled, the fill will be signaled to abort.\n     *\n     * @param {any} key\n     */\n    delete(key) {\n        const cachedEntry = this.cache.get(key);\n        if (cachedEntry) {\n            if (!cachedEntry.settled) {\n                cachedEntry.aborter.abort();\n            }\n            this.cache.delete(key);\n        }\n    }\n    /**\n     * Clear all requests from the cache. Aborts any that have not settled.\n     * @returns {number} count of entries deleted\n     */\n    clear() {\n        // iterate without needing regenerator-runtime\n        const keyIter = this.cache.keys();\n        let deleteCount = 0;\n        for (let result = keyIter.next(); !result.done; result = keyIter.next()) {\n            this.delete(result.value);\n            deleteCount += 1;\n        }\n        return deleteCount;\n    }\n}\nexports.default = AbortablePromiseCache;\n","\"use strict\";\nObject.defineProperty(exports, \"__esModule\", { value: true });\nconst abortcontroller_ponyfill_1 = require(\"./abortcontroller-ponyfill\");\nclass NullSignal {\n}\n/**\n * aggregates a number of abort signals, will only fire the aggregated\n * abort if all of the input signals have been aborted\n */\nclass AggregateAbortController {\n    constructor() {\n        this.signals = new Set();\n        this.abortController = new abortcontroller_ponyfill_1.AbortController();\n    }\n    /**\n     * @param {AbortSignal} [signal] optional AbortSignal to add. if falsy,\n     *  will be treated as a null-signal, and this abortcontroller will no\n     *  longer be abortable.\n     */\n    //@ts-ignore\n    addSignal(signal = new NullSignal()) {\n        if (this.signal.aborted) {\n            throw new Error('cannot add a signal, already aborted!');\n        }\n        // note that a NullSignal will never fire, so if we\n        // have one this thing will never actually abort\n        this.signals.add(signal);\n        if (signal.aborted) {\n            // handle the abort immediately if it is already aborted\n            // for some reason\n            this.handleAborted(signal);\n        }\n        else if (typeof signal.addEventListener === 'function') {\n            signal.addEventListener('abort', () => {\n                this.handleAborted(signal);\n            });\n        }\n    }\n    handleAborted(signal) {\n        this.signals.delete(signal);\n        if (this.signals.size === 0) {\n            this.abortController.abort();\n        }\n    }\n    get signal() {\n        return this.abortController.signal;\n    }\n    abort() {\n        this.abortController.abort();\n    }\n}\nexports.default = AggregateAbortController;\n","\"use strict\";\nObject.defineProperty(exports, \"__esModule\", { value: true });\nclass AggregateStatusReporter {\n    constructor() {\n        this.callbacks = new Set();\n    }\n    addCallback(callback = () => { }) {\n        this.callbacks.add(callback);\n        callback(this.currentMessage);\n    }\n    callback(message) {\n        this.currentMessage = message;\n        this.callbacks.forEach(elt => {\n            elt(message);\n        });\n    }\n}\nexports.default = AggregateStatusReporter;\n","\"use strict\";\n/* eslint-disable */\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.AbortSignal = exports.AbortController = void 0;\nconst cjs_ponyfill_1 = require(\"abortcontroller-polyfill/dist/cjs-ponyfill\");\nvar getGlobal = function () {\n    // the only reliable means to get the global object is\n    // `Function('return this')()`\n    // However, this causes CSP violations in Chrome apps.\n    if (typeof self !== 'undefined') {\n        return self;\n    }\n    if (typeof window !== 'undefined') {\n        return window;\n    }\n    if (typeof global !== 'undefined') {\n        return global;\n    }\n    throw new Error('unable to locate global object');\n};\n//@ts-ignore\nlet AbortController = typeof getGlobal().AbortController === 'undefined' ? cjs_ponyfill_1.AbortController : getGlobal().AbortController;\nexports.AbortController = AbortController;\n//@ts-ignore\nlet AbortSignal = typeof getGlobal().AbortController === 'undefined' ? cjs_ponyfill_1.AbortSignal : getGlobal().AbortSignal;\nexports.AbortSignal = AbortSignal;\n","\"use strict\";\nvar __importDefault = (this && this.__importDefault) || function (mod) {\n    return (mod && mod.__esModule) ? mod : { \"default\": mod };\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nconst AbortablePromiseCache_1 = __importDefault(require(\"./AbortablePromiseCache\"));\nexports.default = AbortablePromiseCache_1.default;\n"],"names":["async","unzip","inputData","strm","pos","i","chunks","inflator","totalSize","remainingInput","subarray","Inflate","push","Z_SYNC_FLUSH","err","Error","msg","next_in","result","length","avail_in","Uint8Array","offset","set","Buffer","from","e","match","unzipChunkSlice","chunk","minv","maxv","cpos","blockPosition","dpos","dataPosition","cpositions","dpositions","buffer","len","origCpos","GziIndex","constructor","filehandle","path","this","TypeError","_readLongWithOverflow","buf","unsigned","long","slice","greaterThan","Number","MAX_SAFE_INTEGER","lessThan","MIN_SAFE_INTEGER","toNumber","_getIndex","index","_readIndex","allocUnsafe","read","numEntries","entries","Array","bufSize","entryNumber","compressedPosition","uncompressedPosition","getLastBlock","getRelevantBlocksForRead","position","endPosition","relevant","compare","entry","nextEntry","nextUncompressedPosition","Infinity","lowerBound","upperBound","searchPosition","Math","floor","comparison","ceil","BgzFilehandle","gziFilehandle","gziPath","gzi","stat","compressedStat","Object","assign","size","getUncompressedFileSize","blocks","undefined","blksize","bytesRead","readUInt32LE","_readAndUncompressBlock","blockBuffer","nextCompressedPosition","next","blockCompressedLength","blockPositions","destinationOffset","blockNum","uncompressedBuffer","sourceOffset","sourceEnd","min","copy","longToNumber","AbortError","checkAbortSignal","signal","aborted","DOMException","code","optimizeChunks","lowest","mergedChunks","lastChunk","sort","c0","c1","dif","forEach","chunk1","chunk2","compareTo","VirtualOffset","toString","b","args","fromBytes","bytes","bigendian","Chunk","bin","fetchedSize","_fetchedSize","toUniqueString","IndexFile","renameRefSeqs","n","renameRefSeq","getMetadata","opts","indices","rest","parse","_findFirstData","currentFdl","virtualOffset","parseP","_parse","catch","hasRefSeq","seqId","binIndex","TabixIndex","lineCount","refName","indexData","refId","refNameToId","stats","readFile","refCount","readInt32LE","formatFlags","coordinateType","format","columnNumbers","ref","start","end","metaValue","metaChar","String","fromCharCode","skipLines","nameSectionLength","refIdToName","_parseNameBytes","firstDataLine","currOffset","fill","map","binCount","j","maxBinNumber","chunkCount","parsePseudoBin","k","u","v","linearCount","linearIndex","maxRefLength","maxBlockSize","namesBytes","currRefId","currNameStart","blocksForRange","max","ba","console","warn","overlappingBins","beg","binChunks","c","nintv","minLin","maxLin","vp","rshift","num","bits","CSI","super","depth","minShift","indexCov","parseAuxData","csiVersion","auxLength","aux","loffset","csi","reg2bins","l","t","s","bins","decoder","TextDecoder","timeout","time","Promise","resolve","setTimeout","TabixIndexedFile","tbiPath","tbiFilehandle","csiPath","csiFilehandle","yieldTime","chunkSizeLimit","chunkCacheSize","chunkCache","cache","maxSize","readChunk","getLines","callback","options","lineCallback","metadata","toLocaleString","last","Date","now","chunkNum","previousStartCoordinate","get","blockStart","indexOf","line","decode","startCoordinate","overlaps","checkLine","trim","getHeaderBuffer","maxFetch","_readRegion","error","lastNewline","newlineByte","charCodeAt","metaByte","getHeader","getReferenceSequenceNames","regionRefName","regionStart","regionEnd","charAt","maxColumn","currentColumnNumber","currentColumnStart","refSeq","parseInt","endCoordinate","_getVcfEnd","info","isTRA","prevChar","valueEnd","alloc","data","__importDefault","mod","__esModule","defineProperty","exports","value","abortcontroller_ponyfill_1","AggregateAbortController_1","AggregateStatusReporter_1","AbortablePromiseCache","delete","fillCallback","isAbortException","exception","name","message","evict","key","statusCallback","aborter","default","statusReporter","addCallback","newEntry","promise","settled","addSignal","addEventListener","then","checkSinglePromise","checkForSingleAbort","has","AbortSignal","cacheEntry","cachedEntry","abort","clear","keyIter","keys","deleteCount","done","NullSignal","signals","Set","abortController","AbortController","add","handleAborted","callbacks","currentMessage","elt","cjs_ponyfill_1","getGlobal","self","window","g","AbortablePromiseCache_1"],"sourceRoot":""}